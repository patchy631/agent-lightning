{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Lightning","text":"<p>Agent Lightning is the absolute trainer to light up AI agents.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! \ud83d\udca4</li> <li>Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, ...); or even WITHOUT agent framework (Python OpenAI). You name it! \ud83e\udd16</li> <li>Selectively optimize one or more agents in a multi-agent system. \ud83c\udfaf</li> <li>Embraces Reinforcement Learning, Automatic Prompt Optimization and more algorithms. \ud83e\udd17</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation - Get started with Agent Lightning</li> <li>Quickstart - Learn the fundamentals of Agent Lightning</li> <li>Train SQL Agent with RL - A practical example of training a SQL agent</li> <li>API Reference - Complete API documentation</li> <li>Join our Discord community - Connect with other users and contributors</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium.</li> <li>8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper.</li> <li>7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit.</li> <li>6/6/2025 Agent Lightning - Microsoft Research Project page.</li> </ul>"},{"location":"#community-projects","title":"Community Projects","text":"<ul> <li>DeepWerewolf \u2014 A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p> <pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>See the LICENSE file for details.</p>"},{"location":"deep-dive/server-client-architecture/","title":"Server-client Architecture","text":"<p>Article to be written.</p> <pre><code>sequenceDiagram\n    participant RL as RL Framework\n    participant TS as Training Server\n    participant AC as Agent Client\n    participant AG as Agent\n\n    AC-&gt;&gt;TS: Upload Dataset (1)\n    RL-&gt;&gt;TS: Start RL Server (2)\n    TS-&gt;&gt;RL: Latest Model (3)\n\n    loop for each batch of tasks\n        loop for each task in the batch\n            AC-&gt;&gt;TS: Request Task (4)\n            TS-&gt;&gt;AC: Send Task &amp; Model API (5)\n            AC-&gt;&gt;AG: Run Agent with Task &amp; Model API (6)\n            loop for each LLM call\n                AC-&gt;&gt;AG: Prompt (7)\n                AG-&gt;&gt;AC: Response (8)\n            end\n            AG-&gt;&gt;AC: Rewarded Trace (9)\n            AC-&gt;&gt;TS: Send Rewarded Trace (10)\n        end\n        TS-&gt;&gt;RL: Send Batch of Traces (11)\n        RL-&gt;&gt;TS: Return Updated Model (12)\n    end</code></pre>"},{"location":"how-to/train-sql-agent/","title":"SQL Agent with Agent Lightning","text":"<p>This tutorial is tested with <code>verl==0.5.0</code> and <code>vllm==0.10.0</code>.</p> <p>This example demonstrates how to build and train a self-correcting SQL agent. It leverages Agent Lightning and the <code>verl</code> framework for Reinforcement Learning (RL) based training, and LangGraph to define the agent's complex, cyclical reasoning workflow. The goal is to fine-tune a Large Language Model (LLM) to accurately convert natural language questions into executable SQL queries.</p>"},{"location":"how-to/train-sql-agent/#sql-agent-implementation","title":"SQL Agent Implementation","text":"<p>The design of Agent-lightning allows flexible integration with various agent frameworks, including AutoGen, CrewAI, OpenAI Agent SDK, LangGraph, and more. It can also work without agent frameworks, allowing you to train an agent built from scratch with Python code. See our example gallery for more details.</p> <p>The core of the agent is a state machine built with LangGraph, which allows for a robust and transparent workflow. The agent's logic, as visualized below, starts by writing a query, executes it, and then enters a refinement loop where it checks and rewrites the query until it is deemed correct or a turn limit is reached.</p> <pre><code>---\nconfig:\n  flowchart:\n    curve: linear\n---\ngraph LR;\n        __start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n        write_query(write_query)\n        execute_query(execute_query)\n        check_query(check_query)\n        rewrite_query(rewrite_query)\n        __end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n        __start__ --&gt; write_query;\n        check_query -.-&gt; __end__;\n        check_query -.-&gt; rewrite_query;\n        execute_query --&gt; check_query;\n        rewrite_query --&gt; execute_query;\n        write_query --&gt; execute_query;\n        classDef default fill:#f2f2f2,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#cccccc</code></pre> <p>This workflow is implemented in the <code>SQLAgent</code> class within <code>sql_agent.py</code>. It consists of the following key steps:</p> <ol> <li>write_query: Given a user's question and database schema, the agent makes an initial attempt to write a SQL query.</li> <li>execute_query: The generated query is run against the target database.</li> <li>check_query: The agent analyzes the original query and its execution result (or error) to check for mistakes. It uses a specific prompt (<code>CHECK_QUERY_PROMPT</code>) to determine if the query is correct.</li> <li>rewrite_query: If the <code>check_query</code> step finds errors, the agent enters this step. It uses the feedback from the previous step to generate a corrected SQL query. The process then loops back to <code>check_query</code> for re-evaluation.</li> <li>END: The loop terminates when <code>check_query</code> confirms the query is correct or the maximum number of turns (<code>max_turns</code>) is exceeded. One turn corresponds to a complete cycle of <code>write_query</code> (if first round), <code>execute_query</code>, <code>check_query</code>, and potentially <code>rewrite_query</code>.</li> </ol> <p>We aim to train write_query and rewrite_query step in the setup of this example. The check_query step is not trained but will share the same LLM weights as the other steps.</p>"},{"location":"how-to/train-sql-agent/#client-server-training-with-agent-lightning","title":"Client-Server Training with Agent Lightning","text":"<p>The training process uses a distributed client-server architecture designed by Agent Lightning to efficiently fine-tune the underlying LLM. This separation allows for scalable data generation across multiple clients while centralizing the computationally intensive model training on a dedicated server with GPUs, and also provides opportunities for customizing algorithms and training strategies (like prompt optimization) with minimal code changes.</p> <ul> <li>Training Server (<code>agentlightning.verl</code>): The server, launched with the first command below, manages the core training loop. It runs an RL algorithm (with <code>verl</code> of course) and hosts an OpenAI-compatible LLM endpoint (with <code>verl</code>'s async server). The server's sole purpose is to receive interaction data from clients and update the LLM's weights to improve its performance. This link points to the implementation of the server, which is built upon <code>verl</code>.</li> <li>Agent Clients (<code>sql_agent.py</code>): The clients run the LangGraph agent logic described above. They connect to the server to fetch tasks (natural language questions) and use the server's OpenAI-compatible endpoint for all generation steps (<code>write_query</code>, <code>check_query</code>, <code>rewrite_query</code>). After completing a task, the client exports its interaction traces (traced by AgentOps and filtered by trace hierarchy), evaluates its correctness to calculate a reward, and sends the entire interaction history (the \"trajectory\") back to the server for training. To adapt any agent to an \"agent client\", you do not need to change the agent logic, but only need to invoke the client's <code>run</code> method with <code>agentlightning.trainer</code>.</li> </ul> <p></p>"},{"location":"how-to/train-sql-agent/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Prepare the dataset: download from here and unzip it to the <code>data</code> folder. It's basically a Spider V1 dataset converted to Parquet format. The dataset contains about 8000 training samples and about 2000 test samples, from which we sampled 500 samples for evaluation.    <pre><code>pip install gdown\ngdown --fuzzy https://drive.google.com/file/d/1oi9J1jZP9TyM35L85CL3qeGWl2jqlnL6/view\nunzip -q spider-data.zip -d data\nrm spider-data.zip\n</code></pre></p> </li> <li> <p>Install the required dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Launch the training server:    <code>bash    python -m agentlightning.verl \\        agentlightning.port=9997 \\        algorithm.adv_estimator=grpo \\        data.train_files=data/train_spider.parquet \\        data.val_files=data/test_dev_500.parquet \\        actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\        trainer.n_gpus_per_node=1 \\        data.train_batch_size=32 \\        actor_rollout_ref.rollout.n=4 \\        actor_rollout_ref.actor.ppo_mini_batch_size=32 \\        actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\        actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \\        actor_rollout_ref.rollout.multi_turn.format=hermes \\        actor_rollout_ref.model.path=meta-llama/Llama-3.2-3B-Instruct \\        data.max_prompt_length=4096 \\        data.max_response_length=2048 \\        data.truncation='error' \\        trainer.val_before_train=True \\        actor_rollout_ref.actor.optim.lr=1e-6 \\        actor_rollout_ref.model.use_remove_padding=True \\        actor_rollout_ref.actor.use_kl_loss=False \\        actor_rollout_ref.actor.kl_loss_coef=0.000 \\        actor_rollout_ref.actor.entropy_coeff=0 \\        actor_rollout_ref.actor.clip_ratio_low=0.2 \\        actor_rollout_ref.actor.clip_ratio_high=0.3 \\        actor_rollout_ref.model.enable_gradient_checkpointing=True \\        actor_rollout_ref.actor.fsdp_config.param_offload=True \\        actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \\        actor_rollout_ref.rollout.name=vllm \\        actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \\        actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \\        actor_rollout_ref.ref.fsdp_config.param_offload=True \\        algorithm.use_kl_in_reward=False \\        trainer.critic_warmup=0 \\        trainer.logger=['console','wandb'] \\        trainer.project_name=AgentLightning \\        trainer.experiment_name=train_sql_agent \\        trainer.nnodes=1 \\        trainer.save_freq=256 \\        trainer.test_freq=32 \\        trainer.total_epochs=2</code></p> </li> <li> <p>Launch agent clients that connect with the server:    <pre><code>export VERL_API_BASE=http://localhost:9997/  # Same as the server port. This is used for receiving tasks and sending results.\npython sql_agent.py \\\n    --litsqlagent.trained-agents write \\  # Will only train the write and rewrite agent.\n    --trainer.n-workers 16 \\\n    --litsqlagent.val-temperature 0\n</code></pre></p> </li> </ol> <p>There is no hard requirement in the launching order of the server and clients. But remember to kill the long-running agent clients after the training is done.</p>"},{"location":"how-to/train-sql-agent/#debug-the-agent-without-verl","title":"Debug the Agent without verl","text":"<p>You can run the agent client alone without the <code>verl</code> server. This is useful for debugging the agent logic and SQL execution.</p> <ol> <li> <p>Copy <code>.env.example</code> to <code>.env</code> and fill in your OpenAI API key. <code>VERL_API_BASE</code> does not really matter here because you are not connecting to the server end.</p> </li> <li> <p>Run the agent client:    <pre><code>dotenv run python sql_agent.py \\\n    --litsqlagent.trained-agents write \\  # Will only select the trajectories related to write and rewrite.\n    --trainer.n-workers 1 \\  # For debug, use single process.\n    --trainer.dev true  # Enable the dev debug mode.\n</code></pre></p> </li> </ol>"},{"location":"how-to/train-sql-agent/#evaluation","title":"Evaluation","text":"<p>The example is evaluated using Llama-3.2-Instruct models. The models are trained on the Spider dataset for 2 epochs, with evaluation performed on a randomly selected subset of 500 test samples to compute held-out accuracy. The default setup for running agent clients during evaluation is as follows:</p> <pre><code>python sql_agent.py \\\n   --litsqlagent.trained-agents write \\\n   --trainer.n-workers 16 \\\n   --trainer.daemon true \\\n   --litsqlagent.val-temperature 0 \\\n   --litsqlagent.max-turns 3 \\\n   --litsqlagent.table-info-truncate 2048 \\\n   --litsqlagent.execution-truncate 2048\n</code></pre> <p>The setup of training server is the same as the command above.</p>"},{"location":"how-to/train-sql-agent/#wb-report","title":"W&amp;B Report","text":"<p>link</p>"},{"location":"how-to/train-sql-agent/#performance-metrics","title":"Performance Metrics","text":"Model Size Context Max Turns Agents Acc (Initial) Acc (Final) Transitions Prompt Length Response Length Llama3.2 1B 2048 3 write|rewrite 21 49.6 2.87 \u2192 3.08 821.2 319.2 \u2192 249.4 Llama3.2 3B 2048 3 write|rewrite 51.8 66.4 2.20 \u2192 2.72 865.6 116.2 \u2192 314.3 <p>Notes:</p> <ol> <li>Context Length: Controlled via <code>--litsqlagent.table-info-truncate &lt;context-length&gt;</code> and <code>--litsqlagent.execution-truncate &lt;context-length&gt;</code></li> <li>Max Turns: Set using <code>--litsqlagent.max-turns &lt;max-turns&gt;</code></li> <li>Agents: Specified with <code>--litsqlagent.agents &lt;regex&gt;</code> (defaults to <code>write</code>, which matches both write and rewrite agents)</li> <li>Transitions: Represents the number of prompt-response pairs traced (collected) during each rollout. Note that this differs from the turn count in the SQL agent workflow, where one turn may encompass 2-3 transitions in the check-rewrite cycle. The number of transitions is also related to which agents get involved in the training.</li> <li>Prompt/Response Length: Average token count per traced prompt/transition response.</li> </ol>"},{"location":"how-to/train-sql-agent/#efficiency-metrics","title":"Efficiency Metrics","text":"Model Size Context Max Turns Agents # GPUs # Steps Time (h) Time/Step (s) Rollout Time (%) Update Actor Time (%) Llama3.2 1B 2048 3 write|rewrite 1 436 13.06 98.9 66.7 25.2 Llama3.2 3B 2048 3 write|rewrite 2 436 10.3 181.3 63.9 27.9"},{"location":"quickstart/getting-started/","title":"Getting Started","text":"<p>This guide walks you through building your first Agent Lightning application - a simple prompt optimization system that finds the best system prompt for an AI agent.</p>"},{"location":"quickstart/getting-started/#what-youll-build","title":"What You'll Build","text":"<p>You'll create a distributed training system with a server that manages optimization algorithms and tasks, a client with multiple workers that execute tasks in parallel, and built-in telemetry for monitoring and debugging.</p> <p>Before starting, ensure you have Python 3.10 or later, Agent Lightning installed (<code>pip install agentlightning</code>), and an OpenAI API key. The complete code is available in the examples/apo directory.</p>"},{"location":"quickstart/getting-started/#part-1-building-your-agent","title":"Part 1: Building Your Agent","text":"<p>Let's start by creating a simple agent that can answer questions using OpenAI's API. Your agent needs to inherit from <code>LitAgent</code> and implement a <code>training_rollout</code> method.</p>"},{"location":"quickstart/getting-started/#step-1-create-your-agent-class","title":"Step 1: Create Your Agent Class","text":"<p>First, import the necessary dependencies and create your agent class:</p> <pre><code>from agentlightning.litagent import LitAgent\n\nclass SimpleAgent(LitAgent):\n    def training_rollout(self, task, rollout_id, resources):\n        \"\"\"Execute a single training rollout.\"\"\"\n</code></pre> <p>The <code>training_rollout</code> method is the heart of your agent. It receives three parameters: a <code>task</code> dictionary containing the work to do (like \"What is the capital of France?\"), a unique <code>rollout_id</code> for tracking this execution, and <code>resources</code> from the server - in our case, the system prompt we're testing.</p>"},{"location":"quickstart/getting-started/#step-2-execute-the-task","title":"Step 2: Execute the Task","text":"<p>Inside the training_rollout method, extract the system prompt from resources and use it to complete the task:</p> <pre><code># Extract the system prompt being tested\nsystem_prompt = resources[\"system_prompt\"].template\n\n# Call OpenAI with this prompt\nresult = openai.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": task[\"prompt\"]},\n    ],\n)\n</code></pre> <p>The server sends different prompts to test, and your agent uses each one to answer the same question. This lets us compare which prompt works best.</p>"},{"location":"quickstart/getting-started/#step-3-return-a-reward-score","title":"Step 3: Return a Reward Score","text":"<p>After executing the task, return a reward score between 0 and 1:</p> <pre><code># In real scenarios, calculate based on response quality\nreturn random.uniform(0, 1)\n</code></pre> <p>Higher rewards mean better performance. In a real system, you'd calculate this with rules, or even an LLM as a judge. For now, we're using random values to demonstrate the flow.</p>"},{"location":"quickstart/getting-started/#step-4-set-up-the-trainer","title":"Step 4: Set Up the Trainer","text":"<p>To run your agent with multiple workers in parallel:</p> <pre><code>from agentlightning.trainer import Trainer\n\nagent = SimpleAgent()\ntrainer = Trainer(n_workers=2)  # Create 2 parallel workers\ntrainer.fit(agent, \"http://127.0.0.1:9997\")\n</code></pre> <p>The trainer creates separate processes for each worker, allowing them to execute tasks independently. This parallelization significantly speeds up the optimization process - with 2 workers, you can test prompts twice as fast.</p>"},{"location":"quickstart/getting-started/#part-2-building-the-optimization-server","title":"Part 2: Building the Optimization Server","text":"<p>The server coordinates the training process and implements your optimization algorithm. It manages resources, distributes tasks, and collects results.</p>"},{"location":"quickstart/getting-started/#step-1-initialize-the-server","title":"Step 1: Initialize the Server","text":"<p>Create an async function to run your optimization:</p> <pre><code>import asyncio\nfrom agentlightning.server import AgentLightningServer\nfrom agentlightning.types import PromptTemplate\n\nasync def prompt_optimization():\n    server = AgentLightningServer(host=\"127.0.0.1\", port=9997)\n    await server.start()\n</code></pre> <p>We use async/await because the server handles multiple clients simultaneously. This allows it to queue tasks without blocking and process results as they arrive from different workers.</p>"},{"location":"quickstart/getting-started/#step-2-test-different-prompts","title":"Step 2: Test Different Prompts","text":"<p>Define the prompts you want to test and iterate through them:</p> <pre><code>prompt_candidates = [\n    \"You are a helpful assistant.\",\n    \"You are a knowledgeable AI.\",\n    \"You are a friendly chatbot.\",\n]\n\nfor prompt in prompt_candidates:\n    # Send this prompt to all connected clients\n    resources = {\n        \"system_prompt\": PromptTemplate(template=prompt, engine=\"f-string\")\n    }\n    await server.update_resources(resources)\n</code></pre> <p>When you update resources, all connected clients immediately receive the new system prompt. The format of resources can be arbitrary. We use the key <code>\"system_prompt\"</code> here as an example. The resources here are exactly you would expect at the client side, who will use this prompt for the next task they process.</p>"},{"location":"quickstart/getting-started/#step-3-queue-tasks-and-collect-results","title":"Step 3: Queue Tasks and Collect Results","text":"<p>For each prompt, queue a task and wait for results. The <code>{\"prompt\": ...}</code> format here is exact what you would expect from the client side code.</p> <pre><code># Queue a task for clients to process\ntask_id = await server.queue_task(\n    sample={\"prompt\": \"What is the capital of France?\"},\n    mode=\"train\"\n)\n\n# Wait for a client to complete it (30 second timeout)\nrollout = await server.poll_completed_rollout(task_id, timeout=30)\n\n# Extract and store the reward (this comes from the return value of the client side)\nreward = rollout.final_reward\n</code></pre> <p>The server queues the same question for each prompt. The rollout object contains not just the reward, but also detailed telemetry and trace information for debugging and optimization.</p>"},{"location":"quickstart/getting-started/#step-4-find-the-best-prompt","title":"Step 4: Find the Best Prompt","text":"<p>After testing all candidates, identify the winner:</p> <pre><code>best_prompt = max(prompt_and_rewards, key=lambda x: x[1])\nprint(f\"Best prompt: '{best_prompt[0]}' with reward {best_prompt[1]:.3f}\")\n</code></pre>"},{"location":"quickstart/getting-started/#running-your-system","title":"Running Your System","text":"<p>The Complete example code can be found on the GitHub repository. To run it:</p> <p>Create a <code>.env</code> file with your API credentials:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\nOPENAI_API_BASE=https://api.openai.com/v1  # Optional\n</code></pre> <p>Start the server first in one terminal: <pre><code>python server.py\n</code></pre></p> <p>Then start the client in another terminal: <pre><code>python client.py\n</code></pre></p>"},{"location":"quickstart/getting-started/#understanding-the-output","title":"Understanding the Output","text":"<p>When you run the system, you'll see detailed logs from both the client and server. Understanding these logs helps you debug issues and optimize performance.</p>"},{"location":"quickstart/getting-started/#client-output-explained","title":"Client Output Explained","text":"<p><pre><code>2025-08-10 12:59:38,224 [INFO] Initializing Trainer...\n</code></pre> The trainer is starting up and preparing to create workers.</p> <p><pre><code>[INFO] Starting AgentOps local server on port 52081...\n</code></pre> A local telemetry server starts to collect metrics and traces. You can access this at <code>http://localhost:52081</code> to see detailed execution traces.</p> <p><pre><code>[INFO] Starting worker process 0...\n[INFO] Starting worker process 1...\n</code></pre> Two separate processes are created. Each can execute tasks independently, doubling your throughput.</p> <p><pre><code>[INFO] [Task 1 Received] ID: rollout-c1eb987b...\nResources: {'system_prompt': PromptTemplate(...)}\n</code></pre> A worker receives a task from the server along with the current prompt to test.</p> <p><pre><code>[INFO] [Worker 0 | Rollout] Completed in 1.09s. Reward: 0.631\n</code></pre> Worker 0 finished executing the task in 1.09 seconds and calculated a reward of 0.631. This tells you both performance (execution time) and quality (reward score).</p>"},{"location":"quickstart/getting-started/#server-output-explained","title":"Server Output Explained","text":"<p><pre><code>[Algo] Testing prompt: 'You are a helpful assistant.'\n</code></pre> The optimization algorithm selects the next prompt to test.</p> <p><pre><code>[Algo] Task 'rollout-c1eb987b...' is now available for clients.\n</code></pre> The task is queued and waiting for an available worker to pick it up.</p> <p><pre><code>[Algo] Received reward: 0.631\n</code></pre> A client completed the task and returned a performance score. The server uses this to compare prompts.</p> <p><pre><code>[Algo] Best prompt: 'You are a knowledgeable AI.' (reward: 0.925)\n</code></pre> After testing all prompts, the server identifies which one performed best.</p>"},{"location":"quickstart/getting-started/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes","text":"<p>Agent Lightning handles several complex operations automatically. Multiple workers process tasks simultaneously. Every API call and execution is tracked through the telemetry tracing system, providing detailed traces for debugging and optimization. If a worker fails, others continue processing, ensuring your optimization doesn't stop.</p> <p>The AgentOps tracer, enabled by default, collects comprehensive data about each execution, including API calls, timing information, token usage, and error traces. The data is sent to the server and can be accessed via <code>rollout.triplets</code> and <code>rollout.traces</code> at the server side to build more advanced automatic optimization algorithms.</p>"},{"location":"quickstart/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a working system, how about:</p> <ul> <li>Replacing the random reward with actual quality metrics based on real response accuracy?</li> <li>Testing the system prompt on a batch of different questions to see how it performs across various tasks?</li> <li>Making the algorithm automatically improve the system prompt based on the best-performing ones?</li> <li>Setting up a real agent system that consists of multiple prompts, and optimizing them together?</li> </ul>"},{"location":"quickstart/installation/","title":"Installation","text":""},{"location":"quickstart/installation/#install-from-pypi","title":"Install from PyPI","text":""},{"location":"quickstart/installation/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>We strongly recommend creating a new virtual environment to avoid conflicts with other packages. You can use either <code>conda</code> or <code>venv</code>. Python 3.10 or later is recommended.</p>"},{"location":"quickstart/installation/#install-core-training-dependencies-optional","title":"Install Core Training Dependencies (Optional)","text":"<p>If you are running RL with Agent-Lightning, the next step is to install the essential packages: <code>PyTorch</code>, <code>FlashAttention</code>, <code>vLLM</code> and <code>VERL</code>. The following versions and installation order have been tested and are confirmed to work.</p> <pre><code>pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install flash-attn --no-build-isolation\npip install vllm==0.9.2\npip install verl==0.5.0\n</code></pre> <p>See this script for a full installation script.</p>"},{"location":"quickstart/installation/#install-agent-lightning","title":"Install Agent Lightning","text":"<p>Now, you're ready to install Agent Lightning itself.</p> <pre><code>pip install agentlightning\n</code></pre>"},{"location":"quickstart/installation/#install-agent-frameworks-optional","title":"Install Agent Frameworks (Optional)","text":"<p>If you plan to use other agent frameworks, you can install them with the following commands. If you don't need these, feel free to skip this step. We recommend doing this as the final step to avoid dependency versions being overwritten by mistake.</p> <pre><code># AutoGen (Recommended to install first)\npip install \"autogen-agentchat\" \"autogen-ext[openai]\"\n\n# LiteLLM\npip install \"litellm[proxy]\"\n\n# MCP\npip install mcp\n\n# UV\npip install uv\n\n# OpenAI Agents\npip install openai-agents\n\n# LangChain\npip install langgraph \"langchain[openai]\" langchain-community langchain-text-splitters\n\n# SQL-related dependencies\npip install sqlparse nltk\n</code></pre>"},{"location":"quickstart/installation/#shortcuts-for-installing-extra-dependencies","title":"Shortcuts for installing Extra Dependencies","text":"<p>For development: <pre><code>pip install agentlightning[dev]\n</code></pre></p> <p>For agent support: <pre><code>pip install agentlightning[agent]\n</code></pre></p>"},{"location":"quickstart/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/microsoft/agent-lightning\ncd agent-lightning\npip install -e .[dev]\n</code></pre> <p>Please run pre-commit hooks before checking in code:</p> <pre><code>pre-commit install\npre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre>"},{"location":"reference/core/","title":"Agent Lightning Core","text":""},{"location":"reference/core/#client-side","title":"Client Side","text":""},{"location":"reference/core/#agentlightning.litagent","title":"<code>agentlightning.litagent</code>","text":""},{"location":"reference/core/#agentlightning.litagent.LitAgent","title":"<code>LitAgent</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Base class for the training and validation logic of an agent.</p> <p>Developers should subclass this class and implement the rollout methods to define the agent's behavior for a single task. The agent's logic is completely decoupled from the server communication and training infrastructure.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>class LitAgent(Generic[T]):\n    \"\"\"Base class for the training and validation logic of an agent.\n\n    Developers should subclass this class and implement the rollout methods\n    to define the agent's behavior for a single task. The agent's logic\n    is completely decoupled from the server communication and training\n    infrastructure.\n    \"\"\"\n\n    def __init__(self, *, trained_agents: Optional[str] = None) -&gt; None:  # FIXME: str | None won't work for cli\n        \"\"\"\n        Initialize the LitAgent.\n\n        Args:\n            trained_agents: Optional string representing the trained agents.\n                            This can be used to track which agents have been trained by this instance.\n                            Deprecated. Configure `agent_match` in adapter instead.\n        \"\"\"\n        if trained_agents is not None:\n            warnings.warn(\n                \"`trained_agents` is deprecated. Configure `agent_match` in adapter instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self.trained_agents = trained_agents\n\n        self._trainer_ref: weakref.ReferenceType[Trainer] | None = None\n        self._runner_ref: weakref.ReferenceType[BaseRunner[T]] | None = None\n\n    @property\n    def is_async(self) -&gt; bool:\n        \"\"\"\n        Check if the agent implements asynchronous rollout methods.\n        Override this property for customized async detection logic.\n\n        Returns:\n            True if the agent has custom async rollout methods, False otherwise.\n        \"\"\"\n        return (\n            (\n                hasattr(self, \"training_rollout_async\")\n                and self.__class__.training_rollout_async is not LitAgent.training_rollout_async  # type: ignore\n            )\n            or (\n                hasattr(self, \"validation_rollout_async\")\n                and self.__class__.validation_rollout_async is not LitAgent.validation_rollout_async  # type: ignore\n            )\n            or (hasattr(self, \"rollout_async\") and self.__class__.rollout_async is not LitAgent.rollout_async)  # type: ignore\n        )\n\n    def set_trainer(self, trainer: Trainer) -&gt; None:\n        \"\"\"\n        Set the trainer for this agent.\n\n        Args:\n            trainer: The Trainer instance that will handle training and validation.\n        \"\"\"\n        self._trainer_ref = weakref.ref(trainer)\n\n    @property\n    def trainer(self) -&gt; Trainer:\n        \"\"\"\n        Get the trainer for this agent.\n\n        Returns:\n            The Trainer instance associated with this agent.\n        \"\"\"\n        if self._trainer_ref is None:\n            raise ValueError(\"Trainer has not been set for this agent.\")\n        trainer = self._trainer_ref()\n        if trainer is None:\n            raise ValueError(\"Trainer reference is no longer valid (object has been garbage collected).\")\n        return trainer\n\n    @property\n    def tracer(self) -&gt; BaseTracer:\n        \"\"\"\n        Get the tracer for this agent.\n\n        Returns:\n            The BaseTracer instance associated with this agent.\n        \"\"\"\n        return self.trainer.tracer\n\n    def set_runner(self, runner: BaseRunner[T]) -&gt; None:\n        \"\"\"\n        Set the runner for this agent.\n\n        Args:\n            runner: The runner instance that will handle the execution of rollouts.\n        \"\"\"\n        self._runner_ref = weakref.ref(runner)\n\n    @property\n    def runner(self) -&gt; BaseRunner[T]:\n        \"\"\"\n        Get the runner for this agent.\n\n        Returns:\n            The runner instance associated with this agent.\n        \"\"\"\n        if self._runner_ref is None:\n            raise ValueError(\"Runner has not been set for this agent.\")\n        runner = self._runner_ref()\n        if runner is None:\n            raise ValueError(\"Runner reference is no longer valid (object has been garbage collected).\")\n        return runner\n\n    def on_rollout_start(self, task: Task, runner: BaseRunner[T], tracer: BaseTracer) -&gt; None:\n        \"\"\"Hook called immediately before a rollout begins.\n\n        Deprecated in favor of `on_rollout_start` in the `Hook` interface.\n\n        Args:\n            task: The :class:`Task` object that will be processed.\n            runner: The :class:`BaseRunner` managing the rollout.\n            tracer: The tracer instance associated with the runner.\n\n        Subclasses can override this method to implement custom logic such as\n        logging, metric collection, or resource setup. By default, this is a\n        no-op.\n        \"\"\"\n\n    def on_rollout_end(self, task: Task, rollout: RolloutV2, runner: BaseRunner[T], tracer: BaseTracer) -&gt; None:\n        \"\"\"Hook called after a rollout completes.\n\n        Deprecated in favor of `on_rollout_end` in the `Hook` interface.\n\n        Args:\n            task: The :class:`Task` object that was processed.\n            rollout: The resulting :class:`Rollout` object.\n            runner: The :class:`BaseRunner` managing the rollout.\n            tracer: The tracer instance associated with the runner.\n\n        Subclasses can override this method for cleanup or additional\n        logging. By default, this is a no-op.\n        \"\"\"\n\n    def rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Main entry point for executing a rollout.\n\n        This method determines whether to call the synchronous or\n        asynchronous rollout method based on the agent's implementation.\n\n        If you don't wish to implement both training rollout and validation\n        rollout separately, you can just implement `rollout` which will work for both.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n                     Most agents should only use `task` and `resources`. Use `rollout`\n                     only if you need to access metadata like `rollout_id`.\n\n        Returns:\n            The result of the rollout, which can be one of:\n            - None. The tracing should be handled by the agent runner.\n            - A float representing the final reward.\n            - A list of `Triplet` objects for detailed, step-by-step feedback.\n            - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n            - A list of dictionaries for any trace spans.\n            - A complete `Rollout` object for full control over reporting.\n        \"\"\"\n        raise NotImplementedError(\"Agents must implement the `rollout` method.\")\n\n    async def rollout_async(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Asynchronous version of the main rollout method.\n\n        This method determines whether to call the synchronous or\n        asynchronous rollout method based on the agent's implementation.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n                     Most agents should only use `task` and `resources`. Use `rollout`\n                     only if you need to access metadata like `rollout_id`.\n\n        Returns:\n            The result of the rollout, which can be one of:\n            - None. The tracing should be handled by the agent runner.\n            - A float representing the final reward.\n            - A list of `Triplet` objects for detailed, step-by-step feedback.\n            - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n            - A list of dictionaries for any trace spans.\n            - A complete `Rollout` object for full control over reporting.\n        \"\"\"\n        raise NotImplementedError(\"Agents must implement the `rollout_async` method for async operations.\")\n\n    def training_rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Defines the agent's behavior for a single training task.\n\n        This method should contain the logic for how the agent processes an\n        input, uses the provided resources (like LLMs or prompts), and\n        produces a result.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources (e.g., LLMs, prompt\n                       templates) for the agent to use.\n            rollout: The full rollout object, please avoid from directly modifying it.\n        \"\"\"\n        return self.rollout(task, resources, rollout)\n\n    def validation_rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Defines the agent's behavior for a single validation task.\n\n        By default, this method redirects to `training_rollout`. Override it\n        if the agent should behave differently during validation.\n\n        Args:\n            task: The task object received from the server, containing the\n                  input data and metadata.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the validation rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return self.rollout(task, resources, rollout)\n\n    async def training_rollout_async(\n        self, task: T, resources: NamedResources, rollout: RolloutV2\n    ) -&gt; RolloutRawResultV2:\n        \"\"\"Asynchronous version of `training_rollout`.\n\n        This method should be implemented by agents that perform asynchronous\n        operations (e.g., non-blocking I/O, concurrent API calls).\n\n        Args:\n            task: The task object received from the server.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the asynchronous training rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return await self.rollout_async(task, resources, rollout)\n\n    async def validation_rollout_async(\n        self, task: T, resources: NamedResources, rollout: RolloutV2\n    ) -&gt; RolloutRawResultV2:\n        \"\"\"Asynchronous version of `validation_rollout`.\n\n        By default, this method redirects to `training_rollout_async`.\n        Override it for different asynchronous validation behavior.\n\n        Args:\n            task: The task object received from the server.\n            resources: A dictionary of named resources for the agent to use.\n            rollout: The full rollout object, avoid from modifying it.\n\n        Returns:\n            The result of the asynchronous validation rollout. See `rollout` for\n            possible return types.\n        \"\"\"\n        return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.is_async","title":"<code>is_async</code>  <code>property</code>","text":"<p>Check if the agent implements asynchronous rollout methods. Override this property for customized async detection logic.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the agent has custom async rollout methods, False otherwise.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.runner","title":"<code>runner</code>  <code>property</code>","text":"<p>Get the runner for this agent.</p> <p>Returns:</p> Type Description <code>BaseRunner[T]</code> <p>The runner instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Get the tracer for this agent.</p> <p>Returns:</p> Type Description <code>BaseTracer</code> <p>The BaseTracer instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.trainer","title":"<code>trainer</code>  <code>property</code>","text":"<p>Get the trainer for this agent.</p> <p>Returns:</p> Type Description <code>Trainer</code> <p>The Trainer instance associated with this agent.</p>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.__init__","title":"<code>__init__(*, trained_agents=None)</code>","text":"<p>Initialize the LitAgent.</p> <p>Parameters:</p> Name Type Description Default <code>trained_agents</code> <code>Optional[str]</code> <p>Optional string representing the trained agents.             This can be used to track which agents have been trained by this instance.             Deprecated. Configure <code>agent_match</code> in adapter instead.</p> <code>None</code> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __init__(self, *, trained_agents: Optional[str] = None) -&gt; None:  # FIXME: str | None won't work for cli\n    \"\"\"\n    Initialize the LitAgent.\n\n    Args:\n        trained_agents: Optional string representing the trained agents.\n                        This can be used to track which agents have been trained by this instance.\n                        Deprecated. Configure `agent_match` in adapter instead.\n    \"\"\"\n    if trained_agents is not None:\n        warnings.warn(\n            \"`trained_agents` is deprecated. Configure `agent_match` in adapter instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    self.trained_agents = trained_agents\n\n    self._trainer_ref: weakref.ReferenceType[Trainer] | None = None\n    self._runner_ref: weakref.ReferenceType[BaseRunner[T]] | None = None\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.on_rollout_end","title":"<code>on_rollout_end(task, rollout, runner, tracer)</code>","text":"<p>Hook called after a rollout completes.</p> <p>Deprecated in favor of <code>on_rollout_end</code> in the <code>Hook</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The :class:<code>Task</code> object that was processed.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The resulting :class:<code>Rollout</code> object.</p> required <code>runner</code> <code>BaseRunner[T]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The tracer instance associated with the runner.</p> required <p>Subclasses can override this method for cleanup or additional logging. By default, this is a no-op.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def on_rollout_end(self, task: Task, rollout: RolloutV2, runner: BaseRunner[T], tracer: BaseTracer) -&gt; None:\n    \"\"\"Hook called after a rollout completes.\n\n    Deprecated in favor of `on_rollout_end` in the `Hook` interface.\n\n    Args:\n        task: The :class:`Task` object that was processed.\n        rollout: The resulting :class:`Rollout` object.\n        runner: The :class:`BaseRunner` managing the rollout.\n        tracer: The tracer instance associated with the runner.\n\n    Subclasses can override this method for cleanup or additional\n    logging. By default, this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.on_rollout_start","title":"<code>on_rollout_start(task, runner, tracer)</code>","text":"<p>Hook called immediately before a rollout begins.</p> <p>Deprecated in favor of <code>on_rollout_start</code> in the <code>Hook</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The :class:<code>Task</code> object that will be processed.</p> required <code>runner</code> <code>BaseRunner[T]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The tracer instance associated with the runner.</p> required <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def on_rollout_start(self, task: Task, runner: BaseRunner[T], tracer: BaseTracer) -&gt; None:\n    \"\"\"Hook called immediately before a rollout begins.\n\n    Deprecated in favor of `on_rollout_start` in the `Hook` interface.\n\n    Args:\n        task: The :class:`Task` object that will be processed.\n        runner: The :class:`BaseRunner` managing the rollout.\n        tracer: The tracer instance associated with the runner.\n\n    Subclasses can override this method to implement custom logic such as\n    logging, metric collection, or resource setup. By default, this is a\n    no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Main entry point for executing a rollout.</p> <p>This method determines whether to call the synchronous or asynchronous rollout method based on the agent's implementation.</p> <p>If you don't wish to implement both training rollout and validation rollout separately, you can just implement <code>rollout</code> which will work for both.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, please avoid from directly modifying it.      Most agents should only use <code>task</code> and <code>resources</code>. Use <code>rollout</code>      only if you need to access metadata like <code>rollout_id</code>.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result of the rollout, which can be one of:</p> <code>RolloutRawResultV2</code> <ul> <li>None. The tracing should be handled by the agent runner.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A float representing the final reward.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of <code>Triplet</code> objects for detailed, step-by-step feedback.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of <code>ReadableSpan</code> objects for OpenTelemetry tracing.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of dictionaries for any trace spans.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A complete <code>Rollout</code> object for full control over reporting.</li> </ul> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Main entry point for executing a rollout.\n\n    This method determines whether to call the synchronous or\n    asynchronous rollout method based on the agent's implementation.\n\n    If you don't wish to implement both training rollout and validation\n    rollout separately, you can just implement `rollout` which will work for both.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n                 Most agents should only use `task` and `resources`. Use `rollout`\n                 only if you need to access metadata like `rollout_id`.\n\n    Returns:\n        The result of the rollout, which can be one of:\n        - None. The tracing should be handled by the agent runner.\n        - A float representing the final reward.\n        - A list of `Triplet` objects for detailed, step-by-step feedback.\n        - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n        - A list of dictionaries for any trace spans.\n        - A complete `Rollout` object for full control over reporting.\n    \"\"\"\n    raise NotImplementedError(\"Agents must implement the `rollout` method.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of the main rollout method.</p> <p>This method determines whether to call the synchronous or asynchronous rollout method based on the agent's implementation.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, please avoid from directly modifying it.      Most agents should only use <code>task</code> and <code>resources</code>. Use <code>rollout</code>      only if you need to access metadata like <code>rollout_id</code>.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result of the rollout, which can be one of:</p> <code>RolloutRawResultV2</code> <ul> <li>None. The tracing should be handled by the agent runner.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A float representing the final reward.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of <code>Triplet</code> objects for detailed, step-by-step feedback.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of <code>ReadableSpan</code> objects for OpenTelemetry tracing.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A list of dictionaries for any trace spans.</li> </ul> <code>RolloutRawResultV2</code> <ul> <li>A complete <code>Rollout</code> object for full control over reporting.</li> </ul> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def rollout_async(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Asynchronous version of the main rollout method.\n\n    This method determines whether to call the synchronous or\n    asynchronous rollout method based on the agent's implementation.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n                 Most agents should only use `task` and `resources`. Use `rollout`\n                 only if you need to access metadata like `rollout_id`.\n\n    Returns:\n        The result of the rollout, which can be one of:\n        - None. The tracing should be handled by the agent runner.\n        - A float representing the final reward.\n        - A list of `Triplet` objects for detailed, step-by-step feedback.\n        - A list of `ReadableSpan` objects for OpenTelemetry tracing.\n        - A list of dictionaries for any trace spans.\n        - A complete `Rollout` object for full control over reporting.\n    \"\"\"\n    raise NotImplementedError(\"Agents must implement the `rollout_async` method for async operations.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.set_runner","title":"<code>set_runner(runner)</code>","text":"<p>Set the runner for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>BaseRunner[T]</code> <p>The runner instance that will handle the execution of rollouts.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def set_runner(self, runner: BaseRunner[T]) -&gt; None:\n    \"\"\"\n    Set the runner for this agent.\n\n    Args:\n        runner: The runner instance that will handle the execution of rollouts.\n    \"\"\"\n    self._runner_ref = weakref.ref(runner)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Set the trainer for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance that will handle training and validation.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def set_trainer(self, trainer: Trainer) -&gt; None:\n    \"\"\"\n    Set the trainer for this agent.\n\n    Args:\n        trainer: The Trainer instance that will handle training and validation.\n    \"\"\"\n    self._trainer_ref = weakref.ref(trainer)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.training_rollout","title":"<code>training_rollout(task, resources, rollout)</code>","text":"<p>Defines the agent's behavior for a single training task.</p> <p>This method should contain the logic for how the agent processes an input, uses the provided resources (like LLMs or prompts), and produces a result.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources (e.g., LLMs, prompt        templates) for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, please avoid from directly modifying it.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def training_rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Defines the agent's behavior for a single training task.\n\n    This method should contain the logic for how the agent processes an\n    input, uses the provided resources (like LLMs or prompts), and\n    produces a result.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources (e.g., LLMs, prompt\n                   templates) for the agent to use.\n        rollout: The full rollout object, please avoid from directly modifying it.\n    \"\"\"\n    return self.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.training_rollout_async","title":"<code>training_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of <code>training_rollout</code>.</p> <p>This method should be implemented by agents that perform asynchronous operations (e.g., non-blocking I/O, concurrent API calls).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result of the asynchronous training rollout. See <code>rollout</code> for</p> <code>RolloutRawResultV2</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def training_rollout_async(\n    self, task: T, resources: NamedResources, rollout: RolloutV2\n) -&gt; RolloutRawResultV2:\n    \"\"\"Asynchronous version of `training_rollout`.\n\n    This method should be implemented by agents that perform asynchronous\n    operations (e.g., non-blocking I/O, concurrent API calls).\n\n    Args:\n        task: The task object received from the server.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the asynchronous training rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.validation_rollout","title":"<code>validation_rollout(task, resources, rollout)</code>","text":"<p>Defines the agent's behavior for a single validation task.</p> <p>By default, this method redirects to <code>training_rollout</code>. Override it if the agent should behave differently during validation.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server, containing the   input data and metadata.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result of the validation rollout. See <code>rollout</code> for</p> <code>RolloutRawResultV2</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def validation_rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Defines the agent's behavior for a single validation task.\n\n    By default, this method redirects to `training_rollout`. Override it\n    if the agent should behave differently during validation.\n\n    Args:\n        task: The task object received from the server, containing the\n              input data and metadata.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the validation rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return self.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgent.validation_rollout_async","title":"<code>validation_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Asynchronous version of <code>validation_rollout</code>.</p> <p>By default, this method redirects to <code>training_rollout_async</code>. Override it for different asynchronous validation behavior.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task object received from the server.</p> required <code>resources</code> <code>NamedResources</code> <p>A dictionary of named resources for the agent to use.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The full rollout object, avoid from modifying it.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result of the asynchronous validation rollout. See <code>rollout</code> for</p> <code>RolloutRawResultV2</code> <p>possible return types.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def validation_rollout_async(\n    self, task: T, resources: NamedResources, rollout: RolloutV2\n) -&gt; RolloutRawResultV2:\n    \"\"\"Asynchronous version of `validation_rollout`.\n\n    By default, this method redirects to `training_rollout_async`.\n    Override it for different asynchronous validation behavior.\n\n    Args:\n        task: The task object received from the server.\n        resources: A dictionary of named resources for the agent to use.\n        rollout: The full rollout object, avoid from modifying it.\n\n    Returns:\n        The result of the asynchronous validation rollout. See `rollout` for\n        possible return types.\n    \"\"\"\n    return await self.rollout_async(task, resources, rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM","title":"<code>LitAgentLLM</code>","text":"<p>               Bases: <code>LitAgent[T]</code></p> <p>A specialized LitAgent that wraps a function-based rollout that accepts dynamically a task input and a configured LLM.</p> <p>This class allows users to define agent behavior using a simple function that takes task input and an LLM resource, rather than implementing a full LitAgent subclass.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>class LitAgentLLM(LitAgent[T]):\n    \"\"\"A specialized LitAgent that wraps a function-based rollout that accepts\n    dynamically a task input and a configured LLM.\n\n    This class allows users to define agent behavior using a simple function\n    that takes task input and an LLM resource, rather than implementing a full\n    LitAgent subclass.\n    \"\"\"\n\n    def __init__(self, llm_rollout_func: LlmRolloutFunc[T], *, strip_proxy: bool = True) -&gt; None:\n        \"\"\"\n        Initialize the LitAgentLLM with an LLM rollout function.\n\n        Args:\n            llm_rollout_func: A function that defines the agent's behavior.\n                              Can be sync or async, and can optionally accept a Rollout parameter.\n            strip_proxy: Whether to strip the ProxyLLM resource into a LLM resource.\n        \"\"\"\n        super().__init__()\n        self.llm_rollout_func = llm_rollout_func\n        self.strip_proxy = strip_proxy\n        self._is_async = inspect.iscoroutinefunction(llm_rollout_func)\n        self._accepts_rollout = \"rollout\" in inspect.signature(llm_rollout_func).parameters\n\n        # Copy function metadata to preserve type hints and other attributes\n        functools.update_wrapper(self, llm_rollout_func)  # type: ignore\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Make the agent instance callable, preserving the original function behavior.\"\"\"\n        return self.llm_rollout_func(*args, **kwargs)  # type: ignore\n\n    @property\n    def is_async(self) -&gt; bool:\n        return self._is_async\n\n    def rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Execute a synchronous rollout using the wrapped function.\n\n        Args:\n            task: The task input data.\n            resources: Dictionary of named resources including LLMs.\n            rollout: The rollout object with metadata.\n\n        Returns:\n            The result from the wrapped rollout function.\n        \"\"\"\n        if self._is_async:\n            raise RuntimeError(\"This LitAgentLLM uses an async function. Use rollout_async instead.\")\n\n        # Find the first LLM resource\n        llm = self._get_llm_resource(resources)\n\n        # Strip ProxyLLM if needed\n        if self.strip_proxy:\n            llm = self._strip_proxy(llm, rollout)\n\n        if self._accepts_rollout:\n            llm_rollout_func = cast(LlmRolloutFuncSync3[T], self.llm_rollout_func)\n            return llm_rollout_func(task, llm=llm, rollout=rollout)\n        else:\n            llm_rollout_func = cast(LlmRolloutFuncSync2[T], self.llm_rollout_func)\n            return llm_rollout_func(task, llm=llm)\n\n    async def rollout_async(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n        \"\"\"Execute an asynchronous rollout using the wrapped function.\n\n        Args:\n            task: The task input data.\n            resources: Dictionary of named resources including LLMs.\n            rollout: The rollout object with metadata.\n\n        Returns:\n            The result from the wrapped rollout function.\n        \"\"\"\n        if not self._is_async:\n            raise RuntimeError(\"This LitAgentLLM uses a sync function. Use rollout instead.\")\n\n        # Find the first LLM resource\n        llm = self._get_llm_resource(resources)\n\n        # Strip ProxyLLM if needed\n        if self.strip_proxy:\n            llm = self._strip_proxy(llm, rollout)\n\n        if self._accepts_rollout:\n            llm_rollout_func = cast(LlmRolloutFuncAsync3[T], self.llm_rollout_func)\n            return await llm_rollout_func(task, llm=llm, rollout=rollout)\n        else:\n            llm_rollout_func = cast(LlmRolloutFuncAsync2[T], self.llm_rollout_func)\n            return await llm_rollout_func(task, llm=llm)\n\n    def _get_llm_resource(self, resources: NamedResources) -&gt; LLM:\n        \"\"\"Extract the first LLM resource from the resources dictionary.\n\n        Args:\n            resources: Dictionary of named resources.\n\n        Returns:\n            The first LLM resource found.\n\n        Raises:\n            ValueError: If no LLM resource is found.\n        \"\"\"\n        resource_found: LLM | None = None\n        for name, resource in resources.items():\n            if isinstance(resource, LLM):\n                if resource_found is not None:\n                    logger.warning(f\"Multiple LLM resources found in resources. Using the first one: '{name}'.\")\n                    break\n                resource_found = resource\n\n        if resource_found is None:\n            raise ValueError(\"No LLM resource found in the provided resources.\")\n        return resource_found\n\n    def _strip_proxy(self, proxy_llm: LLM, rollout: RolloutV2) -&gt; LLM:\n        \"\"\"Strip the ProxyLLM resource into a LLM resource.\"\"\"\n\n        if not isinstance(proxy_llm, ProxyLLM):\n            # Not a ProxyLLM, nothing to strip here.\n            return proxy_llm\n\n        # Rollout is still a RolloutV2 here because API is not stabilized yet.\n        # In practice, it must be an AttemptedRollout.\n        if not isinstance(rollout, AttemptedRollout):\n            raise ValueError(\"Rollout is not an AttemptedRollout.\")\n\n        return proxy_llm.with_attempted_rollout(rollout)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Make the agent instance callable, preserving the original function behavior.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Make the agent instance callable, preserving the original function behavior.\"\"\"\n    return self.llm_rollout_func(*args, **kwargs)  # type: ignore\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.__init__","title":"<code>__init__(llm_rollout_func, *, strip_proxy=True)</code>","text":"<p>Initialize the LitAgentLLM with an LLM rollout function.</p> <p>Parameters:</p> Name Type Description Default <code>llm_rollout_func</code> <code>LlmRolloutFunc[T]</code> <p>A function that defines the agent's behavior.               Can be sync or async, and can optionally accept a Rollout parameter.</p> required <code>strip_proxy</code> <code>bool</code> <p>Whether to strip the ProxyLLM resource into a LLM resource.</p> <code>True</code> Source code in <code>agentlightning/litagent.py</code> <pre><code>def __init__(self, llm_rollout_func: LlmRolloutFunc[T], *, strip_proxy: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the LitAgentLLM with an LLM rollout function.\n\n    Args:\n        llm_rollout_func: A function that defines the agent's behavior.\n                          Can be sync or async, and can optionally accept a Rollout parameter.\n        strip_proxy: Whether to strip the ProxyLLM resource into a LLM resource.\n    \"\"\"\n    super().__init__()\n    self.llm_rollout_func = llm_rollout_func\n    self.strip_proxy = strip_proxy\n    self._is_async = inspect.iscoroutinefunction(llm_rollout_func)\n    self._accepts_rollout = \"rollout\" in inspect.signature(llm_rollout_func).parameters\n\n    # Copy function metadata to preserve type hints and other attributes\n    functools.update_wrapper(self, llm_rollout_func)  # type: ignore\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a synchronous rollout using the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task input data.</p> required <code>resources</code> <code>NamedResources</code> <p>Dictionary of named resources including LLMs.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The rollout object with metadata.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result from the wrapped rollout function.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Execute a synchronous rollout using the wrapped function.\n\n    Args:\n        task: The task input data.\n        resources: Dictionary of named resources including LLMs.\n        rollout: The rollout object with metadata.\n\n    Returns:\n        The result from the wrapped rollout function.\n    \"\"\"\n    if self._is_async:\n        raise RuntimeError(\"This LitAgentLLM uses an async function. Use rollout_async instead.\")\n\n    # Find the first LLM resource\n    llm = self._get_llm_resource(resources)\n\n    # Strip ProxyLLM if needed\n    if self.strip_proxy:\n        llm = self._strip_proxy(llm, rollout)\n\n    if self._accepts_rollout:\n        llm_rollout_func = cast(LlmRolloutFuncSync3[T], self.llm_rollout_func)\n        return llm_rollout_func(task, llm=llm, rollout=rollout)\n    else:\n        llm_rollout_func = cast(LlmRolloutFuncSync2[T], self.llm_rollout_func)\n        return llm_rollout_func(task, llm=llm)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.LitAgentLLM.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute an asynchronous rollout using the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>T</code> <p>The task input data.</p> required <code>resources</code> <code>NamedResources</code> <p>Dictionary of named resources including LLMs.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The rollout object with metadata.</p> required <p>Returns:</p> Type Description <code>RolloutRawResultV2</code> <p>The result from the wrapped rollout function.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>async def rollout_async(self, task: T, resources: NamedResources, rollout: RolloutV2) -&gt; RolloutRawResultV2:\n    \"\"\"Execute an asynchronous rollout using the wrapped function.\n\n    Args:\n        task: The task input data.\n        resources: Dictionary of named resources including LLMs.\n        rollout: The rollout object with metadata.\n\n    Returns:\n        The result from the wrapped rollout function.\n    \"\"\"\n    if not self._is_async:\n        raise RuntimeError(\"This LitAgentLLM uses a sync function. Use rollout instead.\")\n\n    # Find the first LLM resource\n    llm = self._get_llm_resource(resources)\n\n    # Strip ProxyLLM if needed\n    if self.strip_proxy:\n        llm = self._strip_proxy(llm, rollout)\n\n    if self._accepts_rollout:\n        llm_rollout_func = cast(LlmRolloutFuncAsync3[T], self.llm_rollout_func)\n        return await llm_rollout_func(task, llm=llm, rollout=rollout)\n    else:\n        llm_rollout_func = cast(LlmRolloutFuncAsync2[T], self.llm_rollout_func)\n        return await llm_rollout_func(task, llm=llm)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.is_v0_1_rollout_api","title":"<code>is_v0_1_rollout_api(func)</code>","text":"<p>Check if the rollout API is v0.1. Inspect the function signature to see if it has a rollout_id parameter.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to check.</p> required Source code in <code>agentlightning/litagent.py</code> <pre><code>def is_v0_1_rollout_api(func: Callable[..., Any]) -&gt; bool:\n    \"\"\"Check if the rollout API is v0.1.\n    Inspect the function signature to see if it has a rollout_id parameter.\n\n    Args:\n        func: The function to check.\n    \"\"\"\n    return \"rollout_id\" in inspect.signature(func).parameters\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout","title":"<code>llm_rollout(func=None, *, strip_proxy=True)</code>","text":"<pre><code>llm_rollout(func: LlmRolloutFunc[T]) -&gt; LitAgentLLM[T]\n</code></pre><pre><code>llm_rollout(\n    *, strip_proxy: bool = True\n) -&gt; Callable[[LlmRolloutFunc[T]], LitAgentLLM[T]]\n</code></pre> <p>Create a LitAgentLLM from a function that takes (task, llm[, rollout]).</p> <p>This decorator allows you to define an agent using a simple function instead of creating a full LitAgent subclass. The returned LitAgentLLM instance is callable, preserving the original function's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>LlmRolloutFunc[T] | None</code> <p>A function that defines the agent's behavior. Can be:   - sync: (task, llm) -&gt; result   - sync with rollout: (task, llm, rollout) -&gt; result   - async: async (task, llm) -&gt; result   - async with rollout: async (task, llm, rollout) -&gt; result</p> <code>None</code> <code>strip_proxy</code> <code>bool</code> <p>Whether to strip the ProxyLLM resource into a LLM resource.          Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LitAgentLLM[T] | Callable[[LlmRolloutFunc[T]], LitAgentLLM[T]]</code> <p>A callable LitAgentLLM instance that preserves the original function's</p> <code>LitAgentLLM[T] | Callable[[LlmRolloutFunc[T]], LitAgentLLM[T]]</code> <p>type hints and behavior while providing all agent functionality.</p> Example <p>@llm_rollout def my_agent(task, llm):     # Agent logic here     return response</p> <p>@llm_rollout(strip_proxy=False) def my_agent_no_strip(task, llm):     # Agent logic here     return response</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def llm_rollout(\n    func: LlmRolloutFunc[T] | None = None, *, strip_proxy: bool = True\n) -&gt; LitAgentLLM[T] | Callable[[LlmRolloutFunc[T]], LitAgentLLM[T]]:\n    \"\"\"Create a LitAgentLLM from a function that takes (task, llm[, rollout]).\n\n    This decorator allows you to define an agent using a simple function\n    instead of creating a full LitAgent subclass. The returned LitAgentLLM\n    instance is callable, preserving the original function's behavior.\n\n    Args:\n        func: A function that defines the agent's behavior. Can be:\n              - sync: (task, llm) -&gt; result\n              - sync with rollout: (task, llm, rollout) -&gt; result\n              - async: async (task, llm) -&gt; result\n              - async with rollout: async (task, llm, rollout) -&gt; result\n        strip_proxy: Whether to strip the ProxyLLM resource into a LLM resource.\n                     Defaults to True.\n\n    Returns:\n        A callable LitAgentLLM instance that preserves the original function's\n        type hints and behavior while providing all agent functionality.\n\n    Example:\n        @llm_rollout\n        def my_agent(task, llm):\n            # Agent logic here\n            return response\n\n        @llm_rollout(strip_proxy=False)\n        def my_agent_no_strip(task, llm):\n            # Agent logic here\n            return response\n\n        # Function is still callable with original behavior\n        result = my_agent(task, llm)\n\n        # Agent methods are also available\n        result = my_agent.rollout(task, resources, rollout)\n    \"\"\"\n\n    def decorator(f: LlmRolloutFunc[T]) -&gt; LitAgentLLM[T]:\n        return LitAgentLLM(f, strip_proxy=strip_proxy)\n\n    if func is None:\n        # Called with arguments: @llm_rollout(strip_proxy=False)\n        return decorator\n    else:\n        # Called without arguments: @llm_rollout\n        return decorator(func)\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout--function-is-still-callable-with-original-behavior","title":"Function is still callable with original behavior","text":"<p>result = my_agent(task, llm)</p>"},{"location":"reference/core/#agentlightning.litagent.llm_rollout--agent-methods-are-also-available","title":"Agent methods are also available","text":"<p>result = my_agent.rollout(task, resources, rollout)</p>"},{"location":"reference/core/#agentlightning.litagent.rollout","title":"<code>rollout(func)</code>","text":"<p>Create a LitAgent from a function, automatically detecting the appropriate type.</p> <p>This function inspects the provided callable and creates the appropriate agent type based on its signature. The returned agent instance is callable, preserving the original function's behavior and type hints.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Union[LlmRolloutFunc[T], Callable[..., Any]]</code> <p>A function that defines the agent's behavior.</p> required <p>Returns:</p> Type Description <code>LitAgent[T]</code> <p>A callable LitAgent subclass instance that preserves the original function's</p> <code>LitAgent[T]</code> <p>type hints and behavior while providing all agent functionality.</p> Example <p>@rollout def my_agent(task, llm):     client = OpenAI(base_url=llm.endpoint)     response = client.chat.completions.create(         model=llm.model,         messages=[{\"role\": \"user\", \"content\": task.input}],     )</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the function signature doesn't match any known patterns.</p> Source code in <code>agentlightning/litagent.py</code> <pre><code>def rollout(func: Union[LlmRolloutFunc[T], Callable[..., Any]]) -&gt; LitAgent[T]:\n    \"\"\"Create a LitAgent from a function, automatically detecting the appropriate type.\n\n    This function inspects the provided callable and creates the appropriate\n    agent type based on its signature. The returned agent instance is callable,\n    preserving the original function's behavior and type hints.\n\n    Args:\n        func: A function that defines the agent's behavior.\n\n    Returns:\n        A callable LitAgent subclass instance that preserves the original function's\n        type hints and behavior while providing all agent functionality.\n\n    Example:\n        @rollout\n        def my_agent(task, llm):\n            client = OpenAI(base_url=llm.endpoint)\n            response = client.chat.completions.create(\n                model=llm.model,\n                messages=[{\"role\": \"user\", \"content\": task.input}],\n            )\n\n        # Function is still callable with original behavior\n        result = my_agent(task, llm)\n\n        # Agent methods are also available\n        result = my_agent.rollout(task, resources, rollout)\n\n    Raises:\n        NotImplementedError: If the function signature doesn't match any known patterns.\n    \"\"\"\n    sig = inspect.signature(func)\n    params = list(sig.parameters.keys())\n\n    # Check if it matches the LLM rollout API pattern\n    # Should have at least 2 params, with the second one being 'llm' or typed as LLM\n    if len(params) &gt;= 2:\n        second_param = sig.parameters[params[1]]\n        # Check if the second parameter is named 'llm' or has LLM type annotation\n        if second_param.name == \"llm\" or (\n            second_param.annotation != inspect.Parameter.empty\n            and (second_param.annotation == LLM or str(second_param.annotation).endswith(\"LLM\"))\n        ):\n            return llm_rollout(func)\n\n    raise NotImplementedError(\n        f\"Function signature {sig} does not match any known agent patterns. \"\n        \"Expected signatures: (task, llm[, rollout]) or async (task, llm[, rollout])\"\n    )\n</code></pre>"},{"location":"reference/core/#agentlightning.litagent.rollout--function-is-still-callable-with-original-behavior","title":"Function is still callable with original behavior","text":"<p>result = my_agent(task, llm)</p>"},{"location":"reference/core/#agentlightning.litagent.rollout--agent-methods-are-also-available","title":"Agent methods are also available","text":"<p>result = my_agent.rollout(task, resources, rollout)</p>"},{"location":"reference/core/#agentlightning.client","title":"<code>agentlightning.client</code>","text":""},{"location":"reference/core/#agentlightning.client.AgentLightningClient","title":"<code>AgentLightningClient</code>","text":"<p>Client for interacting with a version-aware Agent Lightning Server.</p> <p>This client handles polling for tasks, fetching specific versions of resources (like model configurations), and posting completed rollouts back to the server. It provides both synchronous and asynchronous methods for these operations and includes a cache for resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>class AgentLightningClient:\n    \"\"\"\n    Client for interacting with a version-aware Agent Lightning Server.\n\n    This client handles polling for tasks, fetching specific versions of resources\n    (like model configurations), and posting completed rollouts back to the server.\n    It provides both synchronous and asynchronous methods for these operations and\n    includes a cache for resources.\n    \"\"\"\n\n    _next_task_uri = \"/task\"\n    _resources_uri = \"/resources\"\n    _latest_resources_uri = \"/resources/latest\"\n    _report_rollout_uri = \"/rollout\"\n\n    def __init__(self, endpoint: str, poll_interval: float = 5.0, timeout: float = 10.0):\n        \"\"\"Initializes the AgentLightningClient.\n\n        Args:\n            endpoint: The root URL of the Agent Lightning server.\n            poll_interval: The interval in seconds to wait between polling for new tasks.\n            timeout: The timeout in seconds for HTTP requests.\n        \"\"\"\n        self.endpoint = endpoint\n        self.task_count = 0\n        self.poll_interval = poll_interval\n        self.timeout = timeout\n        self._resource_cache: Dict[str, ResourcesUpdate] = {}  # TODO: mechanism to evict cache\n        self._default_headers = {\"X-AgentLightning-Client\": \"true\"}\n\n    async def _request_json_async(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes an async GET request to the specified URL and returns the JSON response.\n\n        Args:\n            url: The URL to request.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        timeout = aiohttp.ClientTimeout(total=self.timeout)\n        async with aiohttp.ClientSession(timeout=timeout) as session:\n            try:\n                async with session.get(url, headers=self._default_headers) as resp:\n                    resp.raise_for_status()\n                    return await resp.json()\n            except Exception as e:\n                logger.debug(f\"Async GET request failed for {url}: {e}\")\n                return None\n\n    async def _post_json_async(self, url: str, payload: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes an async POST request with a JSON payload.\n\n        Args:\n            url: The URL to post to.\n            payload: The dictionary data to send as JSON.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        timeout = aiohttp.ClientTimeout(total=self.timeout)\n        async with aiohttp.ClientSession(timeout=timeout) as session:\n            try:\n                async with session.post(url, json=payload, headers=self._default_headers) as resp:\n                    resp.raise_for_status()\n                    return await resp.json()\n            except Exception as e:\n                logger.debug(f\"Async POST request failed for {url}: {e}\")\n                return None\n\n    async def poll_next_task_async(self) -&gt; Optional[Task]:\n        \"\"\"Polls the server asynchronously for the next task until one is available.\n\n        Returns:\n            A Task object containing the task details.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n        while True:\n            response = await self._request_json_async(url)\n            if response:\n                task_if_any = TaskIfAny.model_validate(response)\n                if task_if_any.is_available and task_if_any.task:\n                    self.task_count += 1\n                    logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                    return task_if_any.task\n            logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n            await asyncio.sleep(self.poll_interval)\n\n    async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches a specific version of resources by its ID, using a cache.\n\n        Args:\n            resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n        Returns:\n            A ResourcesUpdate object containing the versioned resources, or None if not found.\n        \"\"\"\n        if resource_id in self._resource_cache:\n            logger.debug(f\"Found resources '{resource_id}' in cache.\")\n            return self._resource_cache[resource_id]\n\n        url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n        response = await self._request_json_async(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resource_id] = resources_update\n            logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n            return resources_update\n        return None\n\n    async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches the latest available resources from the server.\n\n        Returns:\n            A ResourcesUpdate object containing the latest resources.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n        response = await self._request_json_async(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            # Cache this result as well\n            self._resource_cache[resources_update.resources_id] = resources_update\n            return resources_update\n        return None\n\n    async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Posts a completed rollout to the server asynchronously.\n\n        Args:\n            rollout: A Rollout object containing the results of a task.\n\n        Returns:\n            The server's JSON response as a dictionary.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n        payload = rollout.model_dump(mode=\"json\")\n        return await self._post_json_async(url, payload)\n\n    def _request_json(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes a sync GET request to the specified URL and returns the JSON response.\n\n        Args:\n            url: The URL to request.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        try:\n            response = requests.get(url, timeout=self.timeout, headers=self._default_headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            logger.debug(f\"Sync GET request failed for {url}: {e}\")\n            return None\n\n    def _post_json(self, url: str, payload: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Makes a sync POST request with a JSON payload.\n\n        Args:\n            url: The URL to post to.\n            payload: The dictionary data to send as JSON.\n\n        Returns:\n            The JSON response as a dictionary or None if the request fails.\n        \"\"\"\n        try:\n            response = requests.post(url, json=payload, timeout=self.timeout, headers=self._default_headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            logger.debug(f\"Sync POST request failed for {url}: {e}\")\n            return None\n\n    def poll_next_task(self) -&gt; Optional[Task]:\n        \"\"\"Polls the server synchronously for the next task until one is available.\n\n        Returns:\n            A Task object containing the task details, including the required `resources_id`.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n        while True:\n            response = self._request_json(url)\n            if response:\n                task_if_any = TaskIfAny.model_validate(response)\n                if task_if_any.is_available and task_if_any.task:\n                    self.task_count += 1\n                    logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                    return task_if_any.task\n            logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n            time.sleep(self.poll_interval)\n\n    def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches a specific version of resources by its ID synchronously, using a cache.\n\n        Args:\n            resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n        Returns:\n            A ResourcesUpdate object containing the versioned resources, or None if not found.\n        \"\"\"\n        if resource_id in self._resource_cache:\n            logger.debug(f\"Found resources '{resource_id}' in cache.\")\n            return self._resource_cache[resource_id]\n\n        url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n        response = self._request_json(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resource_id] = resources_update\n            logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n            return resources_update\n        return None\n\n    def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"Fetches the latest available resources from the server synchronously.\n\n        Returns:\n            A ResourcesUpdate object containing the latest resources.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n        response = self._request_json(url)\n        if response:\n            resources_update = ResourcesUpdate.model_validate(response)\n            self._resource_cache[resources_update.resources_id] = resources_update\n            return resources_update\n        return None\n\n    def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Posts a completed rollout to the server synchronously.\n\n        Args:\n            rollout: A Rollout object containing the results of a task.\n\n        Returns:\n            The server's JSON response as a dictionary.\n        \"\"\"\n        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n        payload = rollout.model_dump(mode=\"json\")\n        return self._post_json(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.__init__","title":"<code>__init__(endpoint, poll_interval=5.0, timeout=10.0)</code>","text":"<p>Initializes the AgentLightningClient.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The root URL of the Agent Lightning server.</p> required <code>poll_interval</code> <code>float</code> <p>The interval in seconds to wait between polling for new tasks.</p> <code>5.0</code> <code>timeout</code> <code>float</code> <p>The timeout in seconds for HTTP requests.</p> <code>10.0</code> Source code in <code>agentlightning/client.py</code> <pre><code>def __init__(self, endpoint: str, poll_interval: float = 5.0, timeout: float = 10.0):\n    \"\"\"Initializes the AgentLightningClient.\n\n    Args:\n        endpoint: The root URL of the Agent Lightning server.\n        poll_interval: The interval in seconds to wait between polling for new tasks.\n        timeout: The timeout in seconds for HTTP requests.\n    \"\"\"\n    self.endpoint = endpoint\n    self.task_count = 0\n    self.poll_interval = poll_interval\n    self.timeout = timeout\n    self._resource_cache: Dict[str, ResourcesUpdate] = {}  # TODO: mechanism to evict cache\n    self._default_headers = {\"X-AgentLightning-Client\": \"true\"}\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_latest_resources","title":"<code>get_latest_resources()</code>","text":"<p>Fetches the latest available resources from the server synchronously.</p> <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the latest resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches the latest available resources from the server synchronously.\n\n    Returns:\n        A ResourcesUpdate object containing the latest resources.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n    response = self._request_json(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resources_update.resources_id] = resources_update\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_latest_resources_async","title":"<code>get_latest_resources_async()</code>  <code>async</code>","text":"<p>Fetches the latest available resources from the server.</p> <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the latest resources.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches the latest available resources from the server.\n\n    Returns:\n        A ResourcesUpdate object containing the latest resources.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)\n    response = await self._request_json_async(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        # Cache this result as well\n        self._resource_cache[resources_update.resources_id] = resources_update\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_resources_by_id","title":"<code>get_resources_by_id(resource_id)</code>","text":"<p>Fetches a specific version of resources by its ID synchronously, using a cache.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The ID of the resources to fetch, usually from a Task's metadata.</p> required <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the versioned resources, or None if not found.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches a specific version of resources by its ID synchronously, using a cache.\n\n    Args:\n        resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n    Returns:\n        A ResourcesUpdate object containing the versioned resources, or None if not found.\n    \"\"\"\n    if resource_id in self._resource_cache:\n        logger.debug(f\"Found resources '{resource_id}' in cache.\")\n        return self._resource_cache[resource_id]\n\n    url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n    response = self._request_json(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resource_id] = resources_update\n        logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.get_resources_by_id_async","title":"<code>get_resources_by_id_async(resource_id)</code>  <code>async</code>","text":"<p>Fetches a specific version of resources by its ID, using a cache.</p> <p>Parameters:</p> Name Type Description Default <code>resource_id</code> <code>str</code> <p>The ID of the resources to fetch, usually from a Task's metadata.</p> required <p>Returns:</p> Type Description <code>Optional[ResourcesUpdate]</code> <p>A ResourcesUpdate object containing the versioned resources, or None if not found.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"Fetches a specific version of resources by its ID, using a cache.\n\n    Args:\n        resource_id: The ID of the resources to fetch, usually from a Task's metadata.\n\n    Returns:\n        A ResourcesUpdate object containing the versioned resources, or None if not found.\n    \"\"\"\n    if resource_id in self._resource_cache:\n        logger.debug(f\"Found resources '{resource_id}' in cache.\")\n        return self._resource_cache[resource_id]\n\n    url = urllib.parse.urljoin(self.endpoint, f\"{self._resources_uri}/{resource_id}\")\n    response = await self._request_json_async(url)\n    if response:\n        resources_update = ResourcesUpdate.model_validate(response)\n        self._resource_cache[resource_id] = resources_update\n        logger.info(f\"Fetched and cached resources for ID: {resource_id}\")\n        return resources_update\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Polls the server synchronously for the next task until one is available.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>A Task object containing the task details, including the required <code>resources_id</code>.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def poll_next_task(self) -&gt; Optional[Task]:\n    \"\"\"Polls the server synchronously for the next task until one is available.\n\n    Returns:\n        A Task object containing the task details, including the required `resources_id`.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n    while True:\n        response = self._request_json(url)\n        if response:\n            task_if_any = TaskIfAny.model_validate(response)\n            if task_if_any.is_available and task_if_any.task:\n                self.task_count += 1\n                logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                return task_if_any.task\n        logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n        time.sleep(self.poll_interval)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.poll_next_task_async","title":"<code>poll_next_task_async()</code>  <code>async</code>","text":"<p>Polls the server asynchronously for the next task until one is available.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>A Task object containing the task details.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def poll_next_task_async(self) -&gt; Optional[Task]:\n    \"\"\"Polls the server asynchronously for the next task until one is available.\n\n    Returns:\n        A Task object containing the task details.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)\n    while True:\n        response = await self._request_json_async(url)\n        if response:\n            task_if_any = TaskIfAny.model_validate(response)\n            if task_if_any.is_available and task_if_any.task:\n                self.task_count += 1\n                logger.info(f\"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}\")\n                return task_if_any.task\n        logger.debug(f\"No task available yet. Retrying in {self.poll_interval} seconds...\")\n        await asyncio.sleep(self.poll_interval)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.post_rollout","title":"<code>post_rollout(rollout)</code>","text":"<p>Posts a completed rollout to the server synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>rollout</code> <code>Rollout</code> <p>A Rollout object containing the results of a task.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The server's JSON response as a dictionary.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Posts a completed rollout to the server synchronously.\n\n    Args:\n        rollout: A Rollout object containing the results of a task.\n\n    Returns:\n        The server's JSON response as a dictionary.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n    payload = rollout.model_dump(mode=\"json\")\n    return self._post_json(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.AgentLightningClient.post_rollout_async","title":"<code>post_rollout_async(rollout)</code>  <code>async</code>","text":"<p>Posts a completed rollout to the server asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>rollout</code> <code>Rollout</code> <p>A Rollout object containing the results of a task.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The server's JSON response as a dictionary.</p> Source code in <code>agentlightning/client.py</code> <pre><code>async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Posts a completed rollout to the server asynchronously.\n\n    Args:\n        rollout: A Rollout object containing the results of a task.\n\n    Returns:\n        The server's JSON response as a dictionary.\n    \"\"\"\n    url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)\n    payload = rollout.model_dump(mode=\"json\")\n    return await self._post_json_async(url, payload)\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader","title":"<code>DevTaskLoader</code>","text":"<p>               Bases: <code>AgentLightningClient</code></p> <p>A local task manager for development that provides sample tasks and resources.</p> <p>This client mocks the server APIs by maintaining a local queue of tasks and resources within the same process. It's designed for development, testing, and scenarios where a full Agent Lightning server is not needed.</p> <p>The DevTaskLoader overrides the polling and resource fetching methods to return data from local collections instead of making HTTP requests to a remote server.</p> Source code in <code>agentlightning/client.py</code> <pre><code>class DevTaskLoader(AgentLightningClient):\n    \"\"\"A local task manager for development that provides sample tasks and resources.\n\n    This client mocks the server APIs by maintaining a local queue of tasks and resources\n    within the same process. It's designed for development, testing, and scenarios where\n    a full Agent Lightning server is not needed.\n\n    The DevTaskLoader overrides the polling and resource fetching methods to return data\n    from local collections instead of making HTTP requests to a remote server.\n    \"\"\"\n\n    def __init__(\n        self,\n        tasks: Union[List[TaskInput], List[Task]],\n        resources: Union[NamedResources, ResourcesUpdate],\n        **kwargs: Any,\n    ):\n        \"\"\"Initializes the DevTaskLoader with pre-defined tasks and resources.\n\n        Args:\n            tasks: Either a List of TaskInput objects or a List of Task objects.\n            resources: Either NamedResources or ResourcesUpdate object.\n            **kwargs: Additional arguments passed to the parent AgentLightningClient.\n        \"\"\"\n        super().__init__(endpoint=\"local://\", **kwargs)\n        self._tasks = tasks.copy()\n        if len(self._tasks) == 0:\n            raise ValueError(\"DevTaskLoader requires at least one task to be provided.\")\n\n        # Check if tasks are mixture of TaskInput and Task\n        if any(isinstance(task, Task) for task in self._tasks):\n            if not all(isinstance(task, Task) for task in self._tasks):\n                raise ValueError(\"All tasks must be either Task or TaskInput objects.\")\n\n        self._task_index = 0\n\n        if isinstance(resources, ResourcesUpdate):\n            self._resources_update = resources\n        else:\n            self._resources_update = ResourcesUpdate(resources_id=\"local\", resources=resources)\n\n        # Store rollouts posted back to the loader for easy debugging of local runs\n        self._rollouts: List[Rollout] = []\n\n    @property\n    def rollouts(self) -&gt; List[Rollout]:\n        \"\"\"Return rollouts that have been posted back to the loader.\"\"\"\n        return self._rollouts\n\n    def poll_next_task(self) -&gt; Optional[Task]:\n        \"\"\"Returns the next task from the local queue.\n\n        If tasks are TaskInput objects, assembles them into Task objects.\n        If tasks are already Task objects, returns them directly.\n\n        Returns:\n            The next Task object from the local task list.\n        \"\"\"\n        if self._task_index &gt;= len(self._tasks):\n            self._task_index = 0\n\n        task_or_input = self._tasks[self._task_index]\n\n        if isinstance(task_or_input, Task):\n            task = task_or_input\n        else:\n            rollout_id = f\"local_task_{self._task_index + 1:03d}\"\n            task = Task(\n                rollout_id=rollout_id,\n                input=task_or_input,\n                resources_id=self._resources_update.resources_id,\n                create_time=time.time(),\n            )\n\n        self._task_index += 1\n        self.task_count += 1\n        logger.info(f\"[Task {self.task_count} Received] Task ID: {task.rollout_id}\")\n        return task\n\n    def get_resources_by_id(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        logger.debug(f\"DevTaskLoader checking resources for ID: {resource_id}\")\n        if resource_id != self._resources_update.resources_id:\n            raise ValueError(\n                f\"Resource ID '{resource_id}' not found. Only '{self._resources_update.resources_id}' is available.\"\n            )\n        return self._resources_update\n\n    def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        logger.debug(\"DevTaskLoader returning latest resources.\")\n        return self._resources_update\n\n    def post_rollout(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        logger.debug(f\"DevTaskLoader received rollout for task: {rollout.rollout_id}\")\n        self._rollouts.append(rollout)\n        return {\"status\": \"received\", \"rollout_id\": rollout.rollout_id}\n\n    async def poll_next_task_async(self) -&gt; Optional[Task]:\n        return self.poll_next_task()\n\n    async def get_resources_by_id_async(self, resource_id: str) -&gt; Optional[ResourcesUpdate]:\n        return self.get_resources_by_id(resource_id)\n\n    async def get_latest_resources_async(self) -&gt; Optional[ResourcesUpdate]:\n        return self.get_latest_resources()\n\n    async def post_rollout_async(self, rollout: Rollout) -&gt; Optional[Dict[str, Any]]:\n        return self.post_rollout(rollout)\n\n    def __repr__(self):\n        return f\"DevTaskLoader(num_tasks={len(self._tasks)}, resources={self._resources_update.resources})\"\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.rollouts","title":"<code>rollouts</code>  <code>property</code>","text":"<p>Return rollouts that have been posted back to the loader.</p>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.__init__","title":"<code>__init__(tasks, resources, **kwargs)</code>","text":"<p>Initializes the DevTaskLoader with pre-defined tasks and resources.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Union[List[TaskInput], List[Task]]</code> <p>Either a List of TaskInput objects or a List of Task objects.</p> required <code>resources</code> <code>Union[NamedResources, ResourcesUpdate]</code> <p>Either NamedResources or ResourcesUpdate object.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parent AgentLightningClient.</p> <code>{}</code> Source code in <code>agentlightning/client.py</code> <pre><code>def __init__(\n    self,\n    tasks: Union[List[TaskInput], List[Task]],\n    resources: Union[NamedResources, ResourcesUpdate],\n    **kwargs: Any,\n):\n    \"\"\"Initializes the DevTaskLoader with pre-defined tasks and resources.\n\n    Args:\n        tasks: Either a List of TaskInput objects or a List of Task objects.\n        resources: Either NamedResources or ResourcesUpdate object.\n        **kwargs: Additional arguments passed to the parent AgentLightningClient.\n    \"\"\"\n    super().__init__(endpoint=\"local://\", **kwargs)\n    self._tasks = tasks.copy()\n    if len(self._tasks) == 0:\n        raise ValueError(\"DevTaskLoader requires at least one task to be provided.\")\n\n    # Check if tasks are mixture of TaskInput and Task\n    if any(isinstance(task, Task) for task in self._tasks):\n        if not all(isinstance(task, Task) for task in self._tasks):\n            raise ValueError(\"All tasks must be either Task or TaskInput objects.\")\n\n    self._task_index = 0\n\n    if isinstance(resources, ResourcesUpdate):\n        self._resources_update = resources\n    else:\n        self._resources_update = ResourcesUpdate(resources_id=\"local\", resources=resources)\n\n    # Store rollouts posted back to the loader for easy debugging of local runs\n    self._rollouts: List[Rollout] = []\n</code></pre>"},{"location":"reference/core/#agentlightning.client.DevTaskLoader.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Returns the next task from the local queue.</p> <p>If tasks are TaskInput objects, assembles them into Task objects. If tasks are already Task objects, returns them directly.</p> <p>Returns:</p> Type Description <code>Optional[Task]</code> <p>The next Task object from the local task list.</p> Source code in <code>agentlightning/client.py</code> <pre><code>def poll_next_task(self) -&gt; Optional[Task]:\n    \"\"\"Returns the next task from the local queue.\n\n    If tasks are TaskInput objects, assembles them into Task objects.\n    If tasks are already Task objects, returns them directly.\n\n    Returns:\n        The next Task object from the local task list.\n    \"\"\"\n    if self._task_index &gt;= len(self._tasks):\n        self._task_index = 0\n\n    task_or_input = self._tasks[self._task_index]\n\n    if isinstance(task_or_input, Task):\n        task = task_or_input\n    else:\n        rollout_id = f\"local_task_{self._task_index + 1:03d}\"\n        task = Task(\n            rollout_id=rollout_id,\n            input=task_or_input,\n            resources_id=self._resources_update.resources_id,\n            create_time=time.time(),\n        )\n\n    self._task_index += 1\n    self.task_count += 1\n    logger.info(f\"[Task {self.task_count} Received] Task ID: {task.rollout_id}\")\n    return task\n</code></pre>"},{"location":"reference/core/#agentlightning.runner","title":"<code>agentlightning.runner</code>","text":""},{"location":"reference/core/#agentlightning.runner.AgentRunner","title":"<code>AgentRunner</code>","text":"<p>               Bases: <code>BaseRunner[Any]</code></p> <p>Manages the agent's execution loop and integrates with AgentOps.</p> <p>This class orchestrates the interaction between the agent (<code>LitAgent</code>) and the server (<code>AgentLightningClient</code>). It handles polling for tasks, executing the agent's logic, and reporting results back to the server. If enabled, it will also automatically trace each rollout using AgentOps.</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The <code>LitAgent</code> instance containing the agent's logic.</p> <code>client</code> <p>The <code>AgentLightningClient</code> for server communication.</p> <code>tracer</code> <p>The tracer instance for this runner/worker.</p> <code>worker_id</code> <p>An optional identifier for the worker process.</p> <code>max_tasks</code> <p>The maximum number of tasks to process before stopping.</p> Source code in <code>agentlightning/runner/legacy.py</code> <pre><code>class AgentRunner(BaseRunner[Any]):\n    \"\"\"Manages the agent's execution loop and integrates with AgentOps.\n\n    This class orchestrates the interaction between the agent (`LitAgent`) and\n    the server (`AgentLightningClient`). It handles polling for tasks, executing\n    the agent's logic, and reporting results back to the server. If enabled,\n    it will also automatically trace each rollout using AgentOps.\n\n    Attributes:\n        agent: The `LitAgent` instance containing the agent's logic.\n        client: The `AgentLightningClient` for server communication.\n        tracer: The tracer instance for this runner/worker.\n        worker_id: An optional identifier for the worker process.\n        max_tasks: The maximum number of tasks to process before stopping.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: LitAgent[Any],\n        client: AgentLightningClient,\n        tracer: BaseTracer,\n        triplet_exporter: TraceTripletAdapter,\n        worker_id: Optional[int] = None,\n        max_tasks: Optional[int] = None,\n    ):\n        super().__init__()\n        self.agent = agent\n        self.client = client\n        self.tracer = tracer\n        self.triplet_exporter = triplet_exporter\n\n        # Worker-specific attributes\n        self.worker_id = worker_id\n        self.max_tasks = max_tasks\n\n    # These methods are overridden by BaseRunner, getting them back to old behavior.\n    def init(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def init_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        self.worker_id = worker_id\n\n    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def _log_prefix(self, rollout_id: Optional[str] = None) -&gt; str:\n        \"\"\"Generates a standardized log prefix for the current worker.\"\"\"\n        if self.worker_id is not None:\n            if rollout_id:\n                return f\"[Worker {self.worker_id} | Rollout {rollout_id}]\"\n            else:\n                return f\"[Worker {self.worker_id}]\"\n        if rollout_id:\n            return f\"[Rollout {rollout_id}]\"\n        return \"[Default Worker]\"\n\n    def _to_rollout_object(\n        self,\n        result: RolloutRawResult,\n        rollout_id: str,\n    ) -&gt; Rollout:\n        \"\"\"Standardizes the agent's return value into a Rollout object.\n\n        Args:\n            result: The output from the agent's rollout method.\n            rollout_id: The unique identifier for the current task.\n\n        Returns:\n            A standardized `Rollout` object for reporting to the server.\n        \"\"\"\n        trace: Any = None\n        final_reward: Optional[float] = None\n        triplets: Optional[List[Triplet]] = None\n        trace_spans: Optional[List[ReadableSpan]] = None\n\n        # Handle different types of results from the agent\n        # Case 1: result is a float (final reward)\n        if isinstance(result, float):\n            final_reward = result\n        # Case 2: result is a list of Triplets\n        if isinstance(result, list) and all(isinstance(t, Triplet) for t in result):\n            triplets = result  # type: ignore\n        # Case 3: result is a list of ReadableSpan (OpenTelemetry spans)\n        if isinstance(result, list) and all(isinstance(t, ReadableSpan) for t in result):\n            trace_spans = result  # type: ignore\n            trace = [json.loads(readable_span.to_json()) for readable_span in trace_spans]  # type: ignore\n        # Case 4: result is a list of dict (trace JSON)\n        if isinstance(result, list) and all(isinstance(t, dict) for t in result):\n            trace = result\n        # Case 5: result is a Rollout object\n        if isinstance(result, Rollout):\n            final_reward = result.final_reward\n            triplets = result.triplets\n            trace = result.trace\n\n        # If the agent has tracing enabled, use the tracer's last trace if not already set\n        if self.tracer and (trace is None or trace_spans is None):\n            spans = self.tracer.get_last_trace()\n            if spans:\n                trace = [json.loads(readable_span.to_json()) for readable_span in spans]\n                trace_spans = spans\n\n        # Always extract triplets from the trace using TraceTripletAdapter\n        if trace_spans:\n            triplets = self.triplet_exporter(trace_spans)\n\n        # If the agent has triplets, use the last one for final reward if not set\n        if triplets and triplets[-1].reward is not None and final_reward is None:\n            final_reward = triplets[-1].reward\n\n        # Create the Rollout object with standardized fields\n        result_dict: Dict[str, Any] = {\n            \"rollout_id\": rollout_id,\n        }\n        if final_reward is not None:\n            result_dict[\"final_reward\"] = final_reward\n        if triplets is not None:\n            result_dict[\"triplets\"] = triplets\n        if trace is not None:\n            result_dict[\"trace\"] = trace\n\n        if isinstance(result, Rollout):\n            return result.model_copy(update=result_dict)\n        return Rollout(**result_dict)\n\n    def run(self) -&gt; bool:  # type: ignore\n        \"\"\"Poll the task and rollout once synchronously.\"\"\"\n        self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n        task = self.client.poll_next_task()\n        if task is None:\n            logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n            return False\n        rollout_id = task.rollout_id\n\n        resources_id = task.resources_id\n        resources_update = None\n        if resources_id:\n            resources_update = self.client.get_resources_by_id(resources_id)\n        else:\n            logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n            resources_update = self.client.get_latest_resources()\n        if not resources_update:\n            logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n            return False\n\n        rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n        try:\n            try:\n                self.agent.on_rollout_start(task, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n            with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n                start_time = time.time()\n                rollout_method = self.agent.training_rollout if task.mode == \"train\" else self.agent.validation_rollout\n                # Pass the task input, not the whole task object\n                if is_v0_1_rollout_api(rollout_method):\n                    result = cast(\n                        RolloutRawResult,\n                        rollout_method(\n                            task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                        ),\n                    )  # type: ignore\n                else:\n                    result = rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n                rollout_obj = self._to_rollout_object(result, task.rollout_id)\n                end_time = time.time()\n                logger.info(\n                    f\"{self._log_prefix(rollout_id)} Completed in \"\n                    f\"{end_time - start_time:.2f}s. Triplet length: \"\n                    f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                    f\"Reward: {rollout_obj.final_reward}\"\n                )\n\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n        finally:\n            try:\n                self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n            self.client.post_rollout(rollout_obj)\n\n        return True\n\n    def iter(self) -&gt; int:  # type: ignore\n        \"\"\"Executes the synchronous polling and rollout loop.\"\"\"\n        num_tasks_processed = 0\n        logger.info(f\"{self._log_prefix()} Started sync rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n        while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n            if self.run():\n                num_tasks_processed += 1\n\n            if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n                logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n\n        logger.info(f\"{self._log_prefix()} Finished sync rollouts. Processed {num_tasks_processed} tasks.\")\n        return num_tasks_processed\n\n    async def run_async(self) -&gt; bool:\n        \"\"\"Poll the task and rollout once.\"\"\"\n        self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n        task = await self.client.poll_next_task_async()\n        if task is None:\n            logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n            return False\n        rollout_id = task.rollout_id\n\n        resources_id = task.resources_id\n        resources_update = None\n        if resources_id:\n            resources_update = await self.client.get_resources_by_id_async(resources_id)\n        else:\n            logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n            resources_update = await self.client.get_latest_resources_async()\n        if not resources_update:\n            logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n            return False\n\n        rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n        try:\n            try:\n                self.agent.on_rollout_start(task, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n            with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n                start_time = time.time()\n                rollout_method = (\n                    self.agent.training_rollout_async if task.mode == \"train\" else self.agent.validation_rollout_async\n                )\n                # Pass the task input, not the whole task object\n                if is_v0_1_rollout_api(rollout_method):\n                    result = cast(\n                        RolloutRawResult,\n                        await rollout_method(\n                            task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                        ),\n                    )  # type: ignore\n                else:\n                    result = await rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n                rollout_obj = self._to_rollout_object(result, task.rollout_id)\n                end_time = time.time()\n                logger.info(\n                    f\"{self._log_prefix(rollout_id)} Completed in \"\n                    f\"{end_time - start_time:.2f}s. Triplet length: \"\n                    f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                    f\"Reward: {rollout_obj.final_reward}\"\n                )\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n        finally:\n            try:\n                self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n            await self.client.post_rollout_async(rollout_obj)\n\n        return True\n\n    async def iter_async(self) -&gt; int:\n        \"\"\"Executes the asynchronous polling and rollout loop.\"\"\"\n        num_tasks_processed = 0\n        logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n        while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n            if await self.run_async():\n                num_tasks_processed += 1\n\n            if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n                logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n        logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n        return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.iter","title":"<code>iter()</code>","text":"<p>Executes the synchronous polling and rollout loop.</p> Source code in <code>agentlightning/runner/legacy.py</code> <pre><code>def iter(self) -&gt; int:  # type: ignore\n    \"\"\"Executes the synchronous polling and rollout loop.\"\"\"\n    num_tasks_processed = 0\n    logger.info(f\"{self._log_prefix()} Started sync rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n    while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n        if self.run():\n            num_tasks_processed += 1\n\n        if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n            logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n\n    logger.info(f\"{self._log_prefix()} Finished sync rollouts. Processed {num_tasks_processed} tasks.\")\n    return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.iter_async","title":"<code>iter_async()</code>  <code>async</code>","text":"<p>Executes the asynchronous polling and rollout loop.</p> Source code in <code>agentlightning/runner/legacy.py</code> <pre><code>async def iter_async(self) -&gt; int:\n    \"\"\"Executes the asynchronous polling and rollout loop.\"\"\"\n    num_tasks_processed = 0\n    logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self.max_tasks or 'unlimited'}).\")\n\n    while self.max_tasks is None or num_tasks_processed &lt; self.max_tasks:\n        if await self.run_async():\n            num_tasks_processed += 1\n\n        if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n            logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self.max_tasks or 'unlimited'}\")\n    logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n    return num_tasks_processed\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.run","title":"<code>run()</code>","text":"<p>Poll the task and rollout once synchronously.</p> Source code in <code>agentlightning/runner/legacy.py</code> <pre><code>def run(self) -&gt; bool:  # type: ignore\n    \"\"\"Poll the task and rollout once synchronously.\"\"\"\n    self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n    task = self.client.poll_next_task()\n    if task is None:\n        logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n        return False\n    rollout_id = task.rollout_id\n\n    resources_id = task.resources_id\n    resources_update = None\n    if resources_id:\n        resources_update = self.client.get_resources_by_id(resources_id)\n    else:\n        logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n        resources_update = self.client.get_latest_resources()\n    if not resources_update:\n        logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n        return False\n\n    rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n    try:\n        try:\n            self.agent.on_rollout_start(task, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n        with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n            start_time = time.time()\n            rollout_method = self.agent.training_rollout if task.mode == \"train\" else self.agent.validation_rollout\n            # Pass the task input, not the whole task object\n            if is_v0_1_rollout_api(rollout_method):\n                result = cast(\n                    RolloutRawResult,\n                    rollout_method(\n                        task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                    ),\n                )  # type: ignore\n            else:\n                result = rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n            rollout_obj = self._to_rollout_object(result, task.rollout_id)\n            end_time = time.time()\n            logger.info(\n                f\"{self._log_prefix(rollout_id)} Completed in \"\n                f\"{end_time - start_time:.2f}s. Triplet length: \"\n                f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                f\"Reward: {rollout_obj.final_reward}\"\n            )\n\n    except Exception:\n        logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n    finally:\n        try:\n            self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n        self.client.post_rollout(rollout_obj)\n\n    return True\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunner.run_async","title":"<code>run_async()</code>  <code>async</code>","text":"<p>Poll the task and rollout once.</p> Source code in <code>agentlightning/runner/legacy.py</code> <pre><code>async def run_async(self) -&gt; bool:\n    \"\"\"Poll the task and rollout once.\"\"\"\n    self.agent.set_runner(self)  # Ensure the agent has a reference to this runner\n\n    task = await self.client.poll_next_task_async()\n    if task is None:\n        logger.info(f\"{self._log_prefix()} Poll returned no task. Exiting.\")\n        return False\n    rollout_id = task.rollout_id\n\n    resources_id = task.resources_id\n    resources_update = None\n    if resources_id:\n        resources_update = await self.client.get_resources_by_id_async(resources_id)\n    else:\n        logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n        resources_update = await self.client.get_latest_resources_async()\n    if not resources_update:\n        logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n        return False\n\n    rollout_obj = Rollout(rollout_id=task.rollout_id, task=task)  # Default empty rollout\n\n    try:\n        try:\n            self.agent.on_rollout_start(task, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_start hook.\")\n\n        with self.tracer.trace_context(name=f\"rollout_{rollout_id}\"):\n            start_time = time.time()\n            rollout_method = (\n                self.agent.training_rollout_async if task.mode == \"train\" else self.agent.validation_rollout_async\n            )\n            # Pass the task input, not the whole task object\n            if is_v0_1_rollout_api(rollout_method):\n                result = cast(\n                    RolloutRawResult,\n                    await rollout_method(\n                        task.input, rollout_id=rollout_obj.rollout_id, resources=resources_update.resources  # type: ignore\n                    ),\n                )  # type: ignore\n            else:\n                result = await rollout_method(task.input, resources=resources_update.resources, rollout=rollout_obj)\n            rollout_obj = self._to_rollout_object(result, task.rollout_id)\n            end_time = time.time()\n            logger.info(\n                f\"{self._log_prefix(rollout_id)} Completed in \"\n                f\"{end_time - start_time:.2f}s. Triplet length: \"\n                f\"{len(rollout_obj.triplets) if rollout_obj.triplets is not None else 'N/A'}. \"\n                f\"Reward: {rollout_obj.final_reward}\"\n            )\n    except Exception:\n        logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n    finally:\n        try:\n            self.agent.on_rollout_end(task, rollout_obj, self, self.tracer)\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n        await self.client.post_rollout_async(rollout_obj)\n\n    return True\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2","title":"<code>AgentRunnerV2</code>","text":"<p>               Bases: <code>BaseRunner[T_task]</code></p> <p>Runner implementation for executing agent tasks with distributed support.</p> <p>This runner manages the complete lifecycle of agent rollout execution, including task polling, resource management, tracing, and hooks. It supports both continuous iteration over tasks from the store and single-step execution.</p> <p>Attributes:</p> Name Type Description <code>worker_id</code> <code>Optional[int]</code> <p>The unique identifier for this worker process.</p> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>class AgentRunnerV2(BaseRunner[T_task]):\n    \"\"\"Runner implementation for executing agent tasks with distributed support.\n\n    This runner manages the complete lifecycle of agent rollout execution,\n    including task polling, resource management, tracing, and hooks. It supports\n    both continuous iteration over tasks from the store and single-step execution.\n\n    Attributes:\n        worker_id: The unique identifier for this worker process.\n    \"\"\"\n\n    def __init__(self, tracer: BaseTracer, max_rollouts: Optional[int] = None, poll_interval: float = 5.0) -&gt; None:\n        \"\"\"Initialize the agent runner.\n\n        Args:\n            tracer: The tracer instance for recording execution traces and spans.\n            max_rollouts: Maximum number of tasks to process in iter() mode. If None,\n                the runner will continue indefinitely until interrupted.\n            poll_interval: Time in seconds to wait between polling attempts when\n                no tasks are available in the store.\n        \"\"\"\n        super().__init__()\n        self._tracer = tracer\n        self._max_rollouts = max_rollouts\n        self._poll_interval = poll_interval\n\n        # Set later\n        self._agent: Optional[LitAgent[T_task]] = None\n        self._hooks: Sequence[Hook] = []\n        self._store: Optional[LightningStore] = None\n        self.worker_id: Optional[int] = None\n\n    def init(self, agent: LitAgent[T_task], *, hooks: Optional[Sequence[Hook]] = None, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the runner with the agent.\n\n        This sets up the agent-runner relationship, registers hooks, and\n        initializes the tracer.\n\n        Args:\n            agent: The LitAgent instance to be managed by this runner.\n            hooks: Optional sequence of Hook objects to be called at various\n                lifecycle stages (on_trace_start, on_trace_end, on_rollout_start,\n                on_rollout_end).\n            **kwargs: Additional initialization arguments (currently unused).\n        \"\"\"\n        self._agent = agent\n        self._agent.set_runner(self)\n        self._hooks = [*hooks] if hooks is not None else []\n\n        self._tracer.init()\n\n    def init_worker(self, worker_id: int, store: LightningStore, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the runner for each worker with worker_id and store.\n\n        This method is called once per worker in a distributed setup to provide\n        the worker with its ID and store connection.\n\n        Args:\n            worker_id: Unique identifier for this worker process.\n            store: The LightningStore instance for task coordination and data persistence.\n            **kwargs: Additional worker-specific initialization arguments (currently unused).\n        \"\"\"\n        self._store = store\n        self.worker_id = worker_id\n\n        self._tracer.init_worker(worker_id)\n\n    def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Teardown the runner and clean up all resources.\n\n        This method resets all internal state including the agent, store,\n        hooks, and worker ID, and calls the tracer's teardown method.\n\n        Args:\n            *args: Additional teardown arguments (currently unused).\n            **kwargs: Additional teardown keyword arguments (currently unused).\n        \"\"\"\n        self._agent = None\n        self._store = None\n        self.worker_id = None\n        self._hooks = []\n\n        self._tracer.teardown()\n\n    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Teardown the runner for a specific worker.\n\n        This method cleans up worker-specific resources and resets the worker ID.\n\n        Args:\n            worker_id: The unique identifier of the worker being torn down.\n            *args: Additional teardown arguments (currently unused).\n            **kwargs: Additional teardown keyword arguments (currently unused).\n        \"\"\"\n        self.worker_id = None\n\n        self._tracer.teardown_worker(worker_id)\n\n    def get_agent(self) -&gt; LitAgent[T_task]:\n        \"\"\"Get the agent instance.\n\n        Returns:\n            The LitAgent instance managed by this runner.\n\n        Raises:\n            ValueError: If the agent has not been initialized via init().\n        \"\"\"\n        if self._agent is None:\n            raise ValueError(\"Agent not initialized. Call init() first.\")\n        return self._agent\n\n    def get_store(self) -&gt; LightningStore:\n        \"\"\"Get the store instance.\n\n        Returns:\n            The LightningStore instance for this worker.\n\n        Raises:\n            ValueError: If the store has not been initialized via init_worker().\n        \"\"\"\n        if self._store is None:\n            raise ValueError(\"Store not initialized. Call init_worker() first.\")\n        return self._store\n\n    def get_worker_id(self) -&gt; str:\n        \"\"\"Get the formatted worker ID string.\n\n        Returns:\n            A formatted string like \"Worker-0\" if initialized, or \"Worker-Unknown\"\n            if the worker ID has not been set.\n        \"\"\"\n        return f\"Worker-{self.worker_id}\" if self.worker_id is not None else \"Worker-Unknown\"\n\n    def _log_prefix(self, rollout_id: Optional[str] = None) -&gt; str:\n        \"\"\"Generate a standardized log prefix for the current worker.\n\n        This creates a consistent prefix format for log messages to identify\n        which worker and rollout the message is associated with.\n\n        Args:\n            rollout_id: Optional rollout ID to include in the prefix.\n\n        Returns:\n            A formatted log prefix string like \"[Worker 0 | Rollout xyz]\",\n            \"[Worker 0]\", \"[Rollout xyz]\", or \"[Default Worker]\".\n        \"\"\"\n        if self.worker_id is not None:\n            if rollout_id:\n                return f\"[Worker {self.worker_id} | Rollout {rollout_id}]\"\n            else:\n                return f\"[Worker {self.worker_id}]\"\n        if rollout_id:\n            return f\"[Rollout {rollout_id}]\"\n        return \"[Default Worker]\"\n\n    async def _trigger_hooks(\n        self,\n        hook_type: Literal[\"on_trace_start\", \"on_trace_end\", \"on_rollout_start\", \"on_rollout_end\"],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Trigger all registered hooks of a specific type.\n\n        This method calls the specified hook method on all registered hooks,\n        catching and logging any exceptions that occur during hook execution\n        to prevent them from disrupting the main execution flow.\n\n        Args:\n            hook_type: The type of hook to trigger. Valid values are:\n                \"on_trace_start\", \"on_trace_end\", \"on_rollout_start\", \"on_rollout_end\".\n            *args: Positional arguments to pass to the hook methods.\n            **kwargs: Keyword arguments to pass to the hook methods.\n        \"\"\"\n        for hook in self._hooks:\n            try:\n                await getattr(hook, hook_type)(*args, **kwargs)\n            except Exception:\n                logger.exception(f\"{self._log_prefix()} Exception during {hook_type} hook {hook}.\")\n\n    async def _post_process_rollout_result(\n        self, rollout: AttemptedRollout, raw_result: RolloutRawResultV2\n    ) -&gt; List[ReadableSpan] | List[Span]:\n        \"\"\"Standardizes the agent's return value and report what's needed to report to the store.\n\n        Args:\n            rollout: The rollout object for the current task.\n            raw_result: The output from the agent's rollout method.\n\n        Returns:\n            The spans that are assumed to be added to the store.\n            This only serves as an estimation for logging purposes. For precise tracking, use the store directly.\n        \"\"\"\n        store = self.get_store()\n\n        trace_spans: list[ReadableSpan] | list[Span] = []\n\n        # Case 0: result is None\n        if raw_result is None:\n            trace_spans = self._tracer.get_last_trace()\n\n        # Case 1: result is a float (final reward)\n        if isinstance(raw_result, float):\n            # Preserve the existing spans before another span is emitted\n            trace_spans = list(self._tracer.get_last_trace())\n            # This will emit another span to the tracer\n            reward_span = emit_reward(raw_result)\n            await store.add_otel_span(rollout.rollout_id, rollout.attempt.attempt_id, reward_span)\n            trace_spans.append(reward_span)\n\n        if isinstance(raw_result, list):\n            # For rollout methods that return a list, we assume that the returned spans\n            # are the complete span set from the whole rollout\n            trace_spans = raw_result\n\n            # Case 2: result is a list of ReadableSpan (OpenTelemetry spans)\n            if len(raw_result) &gt; 0 and all(isinstance(t, ReadableSpan) for t in raw_result):\n\n                if not isinstance(\n                    self._tracer, AgentOpsTracer\n                ):  # TODO: this should be replaced with general OpenTelemetry tracer in next version\n                    for span in raw_result:\n                        await store.add_otel_span(\n                            rollout.rollout_id, rollout.attempt.attempt_id, cast(ReadableSpan, span)\n                        )\n                else:\n                    logger.warning(\n                        f\"{self._log_prefix(rollout.rollout_id)} Tracer is already an OpenTelemetry tracer. \"\n                        \"The traces should have already been added to the store. \"\n                        \"No need to return anything from rollout.\"\n                    )\n\n            # Case 3: result is a list of Span (agentlightning spans)\n            elif len(raw_result) &gt; 0 and all(isinstance(t, Span) for t in raw_result):\n                # Add the spans directly to the store\n                for span in raw_result:\n                    await store.add_span(cast(Span, span))\n                trace_spans = raw_result\n\n            # Left over cases for list\n            elif len(raw_result) == 0:\n                logger.warning(\n                    f\"{self._log_prefix(rollout.rollout_id)} The rollout returns an empty list. \"\n                    \"Please check your rollout implementation.\"\n                )\n                trace_spans = raw_result\n\n            else:\n                types = [type(t).__name__ for t in raw_result][:10]\n                raise ValueError(\n                    f\"Invalid raw result type. It's expected to be a list of ReadableSpan or Span, \"\n                    f\"but got: {', '.join(types)}...\"\n                )\n\n        return trace_spans\n\n    async def _sleep_until_next_poll(self, event: Optional[Event] = None) -&gt; None:\n        \"\"\"Sleep until the next poll interval, with optional event-based interruption.\n\n        If an event is provided, the method will check it periodically (every 0.1s)\n        and return early if the event is set.\n\n        Args:\n            event: Optional Event object that can be used to interrupt the sleep.\n                If set during the sleep period, the method returns immediately.\n        \"\"\"\n        if event is None:\n            await asyncio.sleep(self._poll_interval)\n            return\n        current_time = time.time()\n        next_time = current_time + self._poll_interval\n        while time.time() &lt; next_time:\n            await asyncio.sleep(0.1)\n            if event.is_set():\n                return\n\n    async def _step_impl(self, next_rollout: AttemptedRollout, raise_on_exception: bool = False) -&gt; None:\n        \"\"\"Execute a single rollout implementation.\n\n        This is the core method that handles the execution of a single rollout,\n        including resource fetching, hook triggering, agent invocation, tracing,\n        and result processing.\n\n        Args:\n            next_rollout: The rollout to execute, containing input data, mode,\n                and resources information.\n            raise_on_exception: If True, exceptions during rollout execution will\n                be re-raised. If False, exceptions are logged but not propagated.\n        \"\"\"\n        store = self.get_store()\n        agent = self.get_agent()\n\n        rollout_id = next_rollout.rollout_id\n\n        resources_id = next_rollout.resources_id\n        resources_update = None\n        if resources_id:\n            resources_update = await store.get_resources_by_id(resources_id)\n        else:\n            logger.debug(f\"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.\")\n            resources_update = await store.get_latest_resources()\n        if not resources_update:\n            if raise_on_exception:\n                raise RuntimeError(f\"{self._log_prefix(rollout_id)} Failed to fetch resources\")\n            else:\n                logger.error(f\"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.\")\n                return\n\n        trace_spans: List[ReadableSpan] | List[Span] = []\n        has_exception: bool = False\n\n        try:\n            await self._trigger_hooks(hook_type=\"on_rollout_start\", agent=agent, runner=self, rollout=next_rollout)\n\n            start_time = time.time()\n            with self._tracer.trace_context(\n                name=rollout_id, store=store, rollout_id=rollout_id, attempt_id=next_rollout.attempt.attempt_id\n            ):\n                await self._trigger_hooks(\n                    hook_type=\"on_trace_start\", agent=agent, runner=self, tracer=self._tracer, rollout=next_rollout\n                )\n\n                # NOTE: This is the most costly step in the whole function\n                # If the rollout method becomes unresponsive or timeouts, there is nothing we can do within the runner.\n                # We might need some mechanisms in execution strategy to restart the runner. But that's a future work.\n                if agent.is_async:\n                    rollout_method = (\n                        agent.training_rollout_async if next_rollout.mode == \"train\" else agent.validation_rollout_async\n                    )\n                    result = await rollout_method(\n                        next_rollout.input, resources=resources_update.resources, rollout=next_rollout\n                    )\n                else:\n                    rollout_method = (\n                        agent.training_rollout if next_rollout.mode == \"train\" else agent.validation_rollout\n                    )\n                    result = rollout_method(\n                        next_rollout.input, resources=resources_update.resources, rollout=next_rollout\n                    )\n\n                await self._trigger_hooks(\n                    hook_type=\"on_trace_end\", agent=agent, runner=self, tracer=self._tracer, rollout=next_rollout\n                )\n\n            # Possible exceptions in post_process will be caught in the overall exception handler\n            trace_spans = await self._post_process_rollout_result(next_rollout, result)\n            last_reward = get_last_reward(trace_spans)\n\n            end_time = time.time()\n            logger.info(\n                f\"{self._log_prefix(rollout_id)} Completed in \"\n                f\"{end_time - start_time:.2f}s. Collected {len(trace_spans)} span(s). \"\n                f\"Final reward: {last_reward}\"\n            )\n\n        except Exception:\n            logger.exception(f\"{self._log_prefix(rollout_id)} Exception during rollout.\")\n            has_exception = True\n\n            if raise_on_exception:\n                raise\n        finally:\n            try:\n                await self._trigger_hooks(\n                    hook_type=\"on_rollout_end\", agent=agent, runner=self, rollout=next_rollout, spans=trace_spans\n                )\n            except Exception:\n                logger.exception(f\"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.\")\n\n            try:\n                if has_exception:\n                    # possibly timed out and cancelled?\n                    await store.update_attempt(rollout_id, next_rollout.attempt.attempt_id, status=\"failed\")\n                else:\n                    await store.update_attempt(rollout_id, next_rollout.attempt.attempt_id, status=\"succeeded\")\n            except Exception:\n                logger.exception(\n                    f\"{self._log_prefix(rollout_id)} Exception during update_attempt. Giving up the update.\"\n                )\n\n    async def iter(self, *, event: Optional[Event] = None) -&gt; None:\n        \"\"\"Run the runner, continuously iterating over tasks in the store.\n\n        This method polls the store for new rollouts and executes them until:\n        - The event is set (if provided)\n        - The max_rollouts limit is reached (if configured)\n        - No more tasks are available\n\n        All exceptions during rollout execution are caught and logged but not\n        propagated, allowing the runner to continue processing subsequent tasks.\n\n        Args:\n            event: Optional Event object to signal the runner to stop. The runner\n                will check this event periodically and stop gracefully when set.\n        \"\"\"\n        num_tasks_processed = 0\n        logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self._max_rollouts or 'unlimited'}).\")\n        store = self.get_store()\n\n        while not (event is not None and event.is_set()) and (\n            self._max_rollouts is None or num_tasks_processed &lt; self._max_rollouts\n        ):\n            # Retrieve the next rollout\n            next_rollout: Optional[RolloutV2] = None\n            while not (event is not None and event.is_set()):\n                logger.debug(f\"{self._log_prefix()} Try to poll for next rollout.\")\n                next_rollout = await store.dequeue_rollout()\n                if next_rollout is None:\n                    logger.debug(f\"{self._log_prefix()} No rollout to poll. Waiting for {self._poll_interval} seconds.\")\n                    await self._sleep_until_next_poll(event)\n                else:\n                    break\n\n            if next_rollout is None:\n                return\n\n            try:\n                # Claim the rollout but updating the current worker id\n                await store.update_attempt(\n                    next_rollout.rollout_id, next_rollout.attempt.attempt_id, worker_id=self.get_worker_id()\n                )\n            except Exception:\n                # This exception could happen if the rollout is dequeued and the other end died for some reason\n                logger.exception(f\"{self._log_prefix()} Exception during update_attempt, giving up the rollout.\")\n                continue\n\n            # Execute the step\n            await self._step_impl(next_rollout)\n\n            num_tasks_processed += 1\n            if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n                logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self._max_rollouts or 'unlimited'}\")\n\n        logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n\n    async def step(\n        self,\n        input: T_task,\n        *,\n        resources: Optional[NamedResources] = None,\n        mode: Optional[RolloutMode] = None,\n        event: Optional[Event] = None,\n    ) -&gt; None:\n        \"\"\"Execute a single task directly, bypassing the task queue.\n\n        This method creates a new rollout for the given input and executes it\n        immediately. Unlike iter(), exceptions are propagated to the caller.\n\n        Args:\n            input: The task input to be processed by the agent.\n            resources: Optional named resources to be used for this specific task.\n                If provided, a new resources entry will be created in the store.\n                If not provided, the latest resources from the store will be used.\n            mode: Optional rollout mode (\"train\" or \"validation\"). If not provided,\n                the agent's default mode will be used.\n            event: Optional Event object to signal interruption (currently unused\n                but included for interface consistency).\n\n        Raises:\n            Exception: Any exception that occurs during rollout execution will be\n                re-raised to the caller.\n        \"\"\"\n        store = self.get_store()\n\n        if resources is not None:\n            # TODO: move this to store.add_resources()\n            resources_id = \"resource-\" + str(uuid.uuid4())\n            await store.update_resources(resources_id=resources_id, resources=resources)\n        else:\n            resources_id = None\n\n        attempted_rollout = await self.get_store().start_rollout(input=input, mode=mode, resources_id=resources_id)\n        await self._step_impl(attempted_rollout, raise_on_exception=True)\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.__init__","title":"<code>__init__(tracer, max_rollouts=None, poll_interval=5.0)</code>","text":"<p>Initialize the agent runner.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>BaseTracer</code> <p>The tracer instance for recording execution traces and spans.</p> required <code>max_rollouts</code> <code>Optional[int]</code> <p>Maximum number of tasks to process in iter() mode. If None, the runner will continue indefinitely until interrupted.</p> <code>None</code> <code>poll_interval</code> <code>float</code> <p>Time in seconds to wait between polling attempts when no tasks are available in the store.</p> <code>5.0</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def __init__(self, tracer: BaseTracer, max_rollouts: Optional[int] = None, poll_interval: float = 5.0) -&gt; None:\n    \"\"\"Initialize the agent runner.\n\n    Args:\n        tracer: The tracer instance for recording execution traces and spans.\n        max_rollouts: Maximum number of tasks to process in iter() mode. If None,\n            the runner will continue indefinitely until interrupted.\n        poll_interval: Time in seconds to wait between polling attempts when\n            no tasks are available in the store.\n    \"\"\"\n    super().__init__()\n    self._tracer = tracer\n    self._max_rollouts = max_rollouts\n    self._poll_interval = poll_interval\n\n    # Set later\n    self._agent: Optional[LitAgent[T_task]] = None\n    self._hooks: Sequence[Hook] = []\n    self._store: Optional[LightningStore] = None\n    self.worker_id: Optional[int] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.get_agent","title":"<code>get_agent()</code>","text":"<p>Get the agent instance.</p> <p>Returns:</p> Type Description <code>LitAgent[T_task]</code> <p>The LitAgent instance managed by this runner.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the agent has not been initialized via init().</p> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def get_agent(self) -&gt; LitAgent[T_task]:\n    \"\"\"Get the agent instance.\n\n    Returns:\n        The LitAgent instance managed by this runner.\n\n    Raises:\n        ValueError: If the agent has not been initialized via init().\n    \"\"\"\n    if self._agent is None:\n        raise ValueError(\"Agent not initialized. Call init() first.\")\n    return self._agent\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.get_store","title":"<code>get_store()</code>","text":"<p>Get the store instance.</p> <p>Returns:</p> Type Description <code>LightningStore</code> <p>The LightningStore instance for this worker.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store has not been initialized via init_worker().</p> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def get_store(self) -&gt; LightningStore:\n    \"\"\"Get the store instance.\n\n    Returns:\n        The LightningStore instance for this worker.\n\n    Raises:\n        ValueError: If the store has not been initialized via init_worker().\n    \"\"\"\n    if self._store is None:\n        raise ValueError(\"Store not initialized. Call init_worker() first.\")\n    return self._store\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.get_worker_id","title":"<code>get_worker_id()</code>","text":"<p>Get the formatted worker ID string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string like \"Worker-0\" if initialized, or \"Worker-Unknown\"</p> <code>str</code> <p>if the worker ID has not been set.</p> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def get_worker_id(self) -&gt; str:\n    \"\"\"Get the formatted worker ID string.\n\n    Returns:\n        A formatted string like \"Worker-0\" if initialized, or \"Worker-Unknown\"\n        if the worker ID has not been set.\n    \"\"\"\n    return f\"Worker-{self.worker_id}\" if self.worker_id is not None else \"Worker-Unknown\"\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.init","title":"<code>init(agent, *, hooks=None, **kwargs)</code>","text":"<p>Initialize the runner with the agent.</p> <p>This sets up the agent-runner relationship, registers hooks, and initializes the tracer.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[T_task]</code> <p>The LitAgent instance to be managed by this runner.</p> required <code>hooks</code> <code>Optional[Sequence[Hook]]</code> <p>Optional sequence of Hook objects to be called at various lifecycle stages (on_trace_start, on_trace_end, on_rollout_start, on_rollout_end).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional initialization arguments (currently unused).</p> <code>{}</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def init(self, agent: LitAgent[T_task], *, hooks: Optional[Sequence[Hook]] = None, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the runner with the agent.\n\n    This sets up the agent-runner relationship, registers hooks, and\n    initializes the tracer.\n\n    Args:\n        agent: The LitAgent instance to be managed by this runner.\n        hooks: Optional sequence of Hook objects to be called at various\n            lifecycle stages (on_trace_start, on_trace_end, on_rollout_start,\n            on_rollout_end).\n        **kwargs: Additional initialization arguments (currently unused).\n    \"\"\"\n    self._agent = agent\n    self._agent.set_runner(self)\n    self._hooks = [*hooks] if hooks is not None else []\n\n    self._tracer.init()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Initialize the runner for each worker with worker_id and store.</p> <p>This method is called once per worker in a distributed setup to provide the worker with its ID and store connection.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>Unique identifier for this worker process.</p> required <code>store</code> <code>LightningStore</code> <p>The LightningStore instance for task coordination and data persistence.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional worker-specific initialization arguments (currently unused).</p> <code>{}</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def init_worker(self, worker_id: int, store: LightningStore, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the runner for each worker with worker_id and store.\n\n    This method is called once per worker in a distributed setup to provide\n    the worker with its ID and store connection.\n\n    Args:\n        worker_id: Unique identifier for this worker process.\n        store: The LightningStore instance for task coordination and data persistence.\n        **kwargs: Additional worker-specific initialization arguments (currently unused).\n    \"\"\"\n    self._store = store\n    self.worker_id = worker_id\n\n    self._tracer.init_worker(worker_id)\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method polls the store for new rollouts and executes them until: - The event is set (if provided) - The max_rollouts limit is reached (if configured) - No more tasks are available</p> <p>All exceptions during rollout execution are caught and logged but not propagated, allowing the runner to continue processing subsequent tasks.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Optional[Event]</code> <p>Optional Event object to signal the runner to stop. The runner will check this event periodically and stop gracefully when set.</p> <code>None</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>async def iter(self, *, event: Optional[Event] = None) -&gt; None:\n    \"\"\"Run the runner, continuously iterating over tasks in the store.\n\n    This method polls the store for new rollouts and executes them until:\n    - The event is set (if provided)\n    - The max_rollouts limit is reached (if configured)\n    - No more tasks are available\n\n    All exceptions during rollout execution are caught and logged but not\n    propagated, allowing the runner to continue processing subsequent tasks.\n\n    Args:\n        event: Optional Event object to signal the runner to stop. The runner\n            will check this event periodically and stop gracefully when set.\n    \"\"\"\n    num_tasks_processed = 0\n    logger.info(f\"{self._log_prefix()} Started async rollouts (max: {self._max_rollouts or 'unlimited'}).\")\n    store = self.get_store()\n\n    while not (event is not None and event.is_set()) and (\n        self._max_rollouts is None or num_tasks_processed &lt; self._max_rollouts\n    ):\n        # Retrieve the next rollout\n        next_rollout: Optional[RolloutV2] = None\n        while not (event is not None and event.is_set()):\n            logger.debug(f\"{self._log_prefix()} Try to poll for next rollout.\")\n            next_rollout = await store.dequeue_rollout()\n            if next_rollout is None:\n                logger.debug(f\"{self._log_prefix()} No rollout to poll. Waiting for {self._poll_interval} seconds.\")\n                await self._sleep_until_next_poll(event)\n            else:\n                break\n\n        if next_rollout is None:\n            return\n\n        try:\n            # Claim the rollout but updating the current worker id\n            await store.update_attempt(\n                next_rollout.rollout_id, next_rollout.attempt.attempt_id, worker_id=self.get_worker_id()\n            )\n        except Exception:\n            # This exception could happen if the rollout is dequeued and the other end died for some reason\n            logger.exception(f\"{self._log_prefix()} Exception during update_attempt, giving up the rollout.\")\n            continue\n\n        # Execute the step\n        await self._step_impl(next_rollout)\n\n        num_tasks_processed += 1\n        if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:\n            logger.info(f\"{self._log_prefix()} Progress: {num_tasks_processed}/{self._max_rollouts or 'unlimited'}\")\n\n    logger.info(f\"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task directly, bypassing the task queue.</p> <p>This method creates a new rollout for the given input and executes it immediately. Unlike iter(), exceptions are propagated to the caller.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>T_task</code> <p>The task input to be processed by the agent.</p> required <code>resources</code> <code>Optional[NamedResources]</code> <p>Optional named resources to be used for this specific task. If provided, a new resources entry will be created in the store. If not provided, the latest resources from the store will be used.</p> <code>None</code> <code>mode</code> <code>Optional[RolloutMode]</code> <p>Optional rollout mode (\"train\" or \"validation\"). If not provided, the agent's default mode will be used.</p> <code>None</code> <code>event</code> <code>Optional[Event]</code> <p>Optional Event object to signal interruption (currently unused but included for interface consistency).</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception that occurs during rollout execution will be re-raised to the caller.</p> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>async def step(\n    self,\n    input: T_task,\n    *,\n    resources: Optional[NamedResources] = None,\n    mode: Optional[RolloutMode] = None,\n    event: Optional[Event] = None,\n) -&gt; None:\n    \"\"\"Execute a single task directly, bypassing the task queue.\n\n    This method creates a new rollout for the given input and executes it\n    immediately. Unlike iter(), exceptions are propagated to the caller.\n\n    Args:\n        input: The task input to be processed by the agent.\n        resources: Optional named resources to be used for this specific task.\n            If provided, a new resources entry will be created in the store.\n            If not provided, the latest resources from the store will be used.\n        mode: Optional rollout mode (\"train\" or \"validation\"). If not provided,\n            the agent's default mode will be used.\n        event: Optional Event object to signal interruption (currently unused\n            but included for interface consistency).\n\n    Raises:\n        Exception: Any exception that occurs during rollout execution will be\n            re-raised to the caller.\n    \"\"\"\n    store = self.get_store()\n\n    if resources is not None:\n        # TODO: move this to store.add_resources()\n        resources_id = \"resource-\" + str(uuid.uuid4())\n        await store.update_resources(resources_id=resources_id, resources=resources)\n    else:\n        resources_id = None\n\n    attempted_rollout = await self.get_store().start_rollout(input=input, mode=mode, resources_id=resources_id)\n    await self._step_impl(attempted_rollout, raise_on_exception=True)\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Teardown the runner and clean up all resources.</p> <p>This method resets all internal state including the agent, store, hooks, and worker ID, and calls the tracer's teardown method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Additional teardown arguments (currently unused).</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional teardown keyword arguments (currently unused).</p> <code>{}</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Teardown the runner and clean up all resources.\n\n    This method resets all internal state including the agent, store,\n    hooks, and worker ID, and calls the tracer's teardown method.\n\n    Args:\n        *args: Additional teardown arguments (currently unused).\n        **kwargs: Additional teardown keyword arguments (currently unused).\n    \"\"\"\n    self._agent = None\n    self._store = None\n    self.worker_id = None\n    self._hooks = []\n\n    self._tracer.teardown()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.AgentRunnerV2.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Teardown the runner for a specific worker.</p> <p>This method cleans up worker-specific resources and resets the worker ID.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>The unique identifier of the worker being torn down.</p> required <code>*args</code> <code>Any</code> <p>Additional teardown arguments (currently unused).</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional teardown keyword arguments (currently unused).</p> <code>{}</code> Source code in <code>agentlightning/runner/agent.py</code> <pre><code>def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Teardown the runner for a specific worker.\n\n    This method cleans up worker-specific resources and resets the worker ID.\n\n    Args:\n        worker_id: The unique identifier of the worker being torn down.\n        *args: Additional teardown arguments (currently unused).\n        **kwargs: Additional teardown keyword arguments (currently unused).\n    \"\"\"\n    self.worker_id = None\n\n    self._tracer.teardown_worker(worker_id)\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner","title":"<code>BaseRunner</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code>, <code>Generic[T_task]</code></p> <p>Base class for all runners.</p> <p>This abstract base class defines the interface that all runner implementations must follow. Runners are responsible for executing agent tasks, managing the execution lifecycle, and coordinating with the store.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>class BaseRunner(ParallelWorkerBase, Generic[T_task]):\n    \"\"\"Base class for all runners.\n\n    This abstract base class defines the interface that all runner implementations\n    must follow. Runners are responsible for executing agent tasks, managing the\n    execution lifecycle, and coordinating with the store.\n    \"\"\"\n\n    def init(self, agent: LitAgent[T_task], **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the runner with the agent.\n\n        This method is called once during setup to configure the runner with\n        the agent it will execute.\n\n        Args:\n            agent: The LitAgent instance to be managed by this runner.\n            **kwargs: Additional initialization arguments specific to the runner implementation.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def init_worker(self, worker_id: int, store: LightningStore, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the runner for each worker with worker_id and store.\n\n        This method is called once per worker process in a distributed setup.\n        It provides the worker with its unique ID and the store instance for\n        task coordination.\n\n        Args:\n            worker_id: Unique identifier for this worker process.\n            store: The LightningStore instance for task coordination and data persistence.\n            **kwargs: Additional worker-specific initialization arguments.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def run(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Undefined method - use iter() or step() instead.\n\n        This method is intentionally not implemented as the execution behavior\n        should be defined through iter() for continuous execution or step()\n        for single-task execution.\n\n        Args:\n            *args: Unused positional arguments.\n            **kwargs: Unused keyword arguments.\n\n        Raises:\n            RuntimeError: Always raised to indicate this method should not be used.\n        \"\"\"\n        raise RuntimeError(\"The behavior of run() of Runner is undefined. Use iter() or step() instead.\")\n\n    def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Clean up runner resources and reset state.\n\n        This method is called once during shutdown to clean up any resources\n        allocated during initialization and reset the runner state.\n\n        Args:\n            *args: Additional teardown arguments.\n            **kwargs: Additional teardown keyword arguments.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Clean up worker-specific resources.\n\n        This method is called once per worker during shutdown to clean up\n        any resources specific to that worker.\n\n        Args:\n            worker_id: The unique identifier of the worker being torn down.\n            *args: Additional teardown arguments.\n            **kwargs: Additional teardown keyword arguments.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def iter(self, *, event: Optional[Event] = None) -&gt; None:\n        \"\"\"Run the runner, continuously iterating over tasks in the store.\n\n        This method runs in a loop, polling the store for new tasks and executing\n        them until interrupted by the event or when no more tasks are available.\n\n        Args:\n            event: Optional Event object that can be used to signal the runner\n                to stop gracefully. When set, the runner should finish its current\n                task and exit the iteration loop.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def step(\n        self,\n        input: T_task,\n        *,\n        resources: Optional[NamedResources] = None,\n        mode: Optional[RolloutMode] = None,\n        event: Optional[Event] = None,\n    ) -&gt; None:\n        \"\"\"Execute a single task with the given input.\n\n        This method provides fine-grained control for executing individual tasks\n        directly, bypassing the store's task queue.\n\n        Args:\n            input: The task input to be processed by the agent.\n            resources: Optional named resources to be used for this specific task.\n                If not provided, the latest resources from the store will be used.\n            mode: Optional rollout mode (e.g., \"train\", \"test\"). If not provided,\n                the default mode will be used.\n            event: Optional Event object to signal interruption. When set, the\n                runner may abort the current execution.\n\n        Raises:\n            NotImplementedError: Must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.init","title":"<code>init(agent, **kwargs)</code>","text":"<p>Initialize the runner with the agent.</p> <p>This method is called once during setup to configure the runner with the agent it will execute.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[T_task]</code> <p>The LitAgent instance to be managed by this runner.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional initialization arguments specific to the runner implementation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>def init(self, agent: LitAgent[T_task], **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the runner with the agent.\n\n    This method is called once during setup to configure the runner with\n    the agent it will execute.\n\n    Args:\n        agent: The LitAgent instance to be managed by this runner.\n        **kwargs: Additional initialization arguments specific to the runner implementation.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Initialize the runner for each worker with worker_id and store.</p> <p>This method is called once per worker process in a distributed setup. It provides the worker with its unique ID and the store instance for task coordination.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>Unique identifier for this worker process.</p> required <code>store</code> <code>LightningStore</code> <p>The LightningStore instance for task coordination and data persistence.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional worker-specific initialization arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>def init_worker(self, worker_id: int, store: LightningStore, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the runner for each worker with worker_id and store.\n\n    This method is called once per worker process in a distributed setup.\n    It provides the worker with its unique ID and the store instance for\n    task coordination.\n\n    Args:\n        worker_id: Unique identifier for this worker process.\n        store: The LightningStore instance for task coordination and data persistence.\n        **kwargs: Additional worker-specific initialization arguments.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method runs in a loop, polling the store for new tasks and executing them until interrupted by the event or when no more tasks are available.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Optional[Event]</code> <p>Optional Event object that can be used to signal the runner to stop gracefully. When set, the runner should finish its current task and exit the iteration loop.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>async def iter(self, *, event: Optional[Event] = None) -&gt; None:\n    \"\"\"Run the runner, continuously iterating over tasks in the store.\n\n    This method runs in a loop, polling the store for new tasks and executing\n    them until interrupted by the event or when no more tasks are available.\n\n    Args:\n        event: Optional Event object that can be used to signal the runner\n            to stop gracefully. When set, the runner should finish its current\n            task and exit the iteration loop.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Undefined method - use iter() or step() instead.</p> <p>This method is intentionally not implemented as the execution behavior should be defined through iter() for continuous execution or step() for single-task execution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Unused positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Always raised to indicate this method should not be used.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>def run(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Undefined method - use iter() or step() instead.\n\n    This method is intentionally not implemented as the execution behavior\n    should be defined through iter() for continuous execution or step()\n    for single-task execution.\n\n    Args:\n        *args: Unused positional arguments.\n        **kwargs: Unused keyword arguments.\n\n    Raises:\n        RuntimeError: Always raised to indicate this method should not be used.\n    \"\"\"\n    raise RuntimeError(\"The behavior of run() of Runner is undefined. Use iter() or step() instead.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task with the given input.</p> <p>This method provides fine-grained control for executing individual tasks directly, bypassing the store's task queue.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>T_task</code> <p>The task input to be processed by the agent.</p> required <code>resources</code> <code>Optional[NamedResources]</code> <p>Optional named resources to be used for this specific task. If not provided, the latest resources from the store will be used.</p> <code>None</code> <code>mode</code> <code>Optional[RolloutMode]</code> <p>Optional rollout mode (e.g., \"train\", \"test\"). If not provided, the default mode will be used.</p> <code>None</code> <code>event</code> <code>Optional[Event]</code> <p>Optional Event object to signal interruption. When set, the runner may abort the current execution.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>async def step(\n    self,\n    input: T_task,\n    *,\n    resources: Optional[NamedResources] = None,\n    mode: Optional[RolloutMode] = None,\n    event: Optional[Event] = None,\n) -&gt; None:\n    \"\"\"Execute a single task with the given input.\n\n    This method provides fine-grained control for executing individual tasks\n    directly, bypassing the store's task queue.\n\n    Args:\n        input: The task input to be processed by the agent.\n        resources: Optional named resources to be used for this specific task.\n            If not provided, the latest resources from the store will be used.\n        mode: Optional rollout mode (e.g., \"train\", \"test\"). If not provided,\n            the default mode will be used.\n        event: Optional Event object to signal interruption. When set, the\n            runner may abort the current execution.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Clean up runner resources and reset state.</p> <p>This method is called once during shutdown to clean up any resources allocated during initialization and reset the runner state.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Additional teardown arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional teardown keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Clean up runner resources and reset state.\n\n    This method is called once during shutdown to clean up any resources\n    allocated during initialization and reset the runner state.\n\n    Args:\n        *args: Additional teardown arguments.\n        **kwargs: Additional teardown keyword arguments.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.runner.BaseRunner.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Clean up worker-specific resources.</p> <p>This method is called once per worker during shutdown to clean up any resources specific to that worker.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>The unique identifier of the worker being torn down.</p> required <code>*args</code> <code>Any</code> <p>Additional teardown arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional teardown keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by subclasses.</p> Source code in <code>agentlightning/runner/base.py</code> <pre><code>def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Clean up worker-specific resources.\n\n    This method is called once per worker during shutdown to clean up\n    any resources specific to that worker.\n\n    Args:\n        worker_id: The unique identifier of the worker being torn down.\n        *args: Additional teardown arguments.\n        **kwargs: Additional teardown keyword arguments.\n\n    Raises:\n        NotImplementedError: Must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer","title":"<code>agentlightning.trainer</code>","text":""},{"location":"reference/core/#agentlightning.trainer.Trainer","title":"<code>Trainer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Orchestrates the distributed execution of agent rollouts.</p> <p>The Trainer is responsible for launching one or more worker processes that run the agent's execution loop. It manages multiprocessing, handles graceful shutdown, and serves as the main entry point for running a client-side agent fleet.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <p>An instance of <code>BaseAlgorithm</code> to use for training.</p> <code>store</code> <p>An instance of <code>LightningStore</code> to use for storing tasks and traces.</p> <code>runner</code> <p>An instance of <code>BaseRunner</code> to use for running the agent.</p> <code>dev</code> <p>If True, rollouts are run against the dev endpoint provided in <code>fit</code>.</p> <code>n_runners</code> <p>Number of agent runners to run in parallel.</p> <code>max_rollouts</code> <p>Maximum number of rollouts to process per runner. If None,           workers run until no more rollouts are available.</p> <code>strategy</code> <p>An instance of <code>ExecutionStrategy</code> to use for spawning the algorithm and runners.</p> <code>tracer</code> <p>A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key     that specifies the class full name and other initialization parameters.     If None, a default <code>AgentOpsTracer</code> will be created with the current settings.</p> <code>hooks</code> <p>A sequence of <code>Hook</code> instances to be called at various lifecycle stages (e.g., on_trace_start,    on_trace_end, on_rollout_start, on_rollout_end).</p> <code>adapter</code> <p>An instance of <code>TraceTripletAdapter</code> to export data consumble by algorithms from traces.</p> <code>llm_proxy</code> <p>An instance of <code>LLMProxy</code> to use for intercepting the LLM calls.        If not provided, algorithm will create one on its own.</p> <code>n_workers</code> <p>Number of agent workers to run in parallel. Deprecated in favor of <code>n_runners</code>.</p> <code>max_tasks</code> <p>Maximum number of tasks to process per runner. Deprecated in favor of <code>max_rollouts</code>.</p> <code>daemon</code> <p>Whether worker processes should be daemons. Daemon processes     are terminated automatically when the main process exits. Deprecated.     Only have effect with <code>fit_v0</code>.</p> <code>triplet_exporter</code> <p>An instance of <code>TraceTripletAdapter</code> to export triplets from traces,               or a dictionary with the initialization parameters for the exporter.               Deprecated. Use <code>adapter</code> instead.</p> Source code in <code>agentlightning/trainer/trainer.py</code> <pre><code>class Trainer(ParallelWorkerBase):\n    \"\"\"Orchestrates the distributed execution of agent rollouts.\n\n    The Trainer is responsible for launching one or more worker processes\n    that run the agent's execution loop. It manages multiprocessing,\n    handles graceful shutdown, and serves as the main entry point for\n    running a client-side agent fleet.\n\n    Attributes:\n        algorithm: An instance of `BaseAlgorithm` to use for training.\n        store: An instance of `LightningStore` to use for storing tasks and traces.\n        runner: An instance of `BaseRunner` to use for running the agent.\n        dev: If True, rollouts are run against the dev endpoint provided in `fit`.\n        n_runners: Number of agent runners to run in parallel.\n        max_rollouts: Maximum number of rollouts to process per runner. If None,\n                      workers run until no more rollouts are available.\n        strategy: An instance of `ExecutionStrategy` to use for spawning the algorithm and runners.\n        tracer: A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key\n                that specifies the class full name and other initialization parameters.\n                If None, a default `AgentOpsTracer` will be created with the current settings.\n        hooks: A sequence of `Hook` instances to be called at various lifecycle stages (e.g., on_trace_start,\n               on_trace_end, on_rollout_start, on_rollout_end).\n        adapter: An instance of `TraceTripletAdapter` to export data consumble by algorithms from traces.\n        llm_proxy: An instance of `LLMProxy` to use for intercepting the LLM calls.\n                   If not provided, algorithm will create one on its own.\n        n_workers: Number of agent workers to run in parallel. Deprecated in favor of `n_runners`.\n        max_tasks: Maximum number of tasks to process per runner. Deprecated in favor of `max_rollouts`.\n        daemon: Whether worker processes should be daemons. Daemon processes\n                are terminated automatically when the main process exits. Deprecated.\n                Only have effect with `fit_v0`.\n        triplet_exporter: An instance of `TraceTripletAdapter` to export triplets from traces,\n                          or a dictionary with the initialization parameters for the exporter.\n                          Deprecated. Use `adapter` instead.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dev: bool = False,\n        n_runners: Optional[int] = None,\n        max_rollouts: Optional[int] = None,\n        tracer: ComponentSpec[BaseTracer] = None,\n        adapter: ComponentSpec[TraceAdapter[Any]] = None,\n        store: ComponentSpec[LightningStore] = None,\n        runner: ComponentSpec[BaseRunner[Any]] = None,\n        strategy: ComponentSpec[ExecutionStrategy] = None,\n        algorithm: ComponentSpec[BaseAlgorithm] = None,\n        llm_proxy: ComponentSpec[LLMProxy] = None,\n        n_workers: Optional[int] = None,\n        max_tasks: Optional[int] = None,\n        daemon: bool = True,\n        triplet_exporter: ComponentSpec[TraceTripletAdapter] = None,\n        hooks: Optional[Union[Hook, Sequence[Hook]]] = None,\n    ):\n        super().__init__()\n        self.dev = dev\n        self.daemon = daemon\n        self._client: AgentLightningClient | None = None  # Will be initialized in fit or fit_v0\n\n        if n_workers is not None:\n            warnings.warn(\n                \"`n_workers` is deprecated. Please use `n_runners`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if n_runners is None:\n            n_runners = n_workers if n_workers is not None else 1\n        else:\n            if n_workers is not None and n_workers != n_runners:\n                warnings.warn(\n                    \"`n_workers` is ignored when `n_runners` is provided.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n        self.n_runners = n_runners\n        self.n_workers = n_runners  # Backwards compatibility for fit_v0\n\n        if max_tasks is not None:\n            warnings.warn(\n                \"`max_tasks` is deprecated. Please use `max_rollouts`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if max_rollouts is None:\n            max_rollouts = max_tasks\n        elif max_tasks is not None and max_tasks != max_rollouts:\n            warnings.warn(\n                \"`max_tasks` is ignored when `max_rollouts` is provided.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        self.max_rollouts = max_rollouts\n        self.max_tasks = max_tasks if max_tasks is not None else max_rollouts\n\n        self.tracer = self._make_tracer(tracer)\n\n        if adapter is not None and triplet_exporter is not None:\n            warnings.warn(\n                \"`triplet_exporter` is deprecated and ignored because `adapter` is provided.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        adapter_spec = adapter if adapter is not None else triplet_exporter\n        self.adapter = self._make_adapter(adapter_spec)\n        self.triplet_exporter = self.adapter  # Backwards compatibility\n\n        self.algorithm = self._make_algorithm(algorithm)\n\n        # The active store for the current execution context\n        self.store = self._make_store(store)\n        self.runner = self._make_runner(runner)\n\n        self.strategy = self._make_strategy(strategy, n_runners=self.n_runners)\n        if hasattr(self.strategy, \"n_runners\"):\n            strategy_runners = getattr(self.strategy, \"n_runners\")\n            if isinstance(strategy_runners, int) and strategy_runners &gt; 0:\n                self.n_runners = strategy_runners\n                self.n_workers = strategy_runners\n\n        self.llm_proxy = self._make_llm_proxy(llm_proxy, store=self.store)\n\n        self.hooks = self._normalize_hooks(hooks)\n\n        if not self.daemon:\n            logger.warning(\n                \"daemon=False. Worker processes are non-daemonic. \"\n                \"The worker processes will NOT be terminated when the main process exits. \"\n                \"The cleanup must be handled manually.\"\n            )\n\n    def _make_tracer(self, tracer: ComponentSpec[BaseTracer]) -&gt; BaseTracer:\n        \"\"\"Creates a tracer instance based on the provided configuration.\"\"\"\n        default_factory = lambda: AgentOpsTracer(\n            agentops_managed=True,\n            instrument_managed=True,\n            daemon=self.daemon,\n        )\n        return build_component(\n            tracer,\n            expected_type=BaseTracer,\n            spec_name=\"tracer\",\n            default_factory=default_factory,\n            dict_requires_type=True,\n            invalid_spec_error_fmt=\"Invalid tracer type: {actual_type}. Expected BaseTracer, str, dict, or None.\",\n            type_error_fmt=\"Tracer factory returned {type_name}, which is not a BaseTracer subclass.\",\n        )\n\n    def _make_algorithm(self, algorithm: ComponentSpec[BaseAlgorithm]) -&gt; Optional[BaseAlgorithm]:\n        \"\"\"Creates an algorithm instance based on the provided configuration.\"\"\"\n        return build_component(\n            algorithm,\n            expected_type=BaseAlgorithm,\n            spec_name=\"algorithm\",\n            allow_none=True,\n            invalid_spec_error_fmt=\"Invalid algorithm type: {actual_type}. Expected BaseAlgorithm, str, dict, or None.\",\n            type_error_fmt=\"Algorithm factory returned {type_name}, which is not a BaseAlgorithm subclass.\",\n        )\n\n    def _make_adapter(self, adapter: ComponentSpec[TraceAdapter[Any]]) -&gt; TraceAdapter[Any]:\n        return build_component(\n            adapter,\n            expected_type=TraceAdapter,\n            spec_name=\"adapter\",\n            default_factory=TraceTripletAdapter,\n            dict_requires_type=False,\n            dict_default_cls=TraceTripletAdapter,\n            invalid_spec_error_fmt=\"Invalid adapter type: {actual_type}. Expected TraceAdapter, dict, or None.\",\n            type_error_fmt=\"Adapter factory returned {type_name}, which is not a TraceAdapter subclass.\",\n        )\n\n    def _make_store(self, store: ComponentSpec[LightningStore]) -&gt; LightningStore:\n        return build_component(\n            store,\n            expected_type=LightningStore,\n            spec_name=\"store\",\n            default_factory=InMemoryLightningStore,\n            invalid_spec_error_fmt=\"Invalid store type: {actual_type}. Expected LightningStore, str, dict, or None.\",\n            type_error_fmt=\"Store factory returned {type_name}, which is not a LightningStore subclass.\",\n        )\n\n    def _make_strategy(\n        self,\n        strategy: ComponentSpec[ExecutionStrategy],\n        *,\n        n_runners: int,\n    ) -&gt; ExecutionStrategy:\n        if isinstance(strategy, ExecutionStrategy):\n            return strategy\n        optional_defaults: Dict[str, Callable[[], Any]] = {\"n_runners\": lambda: n_runners}\n\n        def default_factory() -&gt; ExecutionStrategy:\n            return ClientServerExecutionStrategy(n_runners=n_runners, role=\"both\")\n\n        return build_component(\n            strategy,\n            expected_type=ExecutionStrategy,\n            spec_name=\"strategy\",\n            default_factory=default_factory,\n            optional_defaults=optional_defaults,\n            invalid_spec_error_fmt=\"Invalid strategy type: {actual_type}. Expected ExecutionStrategy, str, dict, or None.\",\n            type_error_fmt=\"Strategy factory returned {type_name}, which is not an ExecutionStrategy subclass.\",\n            registry=ExecutionStrategyRegistry,\n        )\n\n    def _make_llm_proxy(\n        self,\n        llm_proxy: ComponentSpec[LLMProxy],\n        *,\n        store: LightningStore,\n    ) -&gt; Optional[LLMProxy]:\n        if isinstance(llm_proxy, LLMProxy):\n            return llm_proxy\n\n        optional_defaults: Dict[str, Callable[[], Any]] = {\"store\": lambda: store}\n        if isinstance(llm_proxy, dict):\n            llm_proxy = {**llm_proxy}\n            llm_proxy.setdefault(\"store\", store)\n\n        return build_component(\n            llm_proxy,\n            expected_type=LLMProxy,\n            spec_name=\"llm_proxy\",\n            allow_none=True,\n            optional_defaults=optional_defaults,\n            invalid_spec_error_fmt=\"Invalid llm_proxy type: {actual_type}. Expected LLMProxy, dict, str, or None.\",\n            type_error_fmt=\"llm_proxy factory returned {type_name}, which is not an LLMProxy subclass.\",\n        )\n\n    def _make_runner(self, runner: ComponentSpec[BaseRunner[Any]]) -&gt; BaseRunner[Any]:\n        optional_defaults: Dict[str, Callable[[], Any]] = {\"tracer\": lambda: self.tracer}\n        if self.max_rollouts is not None:\n            optional_defaults[\"max_rollouts\"] = lambda: self.max_rollouts\n\n        def default_runner_factory() -&gt; BaseRunner[Any]:\n            return instantiate_component(AgentRunnerV2, optional_defaults=optional_defaults)\n\n        return build_component(\n            runner,\n            expected_type=BaseRunner,\n            spec_name=\"runner\",\n            default_factory=default_runner_factory,\n            optional_defaults=optional_defaults,\n            invalid_spec_error_fmt=\"Invalid runner type: {actual_type}. Expected BaseRunner, callable, str, dict, or None.\",\n            type_error_fmt=\"Runner factory returned {type_name}, which is not a BaseRunner subclass.\",\n        )\n\n    def _normalize_hooks(self, hooks: Optional[Union[Hook, Sequence[Hook]]]) -&gt; Sequence[Hook]:\n        if hooks is None:\n            return ()\n        if isinstance(hooks, Hook):\n            return (hooks,)\n        return tuple(hooks)\n\n    def fit_v2(\n        self,\n        agent: LitAgent[T_co],\n        train_dataset: Optional[Dataset[T_co]] = None,\n        *,\n        val_dataset: Optional[Dataset[T_co]] = None,\n        dev_dataset: Optional[Dataset[T_co]] = None,\n    ) -&gt; None:\n        \"\"\"Run the training loop using the configured strategy, store, and runner.\"\"\"\n        agent.set_trainer(self)\n\n        algorithm_bundle = functools.partial(\n            self._algorithm_bundle,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            dev_dataset=dev_dataset,\n        )\n        runner_bundle = functools.partial(self._runner_bundle, agent=agent)\n\n        self.strategy.execute(algorithm_bundle, runner_bundle, self.store)\n\n    async def _algorithm_bundle(\n        self,\n        store: LightningStore,\n        event: Event,\n        train_dataset: Optional[Dataset[T_co]],\n        val_dataset: Optional[Dataset[T_co]],\n        dev_dataset: Optional[Dataset[T_co]],\n    ) -&gt; None:\n        if self.algorithm is not None:\n            self.algorithm.set_trainer(self)\n            self.algorithm.set_store(store)\n            self.algorithm.set_adapter(self.adapter)\n            if self.llm_proxy is not None:\n                self.llm_proxy.set_store(store)\n                self.algorithm.set_llm_proxy(self.llm_proxy)\n\n        if self.algorithm is None:\n            while not event.is_set():\n                await asyncio.sleep(0.1)\n            return\n        try:\n            if inspect.iscoroutinefunction(self.algorithm.run):\n                await self.algorithm.run(\n                    train_dataset=train_dataset,\n                    val_dataset=val_dataset,\n                    dev_dataset=dev_dataset,\n                )\n            else:\n                # This will block the event loop to maximize the debugging experience\n                # It's the responsibility of the execution strategy to enable async execution\n                self.algorithm.run(\n                    train_dataset=train_dataset,\n                    val_dataset=val_dataset,\n                    dev_dataset=dev_dataset,\n                )\n        except Exception:\n            logger.exception(\"Algorithm bundle encountered an error.\")\n            raise\n\n    async def _runner_bundle(self, store: LightningStore, worker_id: int, event: Event, agent: LitAgent[T_co]) -&gt; None:\n        runner_instance: BaseRunner[Any] | None = None\n        runner_initialized = False\n        worker_initialized = False\n        try:\n            # If not using shm execution strategy, we are already in the forked process\n            runner_instance = self.runner\n            runner_instance.init(agent=agent, hooks=self.hooks)\n            runner_initialized = True\n            runner_instance.init_worker(worker_id, store)\n            worker_initialized = True\n            await runner_instance.iter(event=event)\n        except Exception:\n            logger.exception(\"Runner bundle encountered an error (worker_id=%s).\", worker_id)\n            raise\n        finally:\n            if runner_instance is not None:\n                if worker_initialized:\n                    try:\n                        runner_instance.teardown_worker(worker_id)\n                    except Exception:\n                        logger.exception(\"Error during runner worker teardown (worker_id=%s).\", worker_id)\n                if runner_initialized:\n                    try:\n                        runner_instance.teardown()\n                    except Exception:\n                        logger.exception(\"Error during runner teardown (worker_id=%s).\", worker_id)\n\n    def _extract_client_from_data(\n        self, data: Union[str, AgentLightningClient, Dataset[Any]]\n    ) -&gt; Optional[AgentLightningClient]:\n        \"\"\"Extract client from data if it's a string URL or AgentLightningClient.\"\"\"\n        if isinstance(data, str):\n            if not data.startswith(\"http://\") and not data.startswith(\"https://\"):\n                raise ValueError(\"String data must be a valid URL starting with http:// or https://\")\n            return AgentLightningClient(endpoint=data)\n        elif isinstance(data, AgentLightningClient):\n            return data\n        return None\n\n    def _extract_dataset_from_data(\n        self, data: Union[str, AgentLightningClient, Dataset[Any]]\n    ) -&gt; Optional[Dataset[Any]]:\n        \"\"\"Extract dataset from data if it's a Dataset.\"\"\"\n        if isinstance(data, str) or isinstance(data, AgentLightningClient):\n            return None\n        return data\n\n    def _determine_backend(\n        self,\n        train_data: Union[str, AgentLightningClient, Dataset[Any]],\n        dev_data: Union[str, AgentLightningClient, Dataset[Any], None] = None,\n    ) -&gt; Union[str, AgentLightningClient]:\n        \"\"\"Determine which backend to use for initialization.\"\"\"\n        if self.dev:\n            if dev_data is None:\n                raise ValueError(\"dev_data must be provided when dev=True.\")\n            client = self._extract_client_from_data(dev_data)\n            if client is None:\n                raise ValueError(\"dev_data must be a string URL or AgentLightningClient when dev=True.\")\n            return client\n        else:\n            client = self._extract_client_from_data(train_data)\n            if client is None and self.algorithm is None:\n                raise ValueError(\n                    \"train_data must be a string URL or AgentLightningClient when no algorithm is provided.\"\n                )\n            elif client is None and self.algorithm is not None:\n                # Algorithm will be responsible for creating the client\n                client = self.algorithm.get_client()\n                logger.info(f\"Algorithm created client: {client}\")\n                return client\n            if client is None:\n                raise ValueError(\n                    \"train_data must be a string URL or AgentLightningClient when no algorithm is provided.\"\n                )\n            return client\n\n    def init(self, backend: Union[str, AgentLightningClient]) -&gt; None:\n        logger.info(f\"Initializing Trainer...\")\n\n        self._init_client(backend)\n\n        self.tracer.init()\n\n        logger.info(f\"Trainer main initialization complete.\")\n\n    def teardown(self) -&gt; None:\n        logger.info(f\"Cleaning up Trainer...\")\n        self.tracer.teardown()\n\n        self._client = None\n        logger.info(f\"Trainer main cleanup complete.\")\n\n    def client(self) -&gt; AgentLightningClient:\n        \"\"\"Returns the AgentLightningClient instance.\"\"\"\n        if self._client is None:\n            raise RuntimeError(\"AgentLightningClient has not been initialized. Call `init` first.\")\n        return self._client\n\n    def _init_client(self, backend: Union[str, AgentLightningClient]) -&gt; AgentLightningClient:\n        if self._client is None:\n            if isinstance(backend, AgentLightningClient):\n                logger.info(\"Using provided AgentLightningClient instance.\")\n                self._client = backend\n            else:\n                logger.info(f\"Initializing AgentLightningClient with endpoint: {backend}\")\n                if not isinstance(backend, str):  # type: ignore\n                    raise ValueError(\"backend must be a string URL or an AgentLightningClient instance.\")\n                if not backend.startswith(\"http://\") and not backend.startswith(\"https://\"):\n                    raise ValueError(\"backend must be a valid URL starting with http:// or https://\")\n                # Initialize the client with the provided backend URL\n                self._client = AgentLightningClient(endpoint=backend)\n        else:\n            logger.warning(\"AgentLightningClient already initialized. Returning existing instance.\")\n        return self._client\n\n    def _worker_main_loop(self, agent: LitAgent[Any], worker_id: int, is_async: bool):\n        \"\"\"The main function for each worker process.\n\n        This function initializes the client and the loop, then starts the\n        execution. It also configures process-specific settings like the\n        process title and signal handling.\n\n        Args:\n            agent: The `LitAgent` instance to run.\n            worker_id: The unique ID for this worker.\n            is_async: A boolean indicating if the async loop should be run.\n        \"\"\"\n        if self.n_workers &gt; 1:\n            import setproctitle\n\n            # Ignore Ctrl+C in worker processes; the main process handles it\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n            setproctitle.setproctitle(multiprocessing.current_process().name)\n\n        # Now we are in child processes, so we can safely set up the environment.\n        agent.set_trainer(self)\n        if not isinstance(self.triplet_exporter, TraceTripletAdapter):\n            raise ValueError(\"triplet_exporter must be a TraceTripletAdapter for the legacy trainer.\")\n        # TODO: this should be set elsewhere\n        if agent.trained_agents:\n            self.triplet_exporter.agent_match = agent.trained_agents\n        self._initialize_worker_env(worker_id)\n\n        mode = \"Async\" if is_async else \"Sync\"\n        logger.info(f\"[Worker {worker_id}] {mode} worker process started.\")\n\n        num_processed = 0\n\n        try:\n            client = self.client()\n            loop = AgentRunner(\n                agent=agent,\n                client=client,\n                tracer=self.tracer,\n                triplet_exporter=self.triplet_exporter,\n                max_tasks=self.max_tasks,\n                worker_id=worker_id,\n            )\n            loop.init_worker(worker_id)  # type: ignore\n            if is_async:\n                num_processed = asyncio.run(loop.iter_async())\n            else:\n                num_processed = loop.iter()\n        except Exception:\n            logger.exception(f\"[Worker {worker_id}] Unhandled exception in worker loop.\")\n        finally:\n            self._teardown_worker_env(worker_id)\n\n        return num_processed\n\n    def _initialize_worker_env(self, worker_id: int):\n        logger.info(f\"[Worker {worker_id}] Setting up trainer environment...\")  # worker_id included in process name\n        self.tracer.init_worker(worker_id)\n\n    def _teardown_worker_env(self, worker_id: int):\n        logger.info(f\"[Worker {worker_id}] Cleaning up trainer environment...\")\n        self.tracer.teardown_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Environment cleanup complete.\")\n\n    @staticmethod\n    def kill_orphaned_processes() -&gt; None:\n        \"\"\"\n        Kill any orphaned processes that may have been left behind by previous runs.\n        This is useful for cleaning up after crashes or unexpected exits.\n        \"\"\"\n        import psutil\n\n        for proc in psutil.process_iter():  # type: ignore\n            # check whether the process name matches\n            if proc.name().startswith(\"AgentLightning-\"):\n                proc.kill()\n\n    def _terminate_processes(self, processes: List[multiprocessing.Process]) -&gt; None:\n        if self.n_workers &gt; 1 and len(processes) &gt; 0:\n            for i, p in enumerate(processes):\n                if p.is_alive():\n                    logger.info(f\"Terminating worker {i} (name: {p.name}, PID: {p.pid})...\")\n                    p.terminate()\n                else:\n                    logger.info(f\"Worker {i} (name: {p.name}, PID: {p.pid}) is not alive or has already terminated.\")\n            for i, p in enumerate(processes):\n                if p.is_alive():\n                    p.join(timeout=10)  # Give some time to terminate\n                if p.is_alive():  # If still alive, kill\n                    logger.warning(\n                        f\"Worker {i} (name: {p.name}, PID: {p.pid}) did not terminate gracefully, killing...\"\n                    )\n                    p.kill()\n                    p.join(timeout=10)  # Ensure it's reaped\n\n    def fit(\n        self,\n        agent: LitAgent[T_co],\n        train_data: Union[str, AgentLightningClient, Dataset[T_co]],\n        *,\n        val_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n        dev_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n        dev_backend: Union[str, AgentLightningClient, None] = None,\n    ):\n        \"\"\"Train the agent using the provided data.\n\n        Each data argument can be a string URL connecting to a agent-lightning server,\n        or an AgentLightningClient instance connecting to a server (or mock server), or a dataset.\n        If no algorithm is provided when instantiating the trainer, the data must be\n        provided to connecting a server. Otherwise, dataset is also allowed and will be\n        passed to the algorithm.\n\n        If the algorithm is instantiated and there is no URL/client provided,\n        the algorithm will be responsible for creating a client that will connect to itself.\n        It can also create a mock client if the algorithm does not require a server.\n        \"\"\"\n\n        if dev_backend is not None:\n            warnings.warn(\"dev_backend is deprecated. Use dev_data instead.\")\n            if dev_data is not None:\n                raise ValueError(\"dev_data and dev_backend cannot be provided at the same time.\")\n            dev_data = dev_backend\n\n        # Extract datasets for algorithm if available\n        train_dataset = self._extract_dataset_from_data(train_data)\n        val_dataset = self._extract_dataset_from_data(val_data) if val_data else None\n        dev_dataset = self._extract_dataset_from_data(dev_data) if dev_data else None\n\n        # Initialize the algorithm with trainer if provided\n        if self.algorithm is not None:\n            self.algorithm.set_trainer(self)\n            # DO NOT RUN TRAINING HERE. Need to spawn the worker first.\n\n        # Determine the backend to use for client-server mode\n        backend = self._determine_backend(train_data, dev_data)\n\n        if self.dev:\n            logger.warning(f\"Running in dev mode. Using dev backend: {backend}\")\n        else:\n            logger.debug(f\"Running in non-dev mode. Using backend: {backend}\")\n\n        self.init(backend)\n\n        processes: List[multiprocessing.Process] = []\n\n        # Determine if the agent is asynchronous\n\n        mode = \"asynchronous\" if agent.is_async else \"synchronous\"\n\n        try:\n            if self.n_workers == 1:\n                logger.info(f\"Running with n_workers=1 ({mode} in main process).\")\n\n                # Warn if algorithm is set with single worker mode\n                if self.algorithm is not None:\n                    logger.warning(\n                        \"Algorithm is set but using single worker mode. Algorithm will never get the chance to run.\"\n                    )\n                    # Ideally the single worker should be run in a separate thread or process.\n\n                num_tasks = self._worker_main_loop(agent, 0, agent.is_async)\n                logger.info(f\"Single worker mode finished. Tasks processed: {num_tasks}\")\n\n                # If algorithm is provided and we have datasets, run algorithm after worker completes\n                if self.algorithm is not None and train_dataset is not None:\n                    logger.info(\"Running algorithm training after worker completion.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        val_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n            else:\n                logger.info(f\"Running with n_workers={self.n_workers} ({mode} multiprocessing).\")\n                for i in range(self.n_workers):\n                    process_name = f\"AgentLightning-Worker-{i}\"\n                    p = multiprocessing.Process(\n                        target=self._worker_main_loop,\n                        args=(agent, i, agent.is_async),\n                        daemon=self.daemon,\n                        name=process_name,\n                    )\n                    processes.append(p)\n                    logger.info(f\"Starting worker process {i} (name: {process_name})...\")\n                    p.start()\n\n                if self.daemon:\n                    # If algorithm is provided and we have datasets, pass them to the algorithm\n                    if self.algorithm is not None:\n                        logger.info(\"All workers have been spawned. Running algorithm training with provided datasets.\")\n                        self.algorithm.run(\n                            train_dataset=train_dataset,\n                            val_dataset=val_dataset,\n                            dev_dataset=dev_dataset,\n                        )\n                        logger.info(\"Algorithm exits. Killing the workers.\")\n                        self._terminate_processes(processes)\n\n                    for i, p in enumerate(processes):\n                        p.join()  # Wait for the process to complete\n                        logger.info(\n                            f\"Worker process {i} (name: {p.name}, PID: {p.pid}) joined with exit code {p.exitcode}.\"\n                        )\n                        if p.exitcode != 0:\n                            logger.warning(\n                                f\"Worker process {i} (name: {p.name}, PID: {p.pid}) exited with non-zero code: {p.exitcode}.\"\n                            )\n\n                    logger.info(f\"All {self.n_workers} worker processes have completed.\")\n                else:\n                    logger.info(\"All worker processes started. Main process will not wait.\")\n\n                    # A hack to stop the main process from waiting for child processes to finish.\n                    time.sleep(1)  # Give workers time to start\n                    import multiprocessing.process as multiprocessing_process\n\n                    multiprocessing_process._children.clear()  # type: ignore\n\n                    if self.algorithm is not None:\n                        logger.info(\"Main process continues to run algorithm.\")\n                        self.algorithm.run(\n                            train_dataset=train_dataset,\n                            val_dataset=val_dataset,\n                            dev_dataset=dev_dataset,\n                        )\n                        logger.info(\"Algorithm exits. Killing the workers.\")\n                        self._terminate_processes(processes)\n\n        except KeyboardInterrupt:\n            logger.info(\"KeyboardInterrupt received. Killing the workers.\")\n            self._terminate_processes(processes)\n            logger.info(f\"Workers terminated or single worker interrupted.\")\n            raise\n        except Exception:\n            logger.exception(f\"Unhandled exception in fit method.\")\n            self._terminate_processes(processes)\n            logger.info(f\"Workers terminated or single worker interrupted.\")\n            raise\n        finally:\n            if self.daemon:\n                self.teardown()\n            else:\n                logger.info(\"Main process exiting. Please use Trainer.kill_orphaned_processes() for cleanup.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.client","title":"<code>client()</code>","text":"<p>Returns the AgentLightningClient instance.</p> Source code in <code>agentlightning/trainer/trainer.py</code> <pre><code>def client(self) -&gt; AgentLightningClient:\n    \"\"\"Returns the AgentLightningClient instance.\"\"\"\n    if self._client is None:\n        raise RuntimeError(\"AgentLightningClient has not been initialized. Call `init` first.\")\n    return self._client\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.fit","title":"<code>fit(agent, train_data, *, val_data=None, dev_data=None, dev_backend=None)</code>","text":"<p>Train the agent using the provided data.</p> <p>Each data argument can be a string URL connecting to a agent-lightning server, or an AgentLightningClient instance connecting to a server (or mock server), or a dataset. If no algorithm is provided when instantiating the trainer, the data must be provided to connecting a server. Otherwise, dataset is also allowed and will be passed to the algorithm.</p> <p>If the algorithm is instantiated and there is no URL/client provided, the algorithm will be responsible for creating a client that will connect to itself. It can also create a mock client if the algorithm does not require a server.</p> Source code in <code>agentlightning/trainer/trainer.py</code> <pre><code>def fit(\n    self,\n    agent: LitAgent[T_co],\n    train_data: Union[str, AgentLightningClient, Dataset[T_co]],\n    *,\n    val_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n    dev_data: Union[str, AgentLightningClient, Dataset[T_co], None] = None,\n    dev_backend: Union[str, AgentLightningClient, None] = None,\n):\n    \"\"\"Train the agent using the provided data.\n\n    Each data argument can be a string URL connecting to a agent-lightning server,\n    or an AgentLightningClient instance connecting to a server (or mock server), or a dataset.\n    If no algorithm is provided when instantiating the trainer, the data must be\n    provided to connecting a server. Otherwise, dataset is also allowed and will be\n    passed to the algorithm.\n\n    If the algorithm is instantiated and there is no URL/client provided,\n    the algorithm will be responsible for creating a client that will connect to itself.\n    It can also create a mock client if the algorithm does not require a server.\n    \"\"\"\n\n    if dev_backend is not None:\n        warnings.warn(\"dev_backend is deprecated. Use dev_data instead.\")\n        if dev_data is not None:\n            raise ValueError(\"dev_data and dev_backend cannot be provided at the same time.\")\n        dev_data = dev_backend\n\n    # Extract datasets for algorithm if available\n    train_dataset = self._extract_dataset_from_data(train_data)\n    val_dataset = self._extract_dataset_from_data(val_data) if val_data else None\n    dev_dataset = self._extract_dataset_from_data(dev_data) if dev_data else None\n\n    # Initialize the algorithm with trainer if provided\n    if self.algorithm is not None:\n        self.algorithm.set_trainer(self)\n        # DO NOT RUN TRAINING HERE. Need to spawn the worker first.\n\n    # Determine the backend to use for client-server mode\n    backend = self._determine_backend(train_data, dev_data)\n\n    if self.dev:\n        logger.warning(f\"Running in dev mode. Using dev backend: {backend}\")\n    else:\n        logger.debug(f\"Running in non-dev mode. Using backend: {backend}\")\n\n    self.init(backend)\n\n    processes: List[multiprocessing.Process] = []\n\n    # Determine if the agent is asynchronous\n\n    mode = \"asynchronous\" if agent.is_async else \"synchronous\"\n\n    try:\n        if self.n_workers == 1:\n            logger.info(f\"Running with n_workers=1 ({mode} in main process).\")\n\n            # Warn if algorithm is set with single worker mode\n            if self.algorithm is not None:\n                logger.warning(\n                    \"Algorithm is set but using single worker mode. Algorithm will never get the chance to run.\"\n                )\n                # Ideally the single worker should be run in a separate thread or process.\n\n            num_tasks = self._worker_main_loop(agent, 0, agent.is_async)\n            logger.info(f\"Single worker mode finished. Tasks processed: {num_tasks}\")\n\n            # If algorithm is provided and we have datasets, run algorithm after worker completes\n            if self.algorithm is not None and train_dataset is not None:\n                logger.info(\"Running algorithm training after worker completion.\")\n                self.algorithm.run(\n                    train_dataset=train_dataset,\n                    val_dataset=val_dataset,\n                    dev_dataset=dev_dataset,\n                )\n        else:\n            logger.info(f\"Running with n_workers={self.n_workers} ({mode} multiprocessing).\")\n            for i in range(self.n_workers):\n                process_name = f\"AgentLightning-Worker-{i}\"\n                p = multiprocessing.Process(\n                    target=self._worker_main_loop,\n                    args=(agent, i, agent.is_async),\n                    daemon=self.daemon,\n                    name=process_name,\n                )\n                processes.append(p)\n                logger.info(f\"Starting worker process {i} (name: {process_name})...\")\n                p.start()\n\n            if self.daemon:\n                # If algorithm is provided and we have datasets, pass them to the algorithm\n                if self.algorithm is not None:\n                    logger.info(\"All workers have been spawned. Running algorithm training with provided datasets.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        val_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n                    logger.info(\"Algorithm exits. Killing the workers.\")\n                    self._terminate_processes(processes)\n\n                for i, p in enumerate(processes):\n                    p.join()  # Wait for the process to complete\n                    logger.info(\n                        f\"Worker process {i} (name: {p.name}, PID: {p.pid}) joined with exit code {p.exitcode}.\"\n                    )\n                    if p.exitcode != 0:\n                        logger.warning(\n                            f\"Worker process {i} (name: {p.name}, PID: {p.pid}) exited with non-zero code: {p.exitcode}.\"\n                        )\n\n                logger.info(f\"All {self.n_workers} worker processes have completed.\")\n            else:\n                logger.info(\"All worker processes started. Main process will not wait.\")\n\n                # A hack to stop the main process from waiting for child processes to finish.\n                time.sleep(1)  # Give workers time to start\n                import multiprocessing.process as multiprocessing_process\n\n                multiprocessing_process._children.clear()  # type: ignore\n\n                if self.algorithm is not None:\n                    logger.info(\"Main process continues to run algorithm.\")\n                    self.algorithm.run(\n                        train_dataset=train_dataset,\n                        val_dataset=val_dataset,\n                        dev_dataset=dev_dataset,\n                    )\n                    logger.info(\"Algorithm exits. Killing the workers.\")\n                    self._terminate_processes(processes)\n\n    except KeyboardInterrupt:\n        logger.info(\"KeyboardInterrupt received. Killing the workers.\")\n        self._terminate_processes(processes)\n        logger.info(f\"Workers terminated or single worker interrupted.\")\n        raise\n    except Exception:\n        logger.exception(f\"Unhandled exception in fit method.\")\n        self._terminate_processes(processes)\n        logger.info(f\"Workers terminated or single worker interrupted.\")\n        raise\n    finally:\n        if self.daemon:\n            self.teardown()\n        else:\n            logger.info(\"Main process exiting. Please use Trainer.kill_orphaned_processes() for cleanup.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.fit_v2","title":"<code>fit_v2(agent, train_dataset=None, *, val_dataset=None, dev_dataset=None)</code>","text":"<p>Run the training loop using the configured strategy, store, and runner.</p> Source code in <code>agentlightning/trainer/trainer.py</code> <pre><code>def fit_v2(\n    self,\n    agent: LitAgent[T_co],\n    train_dataset: Optional[Dataset[T_co]] = None,\n    *,\n    val_dataset: Optional[Dataset[T_co]] = None,\n    dev_dataset: Optional[Dataset[T_co]] = None,\n) -&gt; None:\n    \"\"\"Run the training loop using the configured strategy, store, and runner.\"\"\"\n    agent.set_trainer(self)\n\n    algorithm_bundle = functools.partial(\n        self._algorithm_bundle,\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        dev_dataset=dev_dataset,\n    )\n    runner_bundle = functools.partial(self._runner_bundle, agent=agent)\n\n    self.strategy.execute(algorithm_bundle, runner_bundle, self.store)\n</code></pre>"},{"location":"reference/core/#agentlightning.trainer.Trainer.kill_orphaned_processes","title":"<code>kill_orphaned_processes()</code>  <code>staticmethod</code>","text":"<p>Kill any orphaned processes that may have been left behind by previous runs. This is useful for cleaning up after crashes or unexpected exits.</p> Source code in <code>agentlightning/trainer/trainer.py</code> <pre><code>@staticmethod\ndef kill_orphaned_processes() -&gt; None:\n    \"\"\"\n    Kill any orphaned processes that may have been left behind by previous runs.\n    This is useful for cleaning up after crashes or unexpected exits.\n    \"\"\"\n    import psutil\n\n    for proc in psutil.process_iter():  # type: ignore\n        # check whether the process name matches\n        if proc.name().startswith(\"AgentLightning-\"):\n            proc.kill()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer","title":"<code>agentlightning.tracer</code>","text":""},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer","title":"<code>AgentOpsTracer</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Traces agent execution using AgentOps.</p> <p>This tracer provides functionality to capture execution details using the AgentOps library. It manages the AgentOps client initialization, server setup, and integration with the OpenTelemetry tracing ecosystem.</p> <p>Attributes:</p> Name Type Description <code>agentops_managed</code> <p>Whether to automatically manage <code>agentops</code>.               When set to true, tracer calls <code>agentops.init()</code>               automatically and launches an agentops endpoint locally.               If not, you are responsible for calling and using it               before using the tracer.</p> <code>instrument_managed</code> <p>Whether to automatically manage instrumentation.                 When set to false, you will manage the instrumentation                 yourself and the tracer might not work as expected.</p> <code>daemon</code> <p>Whether the AgentOps server runs as a daemon process.     Only applicable if <code>agentops_managed</code> is True.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>class AgentOpsTracer(BaseTracer):\n    \"\"\"Traces agent execution using AgentOps.\n\n    This tracer provides functionality to capture execution details using the\n    AgentOps library. It manages the AgentOps client initialization, server setup,\n    and integration with the OpenTelemetry tracing ecosystem.\n\n    Attributes:\n        agentops_managed: Whether to automatically manage `agentops`.\n                          When set to true, tracer calls `agentops.init()`\n                          automatically and launches an agentops endpoint locally.\n                          If not, you are responsible for calling and using it\n                          before using the tracer.\n        instrument_managed: Whether to automatically manage instrumentation.\n                            When set to false, you will manage the instrumentation\n                            yourself and the tracer might not work as expected.\n        daemon: Whether the AgentOps server runs as a daemon process.\n                Only applicable if `agentops_managed` is True.\n    \"\"\"\n\n    def __init__(self, *, agentops_managed: bool = True, instrument_managed: bool = True, daemon: bool = True):\n        super().__init__()\n        self._lightning_span_processor: Optional[LightningSpanProcessor] = None\n        self.agentops_managed = agentops_managed\n        self.instrument_managed = instrument_managed\n        self.daemon = daemon\n\n        self._agentops_server_manager = AgentOpsServerManager(self.daemon)\n        self._agentops_server_port_val: Optional[int] = None\n\n        if not self.agentops_managed:\n            logger.warning(\"agentops_managed=False. You are responsible for AgentOps setup.\")\n        if not self.instrument_managed:\n            logger.warning(\"instrument_managed=False. You are responsible for all instrumentation.\")\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"_agentops_server_manager\"] = None  # Exclude the unpicklable server manager\n        # _agentops_server_port_val (int) is inherently picklable and will be included.\n        logger.debug(f\"Getting state for pickling Trainer (PID {os.getpid()}). _agentops_server_manager excluded.\")\n        return state\n\n    def __setstate__(self, state: Any):\n        self.__dict__.update(state)\n        # In child process, self._agentops_server_manager will be None.\n        logger.debug(f\"Setting state for unpickled Trainer (PID {os.getpid()}). _agentops_server_manager is None.\")\n\n    def init(self, *args: Any, **kwargs: Any):\n        if self.agentops_managed and self._agentops_server_manager:\n            self._agentops_server_manager.start()\n            self._agentops_server_port_val = self._agentops_server_manager.get_port()\n            if self._agentops_server_port_val is None:\n                if (\n                    self._agentops_server_manager.server_process is not None\n                    and self._agentops_server_manager.server_process.is_alive()\n                ):\n                    raise RuntimeError(\"AgentOps server started but port is None. Check server manager logic.\")\n                elif (\n                    self._agentops_server_port_val is None and self._agentops_server_manager.server_process is None\n                ):  # Server failed to start\n                    raise RuntimeError(\"AgentOps server manager indicates server is not running and port is None.\")\n\n    def teardown(self):\n        if self.agentops_managed:\n            self._agentops_server_manager.stop()\n            logger.info(\"AgentOps server stopped.\")\n\n    def instrument(self, worker_id: int):\n        instrument_all()\n\n    def uninstrument(self, worker_id: int):\n        uninstrument_all()\n\n    def init_worker(self, worker_id: int):\n        super().init_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Setting up tracer...\")  # worker_id included in process name\n\n        if self.instrument_managed:\n            self.instrument(worker_id)\n            logger.info(f\"[Worker {worker_id}] Instrumentation applied.\")\n\n        if self.agentops_managed:\n            if self._agentops_server_port_val:  # Use the stored, picklable port value\n                base_url = f\"http://localhost:{self._agentops_server_port_val}\"\n                env_vars_to_set = {\n                    \"AGENTOPS_API_KEY\": \"dummy\",\n                    \"AGENTOPS_API_ENDPOINT\": base_url,\n                    \"AGENTOPS_APP_URL\": f\"{base_url}/notavailable\",\n                    \"AGENTOPS_EXPORTER_ENDPOINT\": f\"{base_url}/traces\",\n                }\n                for key, value in env_vars_to_set.items():\n                    os.environ[key] = value\n                    logger.info(f\"[Worker {worker_id}] Env var set: {key}={value}\")\n            else:\n                logger.warning(\n                    f\"[Worker {worker_id}] AgentOps managed, but local server port is not available. Client may not connect as expected.\"\n                )\n\n            if not agentops.get_client().initialized:\n                agentops.init()  # type: ignore\n                logger.info(f\"[Worker {worker_id}] AgentOps client initialized.\")\n            else:\n                logger.warning(f\"[Worker {worker_id}] AgentOps client was already initialized.\")\n\n        self._lightning_span_processor = LightningSpanProcessor()\n\n        try:\n            # new versions\n            instance = agentops.sdk.core.tracer\n            # TODO: The span processor cannot be deleted once added.\n            # This might be a problem if the tracer is entered and exited multiple times.\n            instance.provider.add_span_processor(self._lightning_span_processor)  # type: ignore\n        except AttributeError:\n            # old versions\n            instance = TracingCore.get_instance()  # type: ignore\n            instance._provider.add_span_processor(self._lightning_span_processor)  # type: ignore\n\n    def teardown_worker(self, worker_id: int) -&gt; None:\n        super().teardown_worker(worker_id)\n\n        if self.instrument_managed:\n            self.uninstrument(worker_id)\n            logger.info(f\"[Worker {worker_id}] Instrumentation removed.\")\n\n    @contextmanager\n    def trace_context(\n        self,\n        name: Optional[str] = None,\n        *,\n        store: Optional[LightningStore] = None,\n        rollout_id: Optional[str] = None,\n        attempt_id: Optional[str] = None,\n    ) -&gt; Iterator[LightningSpanProcessor]:\n        \"\"\"\n        Starts a new tracing context. This should be used as a context manager.\n\n        Args:\n            name: Optional name for the tracing context.\n            store: Optional store to add the spans to.\n            rollout_id: Optional rollout ID to add the spans to.\n            attempt_id: Optional attempt ID to add the spans to.\n\n        Yields:\n            The LightningSpanProcessor instance to collect spans.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n        if store is not None and rollout_id is not None and attempt_id is not None:\n            ctx = self._lightning_span_processor.with_context(store=store, rollout_id=rollout_id, attempt_id=attempt_id)\n            with ctx as processor:\n                yield processor\n        elif store is None and rollout_id is None and attempt_id is None:\n            with self._lightning_span_processor:\n                yield self._lightning_span_processor\n        else:\n            raise ValueError(\"store, rollout_id, and attempt_id must be either all provided or all None\")\n\n    def get_last_trace(self) -&gt; List[ReadableSpan]:\n        \"\"\"\n        Retrieves the raw list of captured spans from the most recent trace.\n\n        Returns:\n            A list of OpenTelemetry `ReadableSpan` objects.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n        return self._lightning_span_processor.spans()\n\n    def get_langchain_callback_handler(self, tags: List[str] | None = None) -&gt; LangchainCallbackHandler:\n        \"\"\"\n        Get the Langchain callback handler for integrating with Langchain.\n\n        Args:\n            tags: Optional list of tags to apply to the Langchain callback handler.\n\n        Returns:\n            An instance of the Langchain callback handler.\n        \"\"\"\n        import agentops\n        from agentops.integration.callbacks.langchain import LangchainCallbackHandler\n\n        tags = tags or []\n        client_instance = agentops.get_client()\n        api_key = None\n        if client_instance.initialized:\n            api_key = client_instance.config.api_key\n        else:\n            logger.warning(\n                \"AgentOps client not initialized when creating LangchainCallbackHandler. API key may be missing.\"\n            )\n        return LangchainCallbackHandler(api_key=api_key, tags=tags)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.get_langchain_callback_handler","title":"<code>get_langchain_callback_handler(tags=None)</code>","text":"<p>Get the Langchain callback handler for integrating with Langchain.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>List[str] | None</code> <p>Optional list of tags to apply to the Langchain callback handler.</p> <code>None</code> <p>Returns:</p> Type Description <code>LangchainCallbackHandler</code> <p>An instance of the Langchain callback handler.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>def get_langchain_callback_handler(self, tags: List[str] | None = None) -&gt; LangchainCallbackHandler:\n    \"\"\"\n    Get the Langchain callback handler for integrating with Langchain.\n\n    Args:\n        tags: Optional list of tags to apply to the Langchain callback handler.\n\n    Returns:\n        An instance of the Langchain callback handler.\n    \"\"\"\n    import agentops\n    from agentops.integration.callbacks.langchain import LangchainCallbackHandler\n\n    tags = tags or []\n    client_instance = agentops.get_client()\n    api_key = None\n    if client_instance.initialized:\n        api_key = client_instance.config.api_key\n    else:\n        logger.warning(\n            \"AgentOps client not initialized when creating LangchainCallbackHandler. API key may be missing.\"\n        )\n    return LangchainCallbackHandler(api_key=api_key, tags=tags)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> Type Description <code>List[ReadableSpan]</code> <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>def get_last_trace(self) -&gt; List[ReadableSpan]:\n    \"\"\"\n    Retrieves the raw list of captured spans from the most recent trace.\n\n    Returns:\n        A list of OpenTelemetry `ReadableSpan` objects.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n    return self._lightning_span_processor.spans()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.AgentOpsTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the tracing context.</p> <code>None</code> <code>store</code> <code>Optional[LightningStore]</code> <p>Optional store to add the spans to.</p> <code>None</code> <code>rollout_id</code> <code>Optional[str]</code> <p>Optional rollout ID to add the spans to.</p> <code>None</code> <code>attempt_id</code> <code>Optional[str]</code> <p>Optional attempt ID to add the spans to.</p> <code>None</code> <p>Yields:</p> Type Description <code>LightningSpanProcessor</code> <p>The LightningSpanProcessor instance to collect spans.</p> Source code in <code>agentlightning/tracer/agentops.py</code> <pre><code>@contextmanager\ndef trace_context(\n    self,\n    name: Optional[str] = None,\n    *,\n    store: Optional[LightningStore] = None,\n    rollout_id: Optional[str] = None,\n    attempt_id: Optional[str] = None,\n) -&gt; Iterator[LightningSpanProcessor]:\n    \"\"\"\n    Starts a new tracing context. This should be used as a context manager.\n\n    Args:\n        name: Optional name for the tracing context.\n        store: Optional store to add the spans to.\n        rollout_id: Optional rollout ID to add the spans to.\n        attempt_id: Optional attempt ID to add the spans to.\n\n    Yields:\n        The LightningSpanProcessor instance to collect spans.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n    if store is not None and rollout_id is not None and attempt_id is not None:\n        ctx = self._lightning_span_processor.with_context(store=store, rollout_id=rollout_id, attempt_id=attempt_id)\n        with ctx as processor:\n            yield processor\n    elif store is None and rollout_id is None and attempt_id is None:\n        with self._lightning_span_processor:\n            yield self._lightning_span_processor\n    else:\n        raise ValueError(\"store, rollout_id, and attempt_id must be either all provided or all None\")\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer","title":"<code>BaseTracer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>An abstract base class for tracers.</p> <p>This class defines a standard interface for tracing code execution, capturing the resulting spans, and providing them for analysis. It is designed to be backend-agnostic, allowing for different implementations (e.g., for AgentOps, OpenTelemetry, Docker, etc.).</p> <p>The primary interaction pattern is through the <code>trace_context</code> context manager, which ensures that traces are properly started and captured, even in the case of exceptions.</p> <p>A typical workflow:</p> <pre><code>tracer = YourTracerImplementation()\n\ntry:\n    with tracer.trace_context(name=\"my_traced_task\"):\n        # ... code to be traced ...\n        run_my_agent_logic()\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Retrieve the trace data after the context block\nspans: list[ReadableSpan] = tracer.get_last_trace()\n\n# Process the trace data\nif trace_tree:\n    rl_triplets = TraceTripletAdapter().adapt(spans)\n    # ... do something with the triplets\n</code></pre> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>class BaseTracer(ParallelWorkerBase):\n    \"\"\"\n    An abstract base class for tracers.\n\n    This class defines a standard interface for tracing code execution,\n    capturing the resulting spans, and providing them for analysis. It is\n    designed to be backend-agnostic, allowing for different implementations\n    (e.g., for AgentOps, OpenTelemetry, Docker, etc.).\n\n    The primary interaction pattern is through the `trace_context`\n    context manager, which ensures that traces are properly started and captured,\n    even in the case of exceptions.\n\n    A typical workflow:\n\n    ```python\n    tracer = YourTracerImplementation()\n\n    try:\n        with tracer.trace_context(name=\"my_traced_task\"):\n            # ... code to be traced ...\n            run_my_agent_logic()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n    # Retrieve the trace data after the context block\n    spans: list[ReadableSpan] = tracer.get_last_trace()\n\n    # Process the trace data\n    if trace_tree:\n        rl_triplets = TraceTripletAdapter().adapt(spans)\n        # ... do something with the triplets\n    ```\n    \"\"\"\n\n    @contextmanager\n    def trace_context(\n        self,\n        name: Optional[str] = None,\n        *,\n        store: Optional[LightningStore] = None,\n        rollout_id: Optional[str] = None,\n        attempt_id: Optional[str] = None,\n    ) -&gt; Iterator[Any]:\n        \"\"\"\n        Starts a new tracing context. This should be used as a context manager.\n\n        The implementation should handle the setup and teardown of the tracing\n        for the enclosed code block. It must ensure that any spans generated\n        within the `with` block are collected and made available via\n        `get_last_trace`.\n\n        If a store is provided, the spans will be added to the store when tracing.\n\n        Args:\n            name: The name for the root span of this trace context.\n            store: The store to add the spans to.\n            rollout_id: The rollout ID to add the spans to.\n            attempt_id: The attempt ID to add the spans to.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_last_trace(self) -&gt; List[ReadableSpan]:\n        \"\"\"\n        Retrieves the raw list of captured spans from the most recent trace.\n\n        Returns:\n            A list of OpenTelemetry `ReadableSpan` objects.\n        \"\"\"\n        raise NotImplementedError()\n\n    def trace_run(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        A convenience wrapper to trace the execution of a single synchronous function.\n\n        Args:\n            func: The synchronous function to execute and trace.\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            The return value of the function.\n        \"\"\"\n        with self.trace_context(name=func.__name__):\n            return func(*args, **kwargs)\n\n    async def trace_run_async(self, func: Callable[..., Awaitable[Any]], *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        A convenience wrapper to trace the execution of a single asynchronous function.\n\n        Args:\n            func: The asynchronous function to execute and trace.\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            The return value of the function.\n        \"\"\"\n        with self.trace_context(name=func.__name__):\n            return await func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> Type Description <code>List[ReadableSpan]</code> <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>def get_last_trace(self) -&gt; List[ReadableSpan]:\n    \"\"\"\n    Retrieves the raw list of captured spans from the most recent trace.\n\n    Returns:\n        A list of OpenTelemetry `ReadableSpan` objects.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>The implementation should handle the setup and teardown of the tracing for the enclosed code block. It must ensure that any spans generated within the <code>with</code> block are collected and made available via <code>get_last_trace</code>.</p> <p>If a store is provided, the spans will be added to the store when tracing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name for the root span of this trace context.</p> <code>None</code> <code>store</code> <code>Optional[LightningStore]</code> <p>The store to add the spans to.</p> <code>None</code> <code>rollout_id</code> <code>Optional[str]</code> <p>The rollout ID to add the spans to.</p> <code>None</code> <code>attempt_id</code> <code>Optional[str]</code> <p>The attempt ID to add the spans to.</p> <code>None</code> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>@contextmanager\ndef trace_context(\n    self,\n    name: Optional[str] = None,\n    *,\n    store: Optional[LightningStore] = None,\n    rollout_id: Optional[str] = None,\n    attempt_id: Optional[str] = None,\n) -&gt; Iterator[Any]:\n    \"\"\"\n    Starts a new tracing context. This should be used as a context manager.\n\n    The implementation should handle the setup and teardown of the tracing\n    for the enclosed code block. It must ensure that any spans generated\n    within the `with` block are collected and made available via\n    `get_last_trace`.\n\n    If a store is provided, the spans will be added to the store when tracing.\n\n    Args:\n        name: The name for the root span of this trace context.\n        store: The store to add the spans to.\n        rollout_id: The rollout ID to add the spans to.\n        attempt_id: The attempt ID to add the spans to.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_run","title":"<code>trace_run(func, *args, **kwargs)</code>","text":"<p>A convenience wrapper to trace the execution of a single synchronous function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The synchronous function to execute and trace.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the function.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>def trace_run(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    A convenience wrapper to trace the execution of a single synchronous function.\n\n    Args:\n        func: The synchronous function to execute and trace.\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The return value of the function.\n    \"\"\"\n    with self.trace_context(name=func.__name__):\n        return func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.BaseTracer.trace_run_async","title":"<code>trace_run_async(func, *args, **kwargs)</code>  <code>async</code>","text":"<p>A convenience wrapper to trace the execution of a single asynchronous function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Awaitable[Any]]</code> <p>The asynchronous function to execute and trace.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the function.</p> Source code in <code>agentlightning/tracer/base.py</code> <pre><code>async def trace_run_async(self, func: Callable[..., Awaitable[Any]], *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    A convenience wrapper to trace the execution of a single asynchronous function.\n\n    Args:\n        func: The asynchronous function to execute and trace.\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The return value of the function.\n    \"\"\"\n    with self.trace_context(name=func.__name__):\n        return await func(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.OtelTracer","title":"<code>OtelTracer</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Tracer that provides a basic OpenTelemetry tracer provider.</p> <p>You should be able to collect signals like rewards with this tracer, but no other function instrumentations like <code>openai.chat.completion</code>.</p> Source code in <code>agentlightning/tracer/otel.py</code> <pre><code>class OtelTracer(BaseTracer):\n    \"\"\"Tracer that provides a basic OpenTelemetry tracer provider.\n\n    You should be able to collect signals like rewards with this tracer,\n    but no other function instrumentations like `openai.chat.completion`.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # This provider is only initialized when the worker is initialized.\n        self._tracer_provider: Optional[TracerProvider] = None\n        self._lightning_span_processor: Optional[LightningSpanProcessor] = None\n        self._initialized: bool = False\n\n    def init_worker(self, worker_id: int):\n        super().init_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Setting up OpenTelemetry tracer...\")\n\n        if self._initialized:\n            logger.error(\"Tracer provider is already initialized. OpenTelemetry may not work as expected.\")\n\n        tracer_provider = TracerProvider()\n        trace_api.set_tracer_provider(tracer_provider)\n        self._lightning_span_processor = LightningSpanProcessor()\n        tracer_provider.add_span_processor(self._lightning_span_processor)\n        self._initialized = True\n\n    def teardown_worker(self, worker_id: int):\n        super().teardown_worker(worker_id)\n        logger.info(f\"[Worker {worker_id}] Tearing down OpenTelemetry tracer...\")\n        self._tracer_provider = None\n\n    @contextmanager\n    def trace_context(\n        self,\n        name: Optional[str] = None,\n        *,\n        store: Optional[LightningStore] = None,\n        rollout_id: Optional[str] = None,\n        attempt_id: Optional[str] = None,\n    ) -&gt; Iterator[LightningSpanProcessor]:\n        \"\"\"\n        Starts a new tracing context. This should be used as a context manager.\n\n        Args:\n            name: Optional name for the tracing context.\n            store: Optional store to add the spans to.\n            rollout_id: Optional rollout ID to add the spans to.\n            attempt_id: Optional attempt ID to add the spans to.\n\n        Yields:\n            The LightningSpanProcessor instance to collect spans.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n        if store is not None and rollout_id is not None and attempt_id is not None:\n            ctx = self._lightning_span_processor.with_context(store=store, rollout_id=rollout_id, attempt_id=attempt_id)\n            with ctx as processor:\n                yield processor\n        elif store is None and rollout_id is None and attempt_id is None:\n            with self._lightning_span_processor:\n                yield self._lightning_span_processor\n        else:\n            raise ValueError(\"store, rollout_id, and attempt_id must be either all provided or all None\")\n\n    def get_last_trace(self) -&gt; List[ReadableSpan]:\n        \"\"\"\n        Retrieves the raw list of captured spans from the most recent trace.\n\n        Returns:\n            A list of OpenTelemetry `ReadableSpan` objects.\n        \"\"\"\n        if not self._lightning_span_processor:\n            raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n        return self._lightning_span_processor.spans()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.OtelTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> Type Description <code>List[ReadableSpan]</code> <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> Source code in <code>agentlightning/tracer/otel.py</code> <pre><code>def get_last_trace(self) -&gt; List[ReadableSpan]:\n    \"\"\"\n    Retrieves the raw list of captured spans from the most recent trace.\n\n    Returns:\n        A list of OpenTelemetry `ReadableSpan` objects.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n    return self._lightning_span_processor.spans()\n</code></pre>"},{"location":"reference/core/#agentlightning.tracer.OtelTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the tracing context.</p> <code>None</code> <code>store</code> <code>Optional[LightningStore]</code> <p>Optional store to add the spans to.</p> <code>None</code> <code>rollout_id</code> <code>Optional[str]</code> <p>Optional rollout ID to add the spans to.</p> <code>None</code> <code>attempt_id</code> <code>Optional[str]</code> <p>Optional attempt ID to add the spans to.</p> <code>None</code> <p>Yields:</p> Type Description <code>LightningSpanProcessor</code> <p>The LightningSpanProcessor instance to collect spans.</p> Source code in <code>agentlightning/tracer/otel.py</code> <pre><code>@contextmanager\ndef trace_context(\n    self,\n    name: Optional[str] = None,\n    *,\n    store: Optional[LightningStore] = None,\n    rollout_id: Optional[str] = None,\n    attempt_id: Optional[str] = None,\n) -&gt; Iterator[LightningSpanProcessor]:\n    \"\"\"\n    Starts a new tracing context. This should be used as a context manager.\n\n    Args:\n        name: Optional name for the tracing context.\n        store: Optional store to add the spans to.\n        rollout_id: Optional rollout ID to add the spans to.\n        attempt_id: Optional attempt ID to add the spans to.\n\n    Yields:\n        The LightningSpanProcessor instance to collect spans.\n    \"\"\"\n    if not self._lightning_span_processor:\n        raise RuntimeError(\"LightningSpanProcessor is not initialized. Call init_worker() first.\")\n\n    if store is not None and rollout_id is not None and attempt_id is not None:\n        ctx = self._lightning_span_processor.with_context(store=store, rollout_id=rollout_id, attempt_id=attempt_id)\n        with ctx as processor:\n            yield processor\n    elif store is None and rollout_id is None and attempt_id is None:\n        with self._lightning_span_processor:\n            yield self._lightning_span_processor\n    else:\n        raise ValueError(\"store, rollout_id, and attempt_id must be either all provided or all None\")\n</code></pre>"},{"location":"reference/core/#agentlightning.reward","title":"<code>agentlightning.reward</code>","text":""},{"location":"reference/core/#agentlightning.reward.emit_reward","title":"<code>emit_reward(reward)</code>","text":"<p>Record a new reward as a new span.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def emit_reward(reward: float) -&gt; ReadableSpan:\n    \"\"\"\n    Record a new reward as a new span.\n    \"\"\"\n    logger.debug(f\"Emitting reward: {reward}\")\n    if isinstance(reward, (int, bool)):\n        reward = float(reward)\n    if not isinstance(reward, float):\n        raise ValueError(f\"Reward must be a number, got: {type(reward)}\")\n\n    # Check for tracer initialization\n    if hasattr(trace_api, \"_TRACER_PROVIDER\") and trace_api._TRACER_PROVIDER is None:  # type: ignore\n        raise RuntimeError(\"Tracer is not initialized. Cannot emit a meaningful span.\")\n\n    tracer_provider = get_tracer_provider()\n\n    tracer = tracer_provider.get_tracer(\"agentlightning\")\n    span = tracer.start_span(SpanNames.REWARD.value, attributes={\"reward\": reward})\n    # Do nothing; it's just a number\n    with span:\n        pass\n    if not isinstance(span, ReadableSpan):\n        raise ValueError(f\"Span is not a ReadableSpan: {span}\")\n    return span\n</code></pre>"},{"location":"reference/core/#agentlightning.reward.find_reward_spans","title":"<code>find_reward_spans(spans)</code>","text":"<p>Find all reward spans in the given list of spans.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>Sequence[SpanLike]</code> <p>A list of spans (either ReadableSpan or Span).</p> required <p>Returns:</p> Type Description <code>List[SpanLike]</code> <p>A list of spans whose name matches the reward span name.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def find_reward_spans(spans: Sequence[SpanLike]) -&gt; List[SpanLike]:\n    \"\"\"\n    Find all reward spans in the given list of spans.\n\n    Args:\n        spans: A list of spans (either ReadableSpan or Span).\n\n    Returns:\n        A list of spans whose name matches the reward span name.\n    \"\"\"\n    return [span for span in spans if is_reward_span(span)]\n</code></pre>"},{"location":"reference/core/#agentlightning.reward.get_last_reward","title":"<code>get_last_reward(spans)</code>","text":"<p>Get the last reward value from a list of spans.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>Sequence[SpanLike]</code> <p>A list of spans (either ReadableSpan or Span).</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The reward value from the last reward span, or None if not found.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def get_last_reward(spans: Sequence[SpanLike]) -&gt; Optional[float]:\n    \"\"\"\n    Get the last reward value from a list of spans.\n\n    Args:\n        spans: A list of spans (either ReadableSpan or Span).\n\n    Returns:\n        The reward value from the last reward span, or None if not found.\n    \"\"\"\n    for span in reversed(spans):\n        reward = get_reward_value(span)\n        if reward is not None:\n            return reward\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.reward.get_reward_value","title":"<code>get_reward_value(span)</code>","text":"<p>Get the reward value from a span.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def get_reward_value(span: SpanLike) -&gt; Optional[float]:\n    \"\"\"\n    Get the reward value from a span.\n    \"\"\"\n    for key in [\n        \"agentops.task.output\",  # newer versions of agentops\n        \"agentops.entity.output\",\n    ]:\n        reward_dict: Dict[str, Any] | None = None\n        if span.attributes:\n            output = span.attributes.get(key)\n            if output:\n                if isinstance(output, dict):\n                    reward_dict = cast(Dict[str, Any], output)\n                elif isinstance(output, str):\n                    try:\n                        reward_dict = cast(Dict[str, Any], json.loads(output))\n                    except json.JSONDecodeError:\n                        reward_dict = None\n\n        if reward_dict and reward_dict.get(\"type\") == \"reward\":\n            reward_value = reward_dict.get(\"value\", None)\n            if reward_value is None:\n                return None\n            if not isinstance(reward_value, float):\n                logger.error(f\"Reward is not a number, got: {type(reward_value)}. This may cause undefined behaviors.\")\n            return cast(float, reward_value)\n\n    # Latest emit reward format\n    if span.name == SpanNames.REWARD.value and span.attributes:\n        reward_value = span.attributes.get(\"reward\", None)\n        if reward_value is None:\n            return None\n        if not isinstance(reward_value, float):\n            logger.error(f\"Reward is not a number, got: {type(reward_value)}. This may cause undefined behaviors.\")\n        return cast(float, reward_value)\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.reward.is_reward_span","title":"<code>is_reward_span(span)</code>","text":"<p>Check if a span is a reward span.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def is_reward_span(span: SpanLike) -&gt; bool:\n    \"\"\"\n    Check if a span is a reward span.\n    \"\"\"\n    maybe_reward = get_reward_value(span)\n    return maybe_reward is not None\n</code></pre>"},{"location":"reference/core/#agentlightning.reward.reward","title":"<code>reward(fn)</code>","text":"<p>A decorator to wrap a function that computes rewards. It will automatically handle the input and output of the function.</p> Source code in <code>agentlightning/reward.py</code> <pre><code>def reward(fn: FnType) -&gt; FnType:\n    \"\"\"\n    A decorator to wrap a function that computes rewards.\n    It will automatically handle the input and output of the function.\n    \"\"\"\n\n    def wrap_result(result: Optional[float]) -&gt; RewardSpanData:\n        \"\"\"\n        Wrap the result of the function in a dict.\n        \"\"\"\n        if result is None:\n            return {\"type\": \"reward\", \"value\": None}\n        if not isinstance(result, (float, int)):  # type: ignore\n            warnings.warn(f\"Reward is ignored because it is not a number: {result}\")\n            return {\"type\": \"reward\", \"value\": None}\n        return {\"type\": \"reward\", \"value\": float(result)}\n\n    # Check if the function is async\n    is_async = asyncio.iscoroutinefunction(fn) or inspect.iscoroutinefunction(fn)\n\n    if is_async:\n\n        async def wrapper_async(*args: Any, **kwargs: Any) -&gt; Any:\n            if not _agentops_initialized():\n                # Track the reward without AgentOps\n                result = await fn(*args, **kwargs)\n                emit_reward(cast(float, result))\n                return result\n\n            result: Optional[float] = None\n\n            @operation\n            async def agentops_reward_operation() -&gt; RewardSpanData:\n                # The reward function we are interested in tracing\n                # It takes zero inputs and return a formatted dict\n                nonlocal result\n                result = await fn(*args, **kwargs)\n                return wrap_result(result)\n\n            await agentops_reward_operation()\n            return result\n\n        return wrapper_async  # type: ignore\n\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            if not _agentops_initialized():\n                # Track the reward without AgentOps\n                result = fn(*args, **kwargs)\n                emit_reward(cast(float, result))\n                return result\n\n            result: Optional[float] = None\n\n            @operation\n            def agentops_reward_operation() -&gt; RewardSpanData:\n                nonlocal result\n                result = fn(*args, **kwargs)\n                return wrap_result(result)\n\n            agentops_reward_operation()\n            return result\n\n        return wrapper  # type: ignore\n</code></pre>"},{"location":"reference/core/#server-side","title":"Server Side","text":""},{"location":"reference/core/#agentlightning.server","title":"<code>agentlightning.server</code>","text":""},{"location":"reference/core/#agentlightning.server.AgentLightningServer","title":"<code>AgentLightningServer</code>","text":"<p>The main SDK class for developers to control the Agent Lightning Server.</p> <p>This class manages the server lifecycle, task queueing, resources updates, and retrieval of results, providing a simple interface for the optimization logic.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class AgentLightningServer:\n    \"\"\"\n    The main SDK class for developers to control the Agent Lightning Server.\n\n    This class manages the server lifecycle, task queueing, resources updates,\n    and retrieval of results, providing a simple interface for the optimization logic.\n    \"\"\"\n\n    def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n        \"\"\"\n        Initializes the server controller.\n\n        Args:\n            host: The host to bind the server to.\n            port: The port to bind the server to.\n            task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.endpoint = f\"http://{host}:{port}\"\n        self._task_timeout_seconds = task_timeout_seconds\n\n        # Defer initialization and use event for cross-thread communication\n        self._store: Optional[ServerDataStore] = None\n        self.loop: Optional[asyncio.AbstractEventLoop] = None\n        self.startup_event = threading.Event()\n\n        # Create FastAPI app instance with a lifespan manager\n        self._app = FastAPI(lifespan=self._lifespan)\n        self._setup_routes()\n\n        self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n        self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n\n    # --- ADDED: Lifespan context manager ---\n    @asynccontextmanager\n    async def _lifespan(self, app: FastAPI):\n        \"\"\"\n        Manages server startup and shutdown. This runs inside the server's event loop.\n        \"\"\"\n        logger.info(\"Server is starting up...\")\n        self.loop = asyncio.get_running_loop()\n        self._store = ServerDataStore()  # Initialize data store here\n        self.startup_event.set()  # Signal that the server is ready\n\n        yield\n\n        logger.info(\"Server is shutting down.\")\n        self._store = None\n        self.startup_event.clear()  # Clear the startup event\n        self.loop = None\n\n    async def _check_and_requeue_stale_tasks(self):\n        \"\"\"\n        Check for stale tasks and requeue them. Called reactively during get_next_task.\n        \"\"\"\n        current_time = time.time()\n        # Ensure store is initialized before checking\n        if not self._store:\n            return\n        processing_tasks = self._store.get_processing_tasks()\n\n        for _, task in processing_tasks.items():\n            if task.last_claim_time and current_time - task.last_claim_time &gt; self._task_timeout_seconds:\n                await self._store.requeue_task(task)\n                logger.warning(\n                    f\"Task {task.rollout_id} timed out after {self._task_timeout_seconds}s, requeued (attempt {task.num_claims})\"\n                )\n\n    def _setup_routes(self):\n        \"\"\"Setup FastAPI routes.\"\"\"\n\n        @self._app.get(\"/task\", response_model=TaskIfAny)\n        async def next_task() -&gt; TaskIfAny:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the next available task.\"\"\"\n            await self._check_and_requeue_stale_tasks()\n\n            if not self._store:\n                return TaskIfAny(is_available=False)\n\n            task = await self._store.get_next_task()\n            if task:\n                logger.debug(f\"Serving task {task.rollout_id} to a client.\")\n                return TaskIfAny(is_available=True, task=task)\n            else:\n                logger.debug(\"No task available for client.\")\n                return TaskIfAny(is_available=False)\n\n        @self._app.get(\"/resources/latest\", response_model=ResourcesUpdate)\n        async def fetch_latest_resources() -&gt; ResourcesUpdate:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the latest available resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_latest_resources()\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=\"No resources have been set on the server.\")\n            logger.debug(f\"Serving latest resources '{resources_update.resources_id}' to a client.\")\n            return resources_update\n\n        @self._app.get(\"/resources/{resource_id}\", response_model=ResourcesUpdate)\n        async def fetch_resources_by_id(  # type: ignore\n            resource_id: str = Path(..., description=\"The unique identifier for the resource version.\")\n        ) -&gt; ResourcesUpdate:\n            \"\"\"Endpoint for clients to fetch a specific version of resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_resources_by_id(resource_id)\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=f\"Resource ID '{resource_id}' not found.\")\n            logger.debug(f\"Serving resources for ID '{resource_id}' to a client.\")\n            return resources_update\n\n        @self._app.post(\"/rollout\", response_model=GenericResponse)\n        async def post_rollout(payload: Rollout) -&gt; GenericResponse:  # type: ignore\n            \"\"\"Endpoint for clients to report a completed rollout.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            await self._store.store_rollout(payload)\n            return GenericResponse(\n                status=\"ok\",\n                message=f\"Rollout {payload.rollout_id} received and stored.\",\n            )\n\n    async def start(self):\n        \"\"\"Starts the FastAPI server in the background.\"\"\"\n        logger.info(f\"Starting server at {self.endpoint}\")\n        asyncio.create_task(self._uvicorn_server.serve())\n        await asyncio.sleep(1)  # Allow time for server to start up.\n\n    async def stop(self):\n        \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n        if self._uvicorn_server.started:\n            logger.info(\"Stopping server...\")\n            self._uvicorn_server.should_exit = True\n            await asyncio.sleep(1)  # Allow time for graceful shutdown.\n            logger.info(\"Server stopped.\")\n\n    async def run_forever(self):\n        \"\"\"\n        Runs the server indefinitely until stopped.\n        This is useful when async start and stop methods do not work.\n        \"\"\"\n        await self._uvicorn_server.serve()\n\n    async def queue_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a task to the queue for a client to process.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n\n    async def update_resources(self, resources: NamedResources) -&gt; str:\n        \"\"\"\n        Updates the resources, creating a new version and setting it as the latest.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        resources_id = f\"res-{uuid.uuid4()}\"\n        update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n        await self._store.update_resources(update)\n        return resources_id\n\n    async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Retrieves a specific completed rollout by its ID.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_rollout(rollout_id)\n\n    async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n        \"\"\"\n        Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            rollout = await self.get_completed_rollout(rollout_id)\n            if rollout:\n                return rollout\n            if timeout and (time.time() - start_time) &gt;= timeout:\n                return None\n            await asyncio.sleep(1)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all available completed trajectories and clears the internal store.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initializes the server controller.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host to bind the server to.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>The port to bind the server to.</p> <code>8000</code> <code>task_timeout_seconds</code> <code>float</code> <p>Time in seconds after which a claimed task is considered stale and requeued.</p> <code>300.0</code> Source code in <code>agentlightning/server.py</code> <pre><code>def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n    \"\"\"\n    Initializes the server controller.\n\n    Args:\n        host: The host to bind the server to.\n        port: The port to bind the server to.\n        task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.endpoint = f\"http://{host}:{port}\"\n    self._task_timeout_seconds = task_timeout_seconds\n\n    # Defer initialization and use event for cross-thread communication\n    self._store: Optional[ServerDataStore] = None\n    self.loop: Optional[asyncio.AbstractEventLoop] = None\n    self.startup_event = threading.Event()\n\n    # Create FastAPI app instance with a lifespan manager\n    self._app = FastAPI(lifespan=self._lifespan)\n    self._setup_routes()\n\n    self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n    self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific completed rollout by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Retrieves a specific completed rollout by its ID.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_rollout(rollout_id)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Polls for a completed rollout by its ID, waiting up to <code>timeout</code> seconds.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n    \"\"\"\n    Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        rollout = await self.get_completed_rollout(rollout_id)\n        if rollout:\n            return rollout\n        if timeout and (time.time() - start_time) &gt;= timeout:\n            return None\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a task to the queue for a client to process.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def queue_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a task to the queue for a client to process.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all available completed trajectories and clears the internal store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all available completed trajectories and clears the internal store.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the server indefinitely until stopped. This is useful when async start and stop methods do not work.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def run_forever(self):\n    \"\"\"\n    Runs the server indefinitely until stopped.\n    This is useful when async start and stop methods do not work.\n    \"\"\"\n    await self._uvicorn_server.serve()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the FastAPI server in the background.\"\"\"\n    logger.info(f\"Starting server at {self.endpoint}\")\n    asyncio.create_task(self._uvicorn_server.serve())\n    await asyncio.sleep(1)  # Allow time for server to start up.\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def stop(self):\n    \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n    if self._uvicorn_server.started:\n        logger.info(\"Stopping server...\")\n        self._uvicorn_server.should_exit = True\n        await asyncio.sleep(1)  # Allow time for graceful shutdown.\n        logger.info(\"Server stopped.\")\n</code></pre>"},{"location":"reference/core/#agentlightning.server.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Updates the resources, creating a new version and setting it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, resources: NamedResources) -&gt; str:\n    \"\"\"\n    Updates the resources, creating a new version and setting it as the latest.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    resources_id = f\"res-{uuid.uuid4()}\"\n    update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n    await self._store.update_resources(update)\n    return resources_id\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore","title":"<code>ServerDataStore</code>","text":"<p>A centralized, thread-safe, async, in-memory data store for the server's state. This holds the task queue, versioned resources, and completed rollouts.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class ServerDataStore:\n    \"\"\"\n    A centralized, thread-safe, async, in-memory data store for the server's state.\n    This holds the task queue, versioned resources, and completed rollouts.\n    \"\"\"\n\n    def __init__(self):\n        self._task_queue: asyncio.Queue[Task] = asyncio.Queue()\n        self._processing_tasks: Dict[str, Task] = {}  # Currently processing tasks\n        self._completed_rollouts: Dict[str, Rollout] = {}\n\n        # Store for versioned resources\n        self._resource_versions: Dict[str, NamedResources] = {}\n        self._latest_resources_id: Optional[str] = None\n\n        # Locks for thread-safe access\n        self._results_lock = asyncio.Lock()\n        self._resources_lock = asyncio.Lock()\n\n    async def add_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a new task to the queue with specific metadata and returns its unique ID.\n        \"\"\"\n        rollout_id = f\"rollout-{uuid.uuid4()}\"\n        task = Task(\n            rollout_id=rollout_id,\n            input=sample,\n            mode=mode,\n            resources_id=resources_id,\n            create_time=time.time(),\n            num_claims=0,\n            metadata=metadata or {},\n        )\n        await self._task_queue.put(task)\n        logger.info(f\"Task queued: {rollout_id} (mode: {mode}, resources_id: {resources_id})\")\n        return rollout_id\n\n    async def get_next_task(self) -&gt; Optional[Task]:\n        \"\"\"\n        Retrieves the next task from the queue without blocking.\n        Returns None if the queue is empty.\n        \"\"\"\n        try:\n            async with self._results_lock:\n                task = self._task_queue.get_nowait()\n                task = task.model_copy(\n                    update={\n                        \"last_claim_time\": time.time(),\n                        \"num_claims\": (task.num_claims or 0) + 1,\n                    }\n                )\n                self._processing_tasks[task.rollout_id] = task\n                if task.num_claims == 1:\n                    logger.debug(f\"Next task retrieved: {task.rollout_id}\")\n                else:\n                    logger.info(f\"Task {task.rollout_id} re-claimed (attempt {task.num_claims})\")\n                return task\n        except asyncio.QueueEmpty:\n            return None\n\n    async def update_resources(self, update: ResourcesUpdate):\n        \"\"\"\n        Safely stores a new version of named resources and sets it as the latest.\n        \"\"\"\n        # TODO: evict old resources if necessary.\n        async with self._resources_lock:\n            self._resource_versions[update.resources_id] = update.resources\n            self._latest_resources_id = update.resources_id\n            logger.info(f\"Resources updated. New version '{update.resources_id}' is now latest.\")\n\n    async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves a specific version of named resources by its ID.\n        \"\"\"\n        async with self._resources_lock:\n            resources = self._resource_versions.get(resources_id)\n            if resources:\n                return ResourcesUpdate(resources_id=resources_id, resources=resources)\n            return None\n\n    async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves the latest version of named resources.\n        \"\"\"\n        if self._latest_resources_id:\n            return await self.get_resources_by_id(self._latest_resources_id)\n        return None\n\n    async def store_rollout(self, rollout: Rollout):\n        \"\"\"\n        Safely stores a completed rollout from a client.\n        \"\"\"\n        async with self._results_lock:\n            self._processing_tasks.pop(rollout.rollout_id, None)\n            self._completed_rollouts[rollout.rollout_id] = rollout\n            logger.info(f\"Rollout received and stored: {rollout.rollout_id}\")\n\n    async def retrieve_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Safely retrieves a single rollout by its ID, removing it from the store.\n        \"\"\"\n        async with self._results_lock:\n            return self._completed_rollouts.pop(rollout_id, None)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all completed rollouts and clears the store.\n        \"\"\"\n        async with self._results_lock:\n            rollouts = list(self._completed_rollouts.values())\n            self._completed_rollouts.clear()\n            return rollouts\n\n    def get_processing_tasks(self) -&gt; Dict[str, Task]:\n        \"\"\"Returns a copy of currently processing tasks for timeout checking.\"\"\"\n        return self._processing_tasks.copy()\n\n    async def requeue_task(self, task: Task):\n        \"\"\"Requeues a task that has timed out and removes it from processing.\"\"\"\n        logger.warning(f\"Requeuing task {task.rollout_id} after timeout (attempt {task.num_claims})\")\n        async with self._results_lock:\n            # Remove from processing tasks\n            self._processing_tasks.pop(task.rollout_id, None)\n            self._task_queue.put_nowait(task)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.add_task","title":"<code>add_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a new task to the queue with specific metadata and returns its unique ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def add_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a new task to the queue with specific metadata and returns its unique ID.\n    \"\"\"\n    rollout_id = f\"rollout-{uuid.uuid4()}\"\n    task = Task(\n        rollout_id=rollout_id,\n        input=sample,\n        mode=mode,\n        resources_id=resources_id,\n        create_time=time.time(),\n        num_claims=0,\n        metadata=metadata or {},\n    )\n    await self._task_queue.put(task)\n    logger.info(f\"Task queued: {rollout_id} (mode: {mode}, resources_id: {resources_id})\")\n    return rollout_id\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Safely retrieves the latest version of named resources.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves the latest version of named resources.\n    \"\"\"\n    if self._latest_resources_id:\n        return await self.get_resources_by_id(self._latest_resources_id)\n    return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_next_task","title":"<code>get_next_task()</code>  <code>async</code>","text":"<p>Retrieves the next task from the queue without blocking. Returns None if the queue is empty.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_next_task(self) -&gt; Optional[Task]:\n    \"\"\"\n    Retrieves the next task from the queue without blocking.\n    Returns None if the queue is empty.\n    \"\"\"\n    try:\n        async with self._results_lock:\n            task = self._task_queue.get_nowait()\n            task = task.model_copy(\n                update={\n                    \"last_claim_time\": time.time(),\n                    \"num_claims\": (task.num_claims or 0) + 1,\n                }\n            )\n            self._processing_tasks[task.rollout_id] = task\n            if task.num_claims == 1:\n                logger.debug(f\"Next task retrieved: {task.rollout_id}\")\n            else:\n                logger.info(f\"Task {task.rollout_id} re-claimed (attempt {task.num_claims})\")\n            return task\n    except asyncio.QueueEmpty:\n        return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_processing_tasks","title":"<code>get_processing_tasks()</code>","text":"<p>Returns a copy of currently processing tasks for timeout checking.</p> Source code in <code>agentlightning/server.py</code> <pre><code>def get_processing_tasks(self) -&gt; Dict[str, Task]:\n    \"\"\"Returns a copy of currently processing tasks for timeout checking.\"\"\"\n    return self._processing_tasks.copy()\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Safely retrieves a specific version of named resources by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves a specific version of named resources by its ID.\n    \"\"\"\n    async with self._resources_lock:\n        resources = self._resource_versions.get(resources_id)\n        if resources:\n            return ResourcesUpdate(resources_id=resources_id, resources=resources)\n        return None\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.requeue_task","title":"<code>requeue_task(task)</code>  <code>async</code>","text":"<p>Requeues a task that has timed out and removes it from processing.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def requeue_task(self, task: Task):\n    \"\"\"Requeues a task that has timed out and removes it from processing.\"\"\"\n    logger.warning(f\"Requeuing task {task.rollout_id} after timeout (attempt {task.num_claims})\")\n    async with self._results_lock:\n        # Remove from processing tasks\n        self._processing_tasks.pop(task.rollout_id, None)\n        self._task_queue.put_nowait(task)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all completed rollouts and clears the store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all completed rollouts and clears the store.\n    \"\"\"\n    async with self._results_lock:\n        rollouts = list(self._completed_rollouts.values())\n        self._completed_rollouts.clear()\n        return rollouts\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.retrieve_rollout","title":"<code>retrieve_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Safely retrieves a single rollout by its ID, removing it from the store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Safely retrieves a single rollout by its ID, removing it from the store.\n    \"\"\"\n    async with self._results_lock:\n        return self._completed_rollouts.pop(rollout_id, None)\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.store_rollout","title":"<code>store_rollout(rollout)</code>  <code>async</code>","text":"<p>Safely stores a completed rollout from a client.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def store_rollout(self, rollout: Rollout):\n    \"\"\"\n    Safely stores a completed rollout from a client.\n    \"\"\"\n    async with self._results_lock:\n        self._processing_tasks.pop(rollout.rollout_id, None)\n        self._completed_rollouts[rollout.rollout_id] = rollout\n        logger.info(f\"Rollout received and stored: {rollout.rollout_id}\")\n</code></pre>"},{"location":"reference/core/#agentlightning.server.ServerDataStore.update_resources","title":"<code>update_resources(update)</code>  <code>async</code>","text":"<p>Safely stores a new version of named resources and sets it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, update: ResourcesUpdate):\n    \"\"\"\n    Safely stores a new version of named resources and sets it as the latest.\n    \"\"\"\n    # TODO: evict old resources if necessary.\n    async with self._resources_lock:\n        self._resource_versions[update.resources_id] = update.resources\n        self._latest_resources_id = update.resources_id\n        logger.info(f\"Resources updated. New version '{update.resources_id}' is now latest.\")\n</code></pre>"},{"location":"reference/core/#utilities","title":"Utilities","text":""},{"location":"reference/core/#agentlightning.config","title":"<code>agentlightning.config</code>","text":"<p>This file is not carefully reviewed. It might contain unintentional bugs and issues. Please always review the parsed construction arguments before using them.</p>"},{"location":"reference/core/#agentlightning.config.lightning_cli","title":"<code>lightning_cli(*classes)</code>","text":"<pre><code>lightning_cli(cls1: Type[_C1]) -&gt; _C1\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2]\n) -&gt; Tuple[_C1, _C2]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3]\n) -&gt; Tuple[_C1, _C2, _C3]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1],\n    cls2: Type[_C2],\n    cls3: Type[_C3],\n    cls4: Type[_C4],\n) -&gt; Tuple[_C1, _C2, _C3, _C4]\n</code></pre><pre><code>lightning_cli(\n    *classes: Type[CliConfigurable],\n) -&gt; Tuple[CliConfigurable, ...]\n</code></pre> <p>Parses command-line arguments to configure and instantiate provided CliConfigurable classes.</p> <p>Parameters:</p> Name Type Description Default <code>*classes</code> <code>Type[CliConfigurable]</code> <p>One or more classes that inherit from CliConfigurable. Each class's       init parameters will be exposed as command-line arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>CliConfigurable | Tuple[CliConfigurable, ...]</code> <p>A tuple of instantiated objects, corresponding to the input classes in order.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def lightning_cli(*classes: Type[CliConfigurable]) -&gt; CliConfigurable | Tuple[CliConfigurable, ...]:  # type: ignore\n    \"\"\"\n    Parses command-line arguments to configure and instantiate provided CliConfigurable classes.\n\n    Args:\n        *classes: One or more classes that inherit from CliConfigurable. Each class's\n                  __init__ parameters will be exposed as command-line arguments.\n\n    Returns:\n        A tuple of instantiated objects, corresponding to the input classes in order.\n    \"\"\"\n    if not classes:\n        return tuple()  # Return an empty tuple if no classes are provided\n\n    parser = _create_argument_parser()\n\n    # This map will store {cls: {init_param_name: argparse_dest_name}}\n    class_arg_configs_maps: Dict[Type[CliConfigurable], Dict[str, str]] = {}\n\n    for cls in classes:\n        _add_arguments_for_class(parser, cls, class_arg_configs_maps)\n\n    parsed_args = parser.parse_args()  # Uses sys.argv[1:] by default\n\n    # Correctly handle single class case for return type matching overloads\n    instances = _instantiate_classes(parsed_args, classes, class_arg_configs_maps)\n    if len(classes) == 1:\n        return instances[0]\n    return instances\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_float","title":"<code>nullable_float(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the float.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_float(value: str) -&gt; float | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the float.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    try:\n        return float(value)\n    except ValueError:\n        raise argparse.ArgumentTypeError(f\"Invalid float value: '{value}'\")\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_int","title":"<code>nullable_int(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the integer.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_int(value: str) -&gt; int | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the integer.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    try:\n        return int(value)\n    except ValueError:\n        raise argparse.ArgumentTypeError(f\"Invalid integer value: '{value}'\")\n</code></pre>"},{"location":"reference/core/#agentlightning.config.nullable_str","title":"<code>nullable_str(value)</code>","text":"<p>Converts specific string values (case-insensitive) to None, otherwise returns the string.</p> Source code in <code>agentlightning/config.py</code> <pre><code>def nullable_str(value: str) -&gt; str | None:\n    \"\"\"Converts specific string values (case-insensitive) to None, otherwise returns the string.\"\"\"\n    if value.lower() in [\"none\", \"null\", \"~\", \"nil\"]:  # Define keywords for None\n        return None\n    return value\n</code></pre>"},{"location":"reference/core/#agentlightning.types","title":"<code>agentlightning.types</code>","text":""},{"location":"reference/core/#agentlightning.types.NamedResources","title":"<code>NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>A dictionary-like class to hold named resources.</p> Example <p>resources: NamedResources = {     'main_llm': LLM(         endpoint=\"http://localhost:8080\",         model=\"llama3\",         sampling_parameters={'temperature': 0.7, 'max_tokens': 100}     ),     'system_prompt': PromptTemplate(         template=\"You are a helpful assistant.\",         engine='f-string'     ) }</p>"},{"location":"reference/core/#agentlightning.types.Attempt","title":"<code>Attempt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>An attempt to execute a rollout. A rollout can have multiple attempts if retries are needed.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Attempt(BaseModel):\n    \"\"\"An attempt to execute a rollout. A rollout can have multiple attempts if retries are needed.\"\"\"\n\n    rollout_id: str  # the rollout this attempt belongs to\n    attempt_id: str  # the universal id for current attempt\n    sequence_id: int  # the sequence number of the attempt, starting from 1\n    start_time: float  # time when the attempt has started\n    end_time: Optional[float] = None  # time when the attempt has ended\n\n    status: AttemptStatus = \"preparing\"\n    # The rollout worker which is executing this attempt\n    worker_id: Optional[str] = None\n\n    last_heartbeat_time: Optional[float] = None  # last time when the worker has reported progress\n\n    # A bucket for any other relevant information\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.types.AttemptedRollout","title":"<code>AttemptedRollout</code>","text":"<p>               Bases: <code>RolloutV2</code></p> <p>A rollout along with its active attempt.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class AttemptedRollout(RolloutV2):\n    \"\"\"A rollout along with its active attempt.\"\"\"\n\n    attempt: Attempt\n\n    @model_validator(mode=\"after\")\n    def check_consistency(self) -&gt; AttemptedRollout:\n        if self.attempt.rollout_id != self.rollout_id:\n            raise ValueError(\"Inconsistent rollout_id between Rollout and Attempt\")\n        return self\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset. You don't have to inherit from this class; you can use a simple list if you want to.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Dataset(Protocol, Generic[T_co]):\n    \"\"\"The general interface for a dataset.\n\n    It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset.\n    You don't have to inherit from this class; you can use a simple list if you want to.\n    \"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co: ...\n\n    def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Event","title":"<code>Event</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corresponding to opentelemetry.trace.Event</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"Corresponding to opentelemetry.trace.Event\"\"\"\n\n    name: str\n    attributes: Attributes\n    timestamp: Optional[float] = None\n\n    class Config:\n        allow_extra = True\n\n    @classmethod\n    def from_opentelemetry(cls, src: OtelEvent) -&gt; \"Event\":\n        return cls(\n            name=src.name,\n            attributes=dict(src.attributes) if src.attributes else {},\n            timestamp=convert_timestamp(src.timestamp),\n            **extract_extra_fields(src, [\"name\", \"attributes\", \"timestamp\"]),\n        )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.GenericResponse","title":"<code>GenericResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A generic response message that can be used for various purposes.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class GenericResponse(BaseModel):\n    \"\"\"\n    A generic response message that can be used for various purposes.\n    \"\"\"\n\n    status: str = \"success\"\n    message: Optional[str] = None\n    data: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Hook","title":"<code>Hook</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Base class for defining hooks in the agent runner's lifecycle.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Hook(ParallelWorkerBase):\n    \"\"\"Base class for defining hooks in the agent runner's lifecycle.\"\"\"\n\n    async def on_trace_start(\n        self, *, agent: LitAgent[Any], runner: BaseRunner[Any], tracer: BaseTracer, rollout: RolloutV2\n    ) -&gt; None:\n        \"\"\"Hook called immediately after the tracer enters the trace context but before the rollout begins.\n\n        Args:\n            agent: The :class:`LitAgent` instance associated with the runner.\n            runner: The :class:`BaseRunner` managing the rollout.\n            tracer: The :class:`BaseTracer` instance associated with the runner.\n            rollout: The :class:`RolloutV2` object that will be processed.\n\n        Subclasses can override this method to implement custom logic such as logging,\n        metric collection, or resource setup. By default, this is a no-op.\n        \"\"\"\n\n    async def on_trace_end(\n        self, *, agent: LitAgent[Any], runner: BaseRunner[Any], tracer: BaseTracer, rollout: RolloutV2\n    ) -&gt; None:\n        \"\"\"Hook called immediately after the rollout completes but before the tracer exits the trace context.\n\n        Args:\n            agent: The :class:`LitAgent` instance associated with the runner.\n            runner: The :class:`BaseRunner` managing the rollout.\n            tracer: The :class:`BaseTracer` instance associated with the runner.\n            rollout: The :class:`RolloutV2` object that has been processed.\n\n        Subclasses can override this method to implement custom logic such as logging,\n        metric collection, or resource cleanup. By default, this is a no-op.\n        \"\"\"\n\n    async def on_rollout_start(self, *, agent: LitAgent[Any], runner: BaseRunner[Any], rollout: RolloutV2) -&gt; None:\n        \"\"\"Hook called immediately before a rollout *attempt* begins.\n\n        Args:\n            agent: The :class:`LitAgent` instance associated with the runner.\n            runner: The :class:`BaseRunner` managing the rollout.\n            rollout: The :class:`RolloutV2` object that will be processed.\n\n        Subclasses can override this method to implement custom logic such as\n        logging, metric collection, or resource setup. By default, this is a\n        no-op.\n        \"\"\"\n\n    async def on_rollout_end(\n        self,\n        *,\n        agent: LitAgent[Any],\n        runner: BaseRunner[Any],\n        rollout: RolloutV2,\n        spans: Union[List[ReadableSpan], List[Span]],\n    ) -&gt; None:\n        \"\"\"Hook called after a rollout *attempt* completes.\n\n        Args:\n            agent: The :class:`LitAgent` instance associated with the runner.\n            runner: The :class:`BaseRunner` managing the rollout.\n            rollout: The :class:`RolloutV2` object that has been processed.\n            spans: The spans that have been added to the store.\n\n        Subclasses can override this method for cleanup or additional\n        logging. By default, this is a no-op.\n        \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Hook.on_rollout_end","title":"<code>on_rollout_end(*, agent, runner, rollout, spans)</code>  <code>async</code>","text":"<p>Hook called after a rollout attempt completes.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[Any]</code> <p>The :class:<code>LitAgent</code> instance associated with the runner.</p> required <code>runner</code> <code>BaseRunner[Any]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The :class:<code>RolloutV2</code> object that has been processed.</p> required <code>spans</code> <code>Union[List[ReadableSpan], List[Span]]</code> <p>The spans that have been added to the store.</p> required <p>Subclasses can override this method for cleanup or additional logging. By default, this is a no-op.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>async def on_rollout_end(\n    self,\n    *,\n    agent: LitAgent[Any],\n    runner: BaseRunner[Any],\n    rollout: RolloutV2,\n    spans: Union[List[ReadableSpan], List[Span]],\n) -&gt; None:\n    \"\"\"Hook called after a rollout *attempt* completes.\n\n    Args:\n        agent: The :class:`LitAgent` instance associated with the runner.\n        runner: The :class:`BaseRunner` managing the rollout.\n        rollout: The :class:`RolloutV2` object that has been processed.\n        spans: The spans that have been added to the store.\n\n    Subclasses can override this method for cleanup or additional\n    logging. By default, this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Hook.on_rollout_start","title":"<code>on_rollout_start(*, agent, runner, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately before a rollout attempt begins.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[Any]</code> <p>The :class:<code>LitAgent</code> instance associated with the runner.</p> required <code>runner</code> <code>BaseRunner[Any]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The :class:<code>RolloutV2</code> object that will be processed.</p> required <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>async def on_rollout_start(self, *, agent: LitAgent[Any], runner: BaseRunner[Any], rollout: RolloutV2) -&gt; None:\n    \"\"\"Hook called immediately before a rollout *attempt* begins.\n\n    Args:\n        agent: The :class:`LitAgent` instance associated with the runner.\n        runner: The :class:`BaseRunner` managing the rollout.\n        rollout: The :class:`RolloutV2` object that will be processed.\n\n    Subclasses can override this method to implement custom logic such as\n    logging, metric collection, or resource setup. By default, this is a\n    no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Hook.on_trace_end","title":"<code>on_trace_end(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the rollout completes but before the tracer exits the trace context.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[Any]</code> <p>The :class:<code>LitAgent</code> instance associated with the runner.</p> required <code>runner</code> <code>BaseRunner[Any]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The :class:<code>BaseTracer</code> instance associated with the runner.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The :class:<code>RolloutV2</code> object that has been processed.</p> required <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource cleanup. By default, this is a no-op.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>async def on_trace_end(\n    self, *, agent: LitAgent[Any], runner: BaseRunner[Any], tracer: BaseTracer, rollout: RolloutV2\n) -&gt; None:\n    \"\"\"Hook called immediately after the rollout completes but before the tracer exits the trace context.\n\n    Args:\n        agent: The :class:`LitAgent` instance associated with the runner.\n        runner: The :class:`BaseRunner` managing the rollout.\n        tracer: The :class:`BaseTracer` instance associated with the runner.\n        rollout: The :class:`RolloutV2` object that has been processed.\n\n    Subclasses can override this method to implement custom logic such as logging,\n    metric collection, or resource cleanup. By default, this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Hook.on_trace_start","title":"<code>on_trace_start(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the tracer enters the trace context but before the rollout begins.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>LitAgent[Any]</code> <p>The :class:<code>LitAgent</code> instance associated with the runner.</p> required <code>runner</code> <code>BaseRunner[Any]</code> <p>The :class:<code>BaseRunner</code> managing the rollout.</p> required <code>tracer</code> <code>BaseTracer</code> <p>The :class:<code>BaseTracer</code> instance associated with the runner.</p> required <code>rollout</code> <code>RolloutV2</code> <p>The :class:<code>RolloutV2</code> object that will be processed.</p> required <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>async def on_trace_start(\n    self, *, agent: LitAgent[Any], runner: BaseRunner[Any], tracer: BaseTracer, rollout: RolloutV2\n) -&gt; None:\n    \"\"\"Hook called immediately after the tracer enters the trace context but before the rollout begins.\n\n    Args:\n        agent: The :class:`LitAgent` instance associated with the runner.\n        runner: The :class:`BaseRunner` managing the rollout.\n        tracer: The :class:`BaseTracer` instance associated with the runner.\n        rollout: The :class:`RolloutV2` object that will be processed.\n\n    Subclasses can override this method to implement custom logic such as logging,\n    metric collection, or resource setup. By default, this is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Provide an LLM endpoint and model name as a resource.</p> <p>Attributes:</p> Name Type Description <code>endpoint</code> <code>str</code> <p>The URL of the LLM API endpoint.</p> <code>model</code> <code>str</code> <p>The identifier for the model to be used (e.g., 'gpt-4o').</p> <code>sampling_parameters</code> <code>SamplingParameters</code> <p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class LLM(Resource):\n    \"\"\"\n    Provide an LLM endpoint and model name as a resource.\n\n    Attributes:\n        endpoint (str): The URL of the LLM API endpoint.\n        model (str): The identifier for the model to be used (e.g., 'gpt-4o').\n        sampling_parameters (SamplingParameters): A dictionary of hyperparameters\n            for model inference, such as temperature, top_p, etc.\n    \"\"\"\n\n    resource_type: Literal[\"llm\"] = \"llm\"\n    endpoint: str\n    model: str\n    api_key: Optional[str] = None\n    sampling_parameters: Dict[str, Any] = Field(default_factory=dict)\n\n    def base_url(self, *args: Any, **kwargs: Any) -&gt; str:\n        \"\"\"The base_url to put into openai.OpenAI.\n\n        Users are encouraged to use `base_url` to get the LLM endpoint instead of accessing `endpoint` directly.\n        \"\"\"\n        return self.endpoint\n</code></pre>"},{"location":"reference/core/#agentlightning.types.LLM.base_url","title":"<code>base_url(*args, **kwargs)</code>","text":"<p>The base_url to put into openai.OpenAI.</p> <p>Users are encouraged to use <code>base_url</code> to get the LLM endpoint instead of accessing <code>endpoint</code> directly.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def base_url(self, *args: Any, **kwargs: Any) -&gt; str:\n    \"\"\"The base_url to put into openai.OpenAI.\n\n    Users are encouraged to use `base_url` to get the LLM endpoint instead of accessing `endpoint` directly.\n    \"\"\"\n    return self.endpoint\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Link","title":"<code>Link</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corresponding to opentelemetry.trace.Link</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class Link(BaseModel):\n    \"\"\"Corresponding to opentelemetry.trace.Link\"\"\"\n\n    context: SpanContext\n    attributes: Optional[Attributes] = None\n\n    class Config:\n        allow_extra = True\n\n    @classmethod\n    def from_opentelemetry(cls, src: trace_api.Link) -&gt; \"Link\":\n        return cls(\n            context=SpanContext.from_opentelemetry(src.context),\n            attributes=dict(src.attributes) if src.attributes else None,\n            **extract_extra_fields(src, [\"context\", \"attributes\"]),\n        )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ParallelWorkerBase","title":"<code>ParallelWorkerBase</code>","text":"<p>Base class for objects that can be parallelized across multiple worker processes.</p> <p>This class defines the standard lifecycle for parallel processing:</p> Main Process <ol> <li>init() - Initialize the object in the main process</li> <li>spawn workers and call init_worker() in each worker</li> <li>run() - Execute the main workload in parallel across workers</li> <li>teardown_worker() - Clean up resources in each worker</li> <li>teardown() - Final cleanup in the main process</li> </ol> <p>Subclasses should implement the run() method and optionally override the lifecycle methods for custom initialization and cleanup behavior.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class ParallelWorkerBase:\n    \"\"\"Base class for objects that can be parallelized across multiple worker processes.\n\n    This class defines the standard lifecycle for parallel processing:\n\n    Main Process:\n        1. init() - Initialize the object in the main process\n        2. spawn workers and call init_worker() in each worker\n        3. run() - Execute the main workload in parallel across workers\n        4. teardown_worker() - Clean up resources in each worker\n        5. teardown() - Final cleanup in the main process\n\n    Subclasses should implement the run() method and optionally override\n    the lifecycle methods for custom initialization and cleanup behavior.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the base class. This method can be overridden by subclasses.\"\"\"\n        self.worker_id: Optional[int] = None\n\n    def init(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def init_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        self.worker_id = worker_id\n\n    def run(self, *args: Any, **kwargs: Any) -&gt; Any:\n        pass\n\n    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def teardown(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ParallelWorkerBase.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the base class. This method can be overridden by subclasses.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the base class. This method can be overridden by subclasses.\"\"\"\n    self.worker_id: Optional[int] = None\n</code></pre>"},{"location":"reference/core/#agentlightning.types.PromptTemplate","title":"<code>PromptTemplate</code>","text":"<p>               Bases: <code>Resource</code></p> <p>A prompt template as a resource.</p> <p>Attributes:</p> Name Type Description <code>template</code> <code>str</code> <p>The template string. The format depends on the engine.</p> <code>engine</code> <code>Literal['jinja', 'f-string', 'poml']</code> <p>The templating engine to use for rendering the prompt. I imagine users can use their own customized engines, but algos can only well operate on a subset of them.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class PromptTemplate(Resource):\n    \"\"\"\n    A prompt template as a resource.\n\n    Attributes:\n        template (str): The template string. The format depends on the engine.\n        engine (Literal['jinja', 'f-string', 'poml']): The templating engine\n            to use for rendering the prompt. I imagine users can use their own\n            customized engines, but algos can only well operate on a subset of them.\n    \"\"\"\n\n    resource_type: Literal[\"prompt_template\"] = \"prompt_template\"\n    template: str\n    engine: Literal[\"jinja\", \"f-string\", \"poml\"]\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ProxyLLM","title":"<code>ProxyLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Proxy LLM resource that is tailored by <code>llm_proxy.LLMProxy</code>.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class ProxyLLM(LLM):\n    \"\"\"Proxy LLM resource that is tailored by `llm_proxy.LLMProxy`.\"\"\"\n\n    resource_type: Literal[\"proxy_llm\"] = \"proxy_llm\"  # type: ignore\n    _initialized: bool = False\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Mark initialization as complete after Pydantic finishes setup.\"\"\"\n        super().model_post_init(__context)\n        object.__setattr__(self, \"_initialized\", True)\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        \"\"\"Override to emit a warning when endpoint is accessed directly.\"\"\"\n        # Check if we're accessing endpoint after initialization and not from base_url\n        if name == \"endpoint\":\n            try:\n                initialized = object.__getattribute__(self, \"_initialized\")\n            except AttributeError:\n                initialized = False\n\n            if initialized:\n                # Check the call stack to see if we're being called from base_url\n                frame = inspect.currentframe()\n                if frame and frame.f_back:\n                    caller_name = frame.f_back.f_code.co_name\n                    if caller_name != \"base_url\":\n                        logger.warning(\n                            \"Accessing 'endpoint' directly on ProxyLLM is discouraged. \"\n                            \"Use 'base_url(rollout_id, attempt_id)' instead to get the properly formatted endpoint.\"\n                        )\n        return super().__getattribute__(name)\n\n    def with_attempted_rollout(self, rollout: AttemptedRollout) -&gt; LLM:\n        \"\"\"Bake the rollout and attempt id into the endpoint.\"\"\"\n        return LLM(\n            endpoint=self.base_url(rollout.rollout_id, rollout.attempt.attempt_id),\n            model=self.model,\n            sampling_parameters=self.sampling_parameters,\n            api_key=self.api_key,\n        )\n\n    def base_url(self, rollout_id: Optional[str], attempt_id: Optional[str]) -&gt; str:\n        if rollout_id is None and attempt_id is None:\n            return self.endpoint\n\n        if not (isinstance(rollout_id, str) and isinstance(attempt_id, str)):\n            raise ValueError(\"rollout_id and attempt_id must be strings or all be empty\")\n\n        prefix = self.endpoint\n        if prefix.endswith(\"/\"):\n            prefix = prefix[:-1]\n        if prefix.endswith(\"/v1\"):\n            prefix = prefix[:-3]\n            has_v1 = True\n        else:\n            has_v1 = False\n        # Now the prefix should look like \"http://localhost:11434\"\n\n        # Append the rollout and attempt id to the prefix\n        prefix = prefix + f\"/rollout/{rollout_id}/attempt/{attempt_id}\"\n        if has_v1:\n            prefix = prefix + \"/v1\"\n        return prefix\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ProxyLLM.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Override to emit a warning when endpoint is accessed directly.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def __getattribute__(self, name: str) -&gt; Any:\n    \"\"\"Override to emit a warning when endpoint is accessed directly.\"\"\"\n    # Check if we're accessing endpoint after initialization and not from base_url\n    if name == \"endpoint\":\n        try:\n            initialized = object.__getattribute__(self, \"_initialized\")\n        except AttributeError:\n            initialized = False\n\n        if initialized:\n            # Check the call stack to see if we're being called from base_url\n            frame = inspect.currentframe()\n            if frame and frame.f_back:\n                caller_name = frame.f_back.f_code.co_name\n                if caller_name != \"base_url\":\n                    logger.warning(\n                        \"Accessing 'endpoint' directly on ProxyLLM is discouraged. \"\n                        \"Use 'base_url(rollout_id, attempt_id)' instead to get the properly formatted endpoint.\"\n                    )\n    return super().__getattribute__(name)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ProxyLLM.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Mark initialization as complete after Pydantic finishes setup.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Mark initialization as complete after Pydantic finishes setup.\"\"\"\n    super().model_post_init(__context)\n    object.__setattr__(self, \"_initialized\", True)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ProxyLLM.with_attempted_rollout","title":"<code>with_attempted_rollout(rollout)</code>","text":"<p>Bake the rollout and attempt id into the endpoint.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def with_attempted_rollout(self, rollout: AttemptedRollout) -&gt; LLM:\n    \"\"\"Bake the rollout and attempt id into the endpoint.\"\"\"\n    return LLM(\n        endpoint=self.base_url(rollout.rollout_id, rollout.attempt.attempt_id),\n        model=self.model,\n        sampling_parameters=self.sampling_parameters,\n        api_key=self.api_key,\n    )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corresponding to opentelemetry.sdk.resources.Resource</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class Resource(BaseModel):\n    \"\"\"Corresponding to opentelemetry.sdk.resources.Resource\"\"\"\n\n    attributes: Attributes\n    schema_url: str\n\n    @classmethod\n    def from_opentelemetry(cls, src: OtelResource) -&gt; \"Resource\":\n        return cls(\n            attributes=dict(src.attributes) if src.attributes else {},\n            schema_url=src.schema_url if src.schema_url else \"\",\n            **extract_extra_fields(src, [\"attributes\", \"schema_url\"]),\n        )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.ResourcesUpdate","title":"<code>ResourcesUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A resource update message to be sent from the server to clients.</p> <p>This message contains a dictionary of resources that clients should use for subsequent tasks. It is used to update the resources available to clients dynamically.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class ResourcesUpdate(BaseModel):\n    \"\"\"\n    A resource update message to be sent from the server to clients.\n\n    This message contains a dictionary of resources that clients should use\n    for subsequent tasks. It is used to update the resources available to\n    clients dynamically.\n    \"\"\"\n\n    resources_id: str\n    resources: NamedResources\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Rollout","title":"<code>Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The standard reporting object from client to server.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Rollout(BaseModel):\n    \"\"\"The standard reporting object from client to server.\"\"\"\n\n    rollout_id: str\n\n    # Echoing the input task\n    task: Optional[Task] = None\n\n    # Primary, high-level feedback\n    final_reward: Optional[float] = None\n\n    # Structured, sequential feedback for RL-style optimization\n    triplets: Optional[List[Triplet]] = None\n\n    # Optional, rich-context data for deep analysis\n    trace: Optional[List[Dict[str, Any]]] = Field(\n        default=None,\n        description=\"A list of spans that conform to the OpenTelemetry JSON format. \"\n        \"Users of the opentelemetry-sdk can generate this by calling \"\n        \"json.loads(readable_span.to_json()).\",\n    )\n    logs: Optional[List[str]] = None\n\n    # A bucket for any other relevant information\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.RolloutConfig","title":"<code>RolloutConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configurations for rollout execution.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class RolloutConfig(BaseModel):\n    \"\"\"Configurations for rollout execution.\"\"\"\n\n    timeout_seconds: Optional[float] = None  # none indicates no timeout\n    unresponsive_seconds: Optional[float] = None  # none indicates no unresponsive timeout\n    max_attempts: int = Field(default=1, ge=1)  # including the first attempt\n    retry_condition: List[AttemptStatus] = Field(\n        default_factory=cast(Callable[[], List[AttemptStatus]], list)\n    )  # list of statuses that should trigger a retry\n</code></pre>"},{"location":"reference/core/#agentlightning.types.SpanContext","title":"<code>SpanContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corresponding to opentelemetry.trace.SpanContext</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class SpanContext(BaseModel):\n    \"\"\"Corresponding to opentelemetry.trace.SpanContext\"\"\"\n\n    trace_id: str\n    span_id: str\n    is_remote: bool\n    trace_state: TraceState\n\n    class Config:\n        allow_extra = True\n\n    @classmethod\n    def from_opentelemetry(cls, src: trace_api.SpanContext) -&gt; \"SpanContext\":\n        return cls(\n            trace_id=trace_api.format_trace_id(src.trace_id),\n            span_id=trace_api.format_span_id(src.span_id),\n            is_remote=src.is_remote,\n            trace_state={k: v for k, v in src.trace_state.items()} if src.trace_state else {},\n            **extract_extra_fields(src, [\"trace_id\", \"span_id\", \"is_remote\", \"trace_state\"]),\n        )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.SpanNames","title":"<code>SpanNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standard span name values for AgentLightning.</p> <p>Currently only reward spans are supported. We will add more spans related to error handling in the future.</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class SpanNames(str, Enum):\n    \"\"\"Standard span name values for AgentLightning.\n\n    Currently only reward spans are supported.\n    We will add more spans related to error handling in the future.\n    \"\"\"\n\n    REWARD = \"agentlightning.reward\"\n    VIRTUAL = \"agentlightning.virtual\"\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task (rollout request) to be processed by the client agent.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"A task (rollout request) to be processed by the client agent.\"\"\"\n\n    rollout_id: str\n    input: TaskInput\n\n    mode: Optional[RolloutMode] = None\n    resources_id: Optional[str] = None\n\n    # Optional fields for tracking task lifecycle\n    create_time: Optional[float] = None\n    last_claim_time: Optional[float] = None\n    num_claims: Optional[int] = None\n\n    # Allow additional metadata fields\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.types.TraceStatus","title":"<code>TraceStatus</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corresponding to opentelemetry.trace.Status</p> Source code in <code>agentlightning/types/tracer.py</code> <pre><code>class TraceStatus(BaseModel):\n    \"\"\"Corresponding to opentelemetry.trace.Status\"\"\"\n\n    status_code: str\n    description: Optional[str] = None\n\n    class Config:\n        allow_extra = True\n\n    @classmethod\n    def from_opentelemetry(cls, src: OtelStatus) -&gt; \"TraceStatus\":\n        return cls(\n            status_code=src.status_code.name,\n            description=src.description,\n            **extract_extra_fields(src, [\"status_code\", \"description\"]),\n        )\n</code></pre>"},{"location":"reference/core/#agentlightning.types.Triplet","title":"<code>Triplet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A standard structure for a single turn in a trajectory.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Triplet(BaseModel):\n    \"\"\"A standard structure for a single turn in a trajectory.\"\"\"\n\n    prompt: Any\n    response: Any\n    reward: Optional[float] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#agentlightning.logging","title":"<code>agentlightning.logging</code>","text":""},{"location":"reference/core/#agentlightning.instrumentation","title":"<code>agentlightning.instrumentation</code>","text":""},{"location":"reference/rl/","title":"Reinforcement Learning API","text":""},{"location":"reference/rl/#agentlightning.verl","title":"<code>agentlightning.verl</code>","text":""},{"location":"reference/rl/#agentlightning.verl.NamedResources","title":"<code>NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>A dictionary-like class to hold named resources.</p> Example <p>resources: NamedResources = {     'main_llm': LLM(         endpoint=\"http://localhost:8080\",         model=\"llama3\",         sampling_parameters={'temperature': 0.7, 'max_tokens': 100}     ),     'system_prompt': PromptTemplate(         template=\"You are a helpful assistant.\",         engine='f-string'     ) }</p>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer","title":"<code>AgentLightningServer</code>","text":"<p>The main SDK class for developers to control the Agent Lightning Server.</p> <p>This class manages the server lifecycle, task queueing, resources updates, and retrieval of results, providing a simple interface for the optimization logic.</p> Source code in <code>agentlightning/server.py</code> <pre><code>class AgentLightningServer:\n    \"\"\"\n    The main SDK class for developers to control the Agent Lightning Server.\n\n    This class manages the server lifecycle, task queueing, resources updates,\n    and retrieval of results, providing a simple interface for the optimization logic.\n    \"\"\"\n\n    def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n        \"\"\"\n        Initializes the server controller.\n\n        Args:\n            host: The host to bind the server to.\n            port: The port to bind the server to.\n            task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.endpoint = f\"http://{host}:{port}\"\n        self._task_timeout_seconds = task_timeout_seconds\n\n        # Defer initialization and use event for cross-thread communication\n        self._store: Optional[ServerDataStore] = None\n        self.loop: Optional[asyncio.AbstractEventLoop] = None\n        self.startup_event = threading.Event()\n\n        # Create FastAPI app instance with a lifespan manager\n        self._app = FastAPI(lifespan=self._lifespan)\n        self._setup_routes()\n\n        self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n        self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n\n    # --- ADDED: Lifespan context manager ---\n    @asynccontextmanager\n    async def _lifespan(self, app: FastAPI):\n        \"\"\"\n        Manages server startup and shutdown. This runs inside the server's event loop.\n        \"\"\"\n        logger.info(\"Server is starting up...\")\n        self.loop = asyncio.get_running_loop()\n        self._store = ServerDataStore()  # Initialize data store here\n        self.startup_event.set()  # Signal that the server is ready\n\n        yield\n\n        logger.info(\"Server is shutting down.\")\n        self._store = None\n        self.startup_event.clear()  # Clear the startup event\n        self.loop = None\n\n    async def _check_and_requeue_stale_tasks(self):\n        \"\"\"\n        Check for stale tasks and requeue them. Called reactively during get_next_task.\n        \"\"\"\n        current_time = time.time()\n        # Ensure store is initialized before checking\n        if not self._store:\n            return\n        processing_tasks = self._store.get_processing_tasks()\n\n        for _, task in processing_tasks.items():\n            if task.last_claim_time and current_time - task.last_claim_time &gt; self._task_timeout_seconds:\n                await self._store.requeue_task(task)\n                logger.warning(\n                    f\"Task {task.rollout_id} timed out after {self._task_timeout_seconds}s, requeued (attempt {task.num_claims})\"\n                )\n\n    def _setup_routes(self):\n        \"\"\"Setup FastAPI routes.\"\"\"\n\n        @self._app.get(\"/task\", response_model=TaskIfAny)\n        async def next_task() -&gt; TaskIfAny:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the next available task.\"\"\"\n            await self._check_and_requeue_stale_tasks()\n\n            if not self._store:\n                return TaskIfAny(is_available=False)\n\n            task = await self._store.get_next_task()\n            if task:\n                logger.debug(f\"Serving task {task.rollout_id} to a client.\")\n                return TaskIfAny(is_available=True, task=task)\n            else:\n                logger.debug(\"No task available for client.\")\n                return TaskIfAny(is_available=False)\n\n        @self._app.get(\"/resources/latest\", response_model=ResourcesUpdate)\n        async def fetch_latest_resources() -&gt; ResourcesUpdate:  # type: ignore\n            \"\"\"Endpoint for clients to poll for the latest available resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_latest_resources()\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=\"No resources have been set on the server.\")\n            logger.debug(f\"Serving latest resources '{resources_update.resources_id}' to a client.\")\n            return resources_update\n\n        @self._app.get(\"/resources/{resource_id}\", response_model=ResourcesUpdate)\n        async def fetch_resources_by_id(  # type: ignore\n            resource_id: str = Path(..., description=\"The unique identifier for the resource version.\")\n        ) -&gt; ResourcesUpdate:\n            \"\"\"Endpoint for clients to fetch a specific version of resources.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            resources_update = await self._store.get_resources_by_id(resource_id)\n            if not resources_update:\n                raise HTTPException(status_code=404, detail=f\"Resource ID '{resource_id}' not found.\")\n            logger.debug(f\"Serving resources for ID '{resource_id}' to a client.\")\n            return resources_update\n\n        @self._app.post(\"/rollout\", response_model=GenericResponse)\n        async def post_rollout(payload: Rollout) -&gt; GenericResponse:  # type: ignore\n            \"\"\"Endpoint for clients to report a completed rollout.\"\"\"\n            if not self._store:\n                raise HTTPException(status_code=503, detail=\"Server not fully initialized.\")\n            await self._store.store_rollout(payload)\n            return GenericResponse(\n                status=\"ok\",\n                message=f\"Rollout {payload.rollout_id} received and stored.\",\n            )\n\n    async def start(self):\n        \"\"\"Starts the FastAPI server in the background.\"\"\"\n        logger.info(f\"Starting server at {self.endpoint}\")\n        asyncio.create_task(self._uvicorn_server.serve())\n        await asyncio.sleep(1)  # Allow time for server to start up.\n\n    async def stop(self):\n        \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n        if self._uvicorn_server.started:\n            logger.info(\"Stopping server...\")\n            self._uvicorn_server.should_exit = True\n            await asyncio.sleep(1)  # Allow time for graceful shutdown.\n            logger.info(\"Server stopped.\")\n\n    async def run_forever(self):\n        \"\"\"\n        Runs the server indefinitely until stopped.\n        This is useful when async start and stop methods do not work.\n        \"\"\"\n        await self._uvicorn_server.serve()\n\n    async def queue_task(\n        self,\n        sample: Any,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Adds a task to the queue for a client to process.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n\n    async def update_resources(self, resources: NamedResources) -&gt; str:\n        \"\"\"\n        Updates the resources, creating a new version and setting it as the latest.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        resources_id = f\"res-{uuid.uuid4()}\"\n        update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n        await self._store.update_resources(update)\n        return resources_id\n\n    async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n        \"\"\"\n        Retrieves a specific completed rollout by its ID.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_rollout(rollout_id)\n\n    async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n        \"\"\"\n        Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            rollout = await self.get_completed_rollout(rollout_id)\n            if rollout:\n                return rollout\n            if timeout and (time.time() - start_time) &gt;= timeout:\n                return None\n            await asyncio.sleep(1)\n\n    async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n        \"\"\"\n        Retrieves all available completed trajectories and clears the internal store.\n        \"\"\"\n        if not self._store:\n            raise RuntimeError(\"Store not initialized. The server may not be running.\")\n        return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initializes the server controller.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host to bind the server to.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>The port to bind the server to.</p> <code>8000</code> <code>task_timeout_seconds</code> <code>float</code> <p>Time in seconds after which a claimed task is considered stale and requeued.</p> <code>300.0</code> Source code in <code>agentlightning/server.py</code> <pre><code>def __init__(self, host: str = \"127.0.0.1\", port: int = 8000, task_timeout_seconds: float = 300.0):\n    \"\"\"\n    Initializes the server controller.\n\n    Args:\n        host: The host to bind the server to.\n        port: The port to bind the server to.\n        task_timeout_seconds: Time in seconds after which a claimed task is considered stale and requeued.\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.endpoint = f\"http://{host}:{port}\"\n    self._task_timeout_seconds = task_timeout_seconds\n\n    # Defer initialization and use event for cross-thread communication\n    self._store: Optional[ServerDataStore] = None\n    self.loop: Optional[asyncio.AbstractEventLoop] = None\n    self.startup_event = threading.Event()\n\n    # Create FastAPI app instance with a lifespan manager\n    self._app = FastAPI(lifespan=self._lifespan)\n    self._setup_routes()\n\n    self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level=\"info\")\n    self._uvicorn_server = uvicorn.Server(self._uvicorn_config)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific completed rollout by its ID.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def get_completed_rollout(self, rollout_id: str) -&gt; Optional[Rollout]:\n    \"\"\"\n    Retrieves a specific completed rollout by its ID.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_rollout(rollout_id)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Polls for a completed rollout by its ID, waiting up to <code>timeout</code> seconds.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -&gt; Optional[Rollout]:\n    \"\"\"\n    Polls for a completed rollout by its ID, waiting up to `timeout` seconds.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        rollout = await self.get_completed_rollout(rollout_id)\n        if rollout:\n            return rollout\n        if timeout and (time.time() - start_time) &gt;= timeout:\n            return None\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a task to the queue for a client to process.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def queue_task(\n    self,\n    sample: Any,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"\n    Adds a task to the queue for a client to process.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Retrieves all available completed trajectories and clears the internal store.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def retrieve_completed_rollouts(self) -&gt; List[Rollout]:\n    \"\"\"\n    Retrieves all available completed trajectories and clears the internal store.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    return await self._store.retrieve_completed_rollouts()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the server indefinitely until stopped. This is useful when async start and stop methods do not work.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def run_forever(self):\n    \"\"\"\n    Runs the server indefinitely until stopped.\n    This is useful when async start and stop methods do not work.\n    \"\"\"\n    await self._uvicorn_server.serve()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def start(self):\n    \"\"\"Starts the FastAPI server in the background.\"\"\"\n    logger.info(f\"Starting server at {self.endpoint}\")\n    asyncio.create_task(self._uvicorn_server.serve())\n    await asyncio.sleep(1)  # Allow time for server to start up.\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def stop(self):\n    \"\"\"Gracefully stops the running FastAPI server.\"\"\"\n    if self._uvicorn_server.started:\n        logger.info(\"Stopping server...\")\n        self._uvicorn_server.should_exit = True\n        await asyncio.sleep(1)  # Allow time for graceful shutdown.\n        logger.info(\"Server stopped.\")\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Updates the resources, creating a new version and setting it as the latest.</p> Source code in <code>agentlightning/server.py</code> <pre><code>async def update_resources(self, resources: NamedResources) -&gt; str:\n    \"\"\"\n    Updates the resources, creating a new version and setting it as the latest.\n    \"\"\"\n    if not self._store:\n        raise RuntimeError(\"Store not initialized. The server may not be running.\")\n    resources_id = f\"res-{uuid.uuid4()}\"\n    update = ResourcesUpdate(resources_id=resources_id, resources=resources)\n    await self._store.update_resources(update)\n    return resources_id\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentLightningTrainer","title":"<code>AgentLightningTrainer</code>","text":"<p>               Bases: <code>RayPPOTrainer</code></p> <p>Specialized PPO trainer for agent-based reinforcement learning.</p> <p>This trainer is designed specifically for scenarios where the model interacts with external environments, tools, or APIs through an AgentLightningServer. It simplifies the training loop by removing the complex conditional logic present in the original RayPPOTrainer and focusing on the agent mode workflow.</p> <p>Key differences from RayPPOTrainer: 1. Uses AgentModeDaemon for server communication 2. Simplified data flow without pop/union operations 3. Direct batch processing through agent daemon 4. Streamlined validation using agent_mode validation</p> Source code in <code>agentlightning/verl/trainer.py</code> <pre><code>class AgentLightningTrainer(RayPPOTrainer):\n    \"\"\"\n    Specialized PPO trainer for agent-based reinforcement learning.\n\n    This trainer is designed specifically for scenarios where the model interacts with\n    external environments, tools, or APIs through an AgentLightningServer. It simplifies\n    the training loop by removing the complex conditional logic present in the original\n    RayPPOTrainer and focusing on the agent mode workflow.\n\n    Key differences from RayPPOTrainer:\n    1. Uses AgentModeDaemon for server communication\n    2. Simplified data flow without pop/union operations\n    3. Direct batch processing through agent daemon\n    4. Streamlined validation using agent_mode validation\n    \"\"\"\n\n    def __init__(\n        self, store: LightningStore | None, llm_proxy: LLMProxy | None, adapter: TraceAdapter | None, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.store = store\n        self.llm_proxy = llm_proxy\n        self.adapter = adapter\n\n    def _validate(self):\n        assert len(self.val_dataloader) == 1, \"Please set val_batch_size to None for better throughput.\"\n\n        test_data = next(iter(self.val_dataloader))\n        test_batch = DataProto.from_single_dict(test_data)\n\n        self.async_rollout_manager.wake_up()\n        self.agent_mode_daemon.set_up_data_and_server(\n            test_batch.non_tensor_batch,\n            self.async_rollout_manager.server_addresses,\n            is_train=False,\n        )\n        self.agent_mode_daemon.run_until_all_finished()\n        test_metrics = self.agent_mode_daemon.get_test_metrics()\n        self.agent_mode_daemon.clear_data_and_server()\n        self.async_rollout_manager.sleep()\n        return test_metrics\n\n    def _train_step(self, batch_dict: dict) -&gt; dict:\n        # Isolate in a separate method to automatically recycle the variables before validation.\n        batch: DataProto = DataProto.from_single_dict(batch_dict)\n        metrics = {}\n        timing_raw = {}\n\n        with _timer(\"step\", timing_raw):\n\n            # When agent mode is enabled, we read the batch as it is.\n            gen_batch = batch\n\n            # generate a batch\n            with _timer(\"gen\", timing_raw):\n                self.async_rollout_manager.wake_up()\n                self.agent_mode_daemon.set_up_data_and_server(\n                    gen_batch.non_tensor_batch, self.async_rollout_manager.server_addresses\n                )\n                self.agent_mode_daemon.run_until_all_finished()\n                batch, agent_metrics = self.agent_mode_daemon.get_train_data_batch(\n                    max_prompt_length=self.config.data.max_prompt_length,\n                    max_response_length=self.config.data.max_response_length,\n                    device=gen_batch.batch[\"fake_ids\"].device,\n                )\n                metrics.update(agent_metrics)\n                self.agent_mode_daemon.clear_data_and_server()\n                self.async_rollout_manager.sleep()\n\n            if self.config.algorithm.adv_estimator == AdvantageEstimator.REMAX:\n                with _timer(\"gen_max\", timing_raw):\n                    gen_baseline_batch = deepcopy(gen_batch)\n                    gen_baseline_batch.meta_info[\"do_sample\"] = False\n                    gen_baseline_output = self.async_rollout_manager.generate_sequences(gen_baseline_batch)\n\n                    batch = batch.union(gen_baseline_output)\n                    reward_baseline_tensor = self.reward_fn(batch)\n                    reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)\n\n                    batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))\n\n                    batch.batch[\"reward_baselines\"] = reward_baseline_tensor\n\n                    del gen_baseline_batch, gen_baseline_output\n\n            # uid is used for algorithm like GRPO, should be aligned to data id\n            batch.non_tensor_batch[\"uid\"] = batch.non_tensor_batch[\"data_id_list\"]\n\n            batch.batch[\"response_mask\"] = compute_response_mask(batch)\n\n            # compute global_valid tokens\n            batch.meta_info[\"global_token_num\"] = torch.sum(batch.batch[\"attention_mask\"], dim=-1).tolist()\n\n            with _timer(\"reward\", timing_raw):\n                # compute reward model score\n                if self.use_rm:\n                    reward_tensor = self.rm_wg.compute_rm_score(batch)\n                    batch = batch.union(reward_tensor)\n\n                reward_extra_infos_dict = {}\n\n            # for agent mode, pad the lengths to calculate old log prob, ref, and values\n            batch, pad_size = pad_dataproto_to_divisor(batch, self.actor_rollout_wg.world_size)\n\n            # recompute old_log_probs\n            with _timer(\"old_log_prob\", timing_raw):\n                old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n                entropys = old_log_prob.batch[\"entropys\"]\n                response_masks = batch.batch[\"response_mask\"]\n                loss_agg_mode = self.config.actor_rollout_ref.actor.loss_agg_mode\n                entropy_loss = agg_loss(loss_mat=entropys, loss_mask=response_masks, loss_agg_mode=loss_agg_mode)\n                old_log_prob_metrics = {\"actor/entropy_loss\": entropy_loss.detach().item()}\n                metrics.update(old_log_prob_metrics)\n                old_log_prob.batch.pop(\"entropys\")\n                batch = batch.union(old_log_prob)\n\n            if self.use_reference_policy:\n                # compute reference log_prob\n                with _timer(\"ref\", timing_raw):\n                    ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n                    batch = batch.union(ref_log_prob)\n\n            # compute values\n            if self.use_critic:\n                with _timer(\"values\", timing_raw):\n                    values = self.critic_wg.compute_values(batch)\n                    batch = batch.union(values)\n\n            # for agent mode, unpad to calculate adv\n            # it is important, as adv should be based on the raw traces\n            batch = unpad_dataproto(batch, pad_size=pad_size)\n\n            with _timer(\"adv\", timing_raw):\n                # if agent_mode is enabled, there is already token_level_scores\n                # token_level_scores is not needed to compute here\n\n                # compute rewards. apply_kl_penalty if available\n                if self.config.algorithm.use_kl_in_reward:\n                    batch, kl_metrics = apply_kl_penalty(\n                        batch, kl_ctrl=self.kl_ctrl_in_reward, kl_penalty=self.config.algorithm.kl_penalty\n                    )\n                    metrics.update(kl_metrics)\n                else:\n                    batch.batch[\"token_level_rewards\"] = batch.batch[\"token_level_scores\"]\n\n                # compute advantages, executed on the driver process\n\n                norm_adv_by_std_in_grpo = self.config.algorithm.get(\n                    \"norm_adv_by_std_in_grpo\", True\n                )  # GRPO adv normalization factor\n\n                batch = compute_advantage(\n                    batch,\n                    adv_estimator=self.config.algorithm.adv_estimator,\n                    gamma=self.config.algorithm.gamma,\n                    lam=self.config.algorithm.lam,\n                    num_repeat=self.config.actor_rollout_ref.rollout.n,\n                    norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n                    config=self.config.algorithm,\n                )\n\n            # after advantages are assinged, we begin to drop (1) long prompt (2) floor to ppo minisize\n            keep_indices = (~batch.batch[\"is_drop_mask\"]).nonzero(as_tuple=True)[0]\n            metrics[\"training/n_triplets_prompt_too_long\"] = (\n                batch.batch[\"is_drop_mask\"].shape[0] - keep_indices.shape[0]\n            )\n            batch = batch[keep_indices]\n            # next, round to minibatch size\n            mini_batch_size = self.config.actor_rollout_ref.actor.ppo_mini_batch_size\n            n_transition = len(batch)\n            random_indices = list(range(n_transition))\n            random.shuffle(random_indices)\n            batch.reorder(torch.tensor(random_indices).type(torch.int32))\n            n_remained_transition = n_transition // mini_batch_size * mini_batch_size\n            batch = batch[list(range(n_remained_transition))]\n            metrics[\"training/n_triplets_dropped_remainder\"] = n_transition - n_remained_transition\n\n            # Agent mode note: Change the order of balance batch;\n            #     1. first calculate advantage\n            #     2. then drop the samples (too long prompt &amp; floor to ppo minisize)\n            #     3. balance\n            # balance the number of valid tokens on each dp rank.\n            # Note that this breaks the order of data inside the batch.\n            # Please take care when you implement group based adv computation such as GRPO and rloo\n            if self.config.trainer.balance_batch:\n                self._balance_batch(batch, metrics=metrics)\n\n            # update critic\n            if self.use_critic:\n                with _timer(\"update_critic\", timing_raw):\n                    critic_output = self.critic_wg.update_critic(batch)\n                critic_output_metrics = reduce_metrics(critic_output.meta_info[\"metrics\"])\n                metrics.update(critic_output_metrics)\n\n            # implement critic warmup\n            if self.config.trainer.critic_warmup &lt;= self.global_steps:\n                # update actor\n                with _timer(\"update_actor\", timing_raw):\n                    batch.meta_info[\"multi_turn\"] = self.config.actor_rollout_ref.rollout.multi_turn.enable\n                    actor_output = self.actor_rollout_wg.update_actor(batch)\n                actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n                metrics.update(actor_output_metrics)\n\n            # Log rollout generations if enabled\n            rollout_data_dir = self.config.trainer.get(\"rollout_data_dir\", None)\n            if rollout_data_dir:\n                with _timer(\"dump_rollout_generations\", timing_raw):\n                    print(batch.batch.keys())\n                    inputs = self.tokenizer.batch_decode(batch.batch[\"prompts\"], skip_special_tokens=True)\n                    outputs = self.tokenizer.batch_decode(batch.batch[\"responses\"], skip_special_tokens=True)\n                    scores = batch.batch[\"token_level_scores\"].sum(-1).cpu().tolist()\n                    self._dump_generations(\n                        inputs=inputs,\n                        outputs=outputs,\n                        scores=scores,\n                        reward_extra_infos_dict=reward_extra_infos_dict,\n                        dump_path=rollout_data_dir,\n                    )\n\n        # compute training metrics\n        metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n        metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n        # TODO: implement actual tflpo and theoretical tflpo\n        n_gpus = self.resource_pool_manager.get_n_gpus()\n        metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, n_gpus=n_gpus))\n\n        return metrics\n\n    def fit(self):\n        logger = Tracking(\n            project_name=self.config.trainer.project_name,\n            experiment_name=self.config.trainer.experiment_name,\n            default_backend=self.config.trainer.logger,\n            config=OmegaConf.to_container(self.config, resolve=True),\n        )\n\n        self.global_steps = 0\n\n        # load checkpoint before doing anything\n        self._load_checkpoint()\n\n        assert self.async_rollout_mode, \"If agent mode is enabled, async server must be enabled\"\n        if self.adapter is not None and not isinstance(self.adapter, BaseTraceTripletAdapter):\n            raise ValueError(\"Adapter must be a BaseTraceTripletAdapter for currently VERL implementation.\")\n        self.agent_mode_daemon = AgentModeDaemon(\n            self.config.agentlightning.port,\n            self.config.actor_rollout_ref.rollout.n,\n            train_information={\n                # Note (Zhiyuan): To avoid further patch into vllm async server, using the same sentence to get the naming here.\n                # However, it is possible that verl updates the naming and causes incompatibility.\n                # Reference: https://github.com/volcengine/verl/blob/5b5e09d9cc20625e436d01f69d9cc739ff681c54/verl/workers/rollout/vllm_rollout/vllm_async_server.py#L217\n                \"model\": \"/\".join(self.config.actor_rollout_ref.model.path.split(\"/\")[-2:]),\n                \"temperature\": self.config.actor_rollout_ref.rollout.temperature,\n            },\n            tokenizer=self.tokenizer,\n            mini_batch_size=self.config.actor_rollout_ref.actor.ppo_mini_batch_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            mode=\"v1\" if self.store is not None else \"v0\",\n            store=self.store,\n            llm_proxy=self.llm_proxy,\n            adapter=self.adapter,\n        )\n        self.agent_mode_daemon.start()\n\n        # perform validation before training\n        # currently, we only support validation using the reward_function.\n        if self.val_reward_fn is not None and self.config.trainer.get(\"val_before_train\", True):\n            val_metrics = self._validate()\n            assert val_metrics, f\"{val_metrics=}\"\n            pprint(f\"Initial validation metrics: {val_metrics}\")\n            logger.log(data=val_metrics, step=self.global_steps)\n            if self.config.trainer.get(\"val_only\", False):\n                return\n\n        # add tqdm\n        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n\n        # we start from step 1\n        self.global_steps += 1\n        last_val_metrics = None\n\n        for epoch in range(self.config.trainer.total_epochs):\n            for batch_dict in self.train_dataloader:\n                metrics = {}\n                timing_raw = {}\n                is_last_step = self.global_steps &gt;= self.total_training_steps\n\n                # train step\n                metrics = self._train_step(batch_dict)\n\n                # validate\n                if (\n                    self.val_reward_fn is not None\n                    and self.config.trainer.test_freq &gt; 0\n                    and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n                ):\n                    with _timer(\"validate\", timing_raw):\n                        val_metrics: dict = self._validate()\n                        if is_last_step:\n                            last_val_metrics = val_metrics\n                    metrics.update(val_metrics)\n\n                if self.config.trainer.save_freq &gt; 0 and (\n                    is_last_step or self.global_steps % self.config.trainer.save_freq == 0\n                ):\n                    with _timer(\"save_checkpoint\", timing_raw):\n                        self._save_checkpoint()\n\n                # step metrics\n                metrics.update(\n                    {\n                        \"training/global_step\": self.global_steps,\n                        \"training/epoch\": epoch,\n                    }\n                )\n\n                # TODO: make a canonical logger that supports various backend\n                logger.log(data=metrics, step=self.global_steps)\n\n                if is_last_step:\n                    pprint(f\"Final validation metrics: {last_val_metrics}\")\n                    progress_bar.close()\n\n                    # This exit logic is to ensure a robust CI.\n                    pprint(f\"Flush the logger...\")\n                    del logger  # Make sure the loggers are flushed and closed properly\n                    pprint(f\"Training finished at step {self.global_steps}.\")\n                    return\n\n                progress_bar.update(1)\n                self.global_steps += 1\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon","title":"<code>AgentModeDaemon</code>","text":"<p>AgentModeDaemon using the AgentLightningServer SDK.</p> <p>This class manages the server lifecycle, task queueing, and results retrieval, while also running a proxy server for LLM requests. It maintains the original interface for compatibility with the RayPPOTrainer.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>class AgentModeDaemon:\n    \"\"\"\n    AgentModeDaemon using the AgentLightningServer SDK.\n\n    This class manages the server lifecycle, task queueing, and results\n    retrieval, while also running a proxy server for LLM requests. It maintains\n    the original interface for compatibility with the RayPPOTrainer.\n    \"\"\"\n\n    def __init__(\n        self,\n        port: Optional[int],\n        train_rollout_n: int,\n        train_information: Dict[str, Any],\n        tokenizer: Any,\n        mini_batch_size: int,\n        pad_token_id: int,\n        reward_fillna_value: float = 0.0,\n        llm_timeout_seconds: float = 1200.0,\n        mode: Literal[\"v0\", \"v1\"] = \"v1\",\n        llm_proxy: LLMProxy | None = None,\n        store: LightningStore | None = None,\n        adapter: BaseTraceTripletAdapter | None = None,\n    ):\n        self.mode = mode\n\n        # Server and Task Configuration\n        if mode == \"v0\":\n            assert port is not None\n            self.server_port = port\n            self.llm_timeout_seconds = llm_timeout_seconds\n            self.server = AgentLightningServer(\n                host=\"0.0.0.0\", port=self.server_port, task_timeout_seconds=self.llm_timeout_seconds\n            )\n            self.proxy_port = _find_available_port()  # Run proxy on a different port\n        else:\n            assert store is not None\n            self.store = store\n            if llm_proxy is None:\n                self.llm_proxy = LLMProxy(\n                    port=_find_available_port(),\n                    model_list=[],\n                    store=store,\n                )\n            else:\n                # Reuse the existing LLM proxy (probably configured by user)\n                self.llm_proxy = llm_proxy\n            if adapter is None:\n                self.adapter = TraceTripletAdapter()\n            else:\n                # Reuse the one from trainer\n                self.adapter = adapter\n            self._internal_loop: Optional[asyncio.AbstractEventLoop] = None\n            self._internal_loop_thread = threading.Thread(target=self._internal_loop_runner, daemon=True)\n            self._internal_loop_thread.start()\n\n        # Training and Data Configuration\n        self.train_rollout_n = train_rollout_n\n        self.train_information = train_information\n        self.mini_batch_size = mini_batch_size\n        self.pad_token_id = pad_token_id\n        self.tokenizer = tokenizer\n        self.reward_fillna_value = reward_fillna_value\n\n        # Internal State\n        self.backend_llm_server_addresses: List[str] = []\n        self._total_tasks_queued = 0\n        self._completed_rollouts_v0: Dict[str, Rollout] = {}\n        self._task_id_to_original_sample: Dict[str, Dict[str, Any]] = {}\n        self._server_thread: Optional[threading.Thread] = None\n        self._proxy_thread: Optional[threading.Thread] = None\n        self.is_train = True\n\n    def _internal_loop_runner(self):\n        \"\"\"Run the internal loop.\"\"\"\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        self._internal_loop = loop\n        loop.run_forever()\n        loop.close()\n\n    def _start_proxy_server_v0(self):\n        \"\"\"\n        Initializes and runs a Flask-based proxy server in a separate thread.\n        This proxy load-balances requests to the actual backend LLM servers.\n        \"\"\"\n        app = Flask(__name__)\n\n        num_requests = 0\n        last_request_time = 0\n\n        @app.route(\"/v1/&lt;path:path&gt;\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"OPTIONS\", \"HEAD\"])\n        def proxy(path: str):  # type: ignore\n            if not self.backend_llm_server_addresses:\n                abort(503, description=\"No backend LLM servers available.\")\n\n            # Randomly choose a backend server for load balancing\n            target_server = random.choice(self.backend_llm_server_addresses)\n            target_url = f\"http://{target_server}/v1/{path}\"\n\n            # Copy client request headers, removing the Host header\n            headers = {key: value for key, value in request.headers if key.lower() != \"host\"}\n\n            # Log the request for debugging\n            nonlocal num_requests, last_request_time\n            current_time = time.time()\n            num_requests += 1\n            if current_time - last_request_time &gt; 60 or num_requests == 1 or num_requests % 100 == 0:\n                print(f\"Proxying {request.method} request to {target_server}. Request data: {request.get_data()}\")\n            last_request_time = current_time\n\n            try:\n                # Forward the request to the target backend\n                resp = requests.request(\n                    method=request.method,\n                    url=target_url,\n                    headers=headers,\n                    params=request.args,  # type: ignore\n                    data=request.get_data(),\n                    cookies=request.cookies,\n                    allow_redirects=False,\n                    timeout=self.llm_timeout_seconds,\n                )\n                # Filter out hop-by-hop headers before returning the response\n                excluded_headers = [\n                    \"content-encoding\",\n                    \"content-length\",\n                    \"transfer-encoding\",\n                    \"connection\",\n                    \"keep-alive\",\n                    \"proxy-authenticate\",\n                    \"proxy-authorization\",\n                    \"te\",\n                    \"trailers\",\n                    \"upgrade\",\n                ]\n                response_headers = [\n                    (name, value) for name, value in resp.raw.headers.items() if name.lower() not in excluded_headers\n                ]\n                if resp.status_code == 200:\n                    # NOTE: from Zhiyuan's code.\n                    # https://github.com/hzy46/verl_agent_mode/blob/2db65ea9858f645a914120357412a7540f8bd82d/verl/trainer/ppo/ray_trainer.py#L692-L711\n                    # request_json = json.loads(request.get_data().decode(\"utf-8\"))\n                    response_json = json.loads(resp.content.decode(\"utf-8\"))\n                    # response_message = ChatCompletion(**response_json).choices[0].message.model_dump(exclude_unset=True, exclude_none=True)\n                    # tool_schemas = request_json.get(\"tools\", None)\n                    # prompt_ids = self.tokenizer.apply_chat_template(request_json[\"messages\"], tools=tool_schemas, add_generation_prompt=True, tokenize=True)\n                    # full_ids = self.tokenizer.apply_chat_template(request_json[\"messages\"] + [response_message], tools=tool_schemas, add_generation_prompt=False, tokenize=True)\n                    # TBD: response_ids sometimes ends with \"&lt;eos_id&gt;\\n\", shall we keep the extra \"\\n\"?\n                    # sometimes it has some differences with the hacky method in the end, but this should align with ToolCompletionCallback\n                    # response_ids = full_ids[len(prompt_ids):]\n\n                    # NOTE (yuge): They are different. Don't know why.\n                    # assert response_json['prompt_token_ids'] == prompt_ids\n                    # patched_response_ids = response_json['response_token_ids'][0]\n                    # assert patched_response_ids == response_ids[:len(patched_response_ids)], f\"{patched_response_ids} != {response_ids[:len(patched_response_ids)]}\"\n                    # response_json['prompt_token_ids'] = prompt_ids\n                    # response_json['response_token_ids'] = [response_ids]\n                    replaced_return_content = json.dumps(response_json).encode(\"utf-8\")\n                    return Response(replaced_return_content, status=resp.status_code, headers=response_headers)\n                return Response(resp.content, resp.status_code, response_headers)\n            except requests.exceptions.RequestException as e:\n                abort(500, description=f\"Error proxying request: {e}\")\n\n        def run_app():\n            app.run(host=\"0.0.0.0\", port=self.proxy_port, threaded=True, debug=False)\n\n        self._proxy_thread = threading.Thread(target=run_app, daemon=True)\n        self._proxy_thread.start()\n        print(f\"Proxy server running on port {self.proxy_port}\")\n\n    def _update_proxy_server_v1(self):\n        model_name = self.train_information.get(\"model\")\n        if not model_name:\n            raise ValueError(\"Model name is not set.\")\n        self.llm_proxy.update_model_list(\n            [\n                ModelConfig(\n                    {\n                        \"model_name\": model_name,\n                        \"litellm_params\": {\n                            \"model\": \"hosted_vllm/\" + model_name,\n                            \"api_base\": f\"http://{address}/v1/\",\n                        },\n                    }\n                )\n                for address in self.backend_llm_server_addresses\n            ],\n        )\n\n        if self.llm_proxy.is_running():\n            # FIXME: Need to switch to a different port right now\n            # because the forked processes carried the old fd\n            self.llm_proxy.restart(_port=_find_available_port())\n        else:\n            self.llm_proxy.start()\n\n    def start(self):\n        \"\"\"Starts the main AgentLightningServer and the proxy server.\"\"\"\n\n        if self.mode == \"v0\":\n\n            def run_server():\n                \"\"\"Run the AgentLightningServer in a separate thread.\"\"\"\n                asyncio.run(self.server.run_forever())\n\n            self._server_thread = threading.Thread(target=run_server, daemon=True)\n            self._server_thread.start()\n\n            # Wait for the server's internal startup event to be set.\n            print(\"Waiting for AgentLightningServer to start...\")\n            is_ready = self.server.startup_event.wait(timeout=20.0)  # Wait up to 20s\n            if not is_ready:\n                raise RuntimeError(\"AgentLightningServer failed to start within the timeout period.\")\n\n            print(f\"AgentLightningServer control plane running on port {self.server_port}\")\n\n            self._start_proxy_server_v0()\n        else:\n            # Agent lightning server is no longer needed;\n            # Start proxy server in _async_set_up\n            pass\n\n    async def _async_set_up(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n        \"\"\"Async helper to set up data and resources on the server.\"\"\"\n        self.clear_data_and_server()\n        if server_addresses != self.backend_llm_server_addresses:\n            self.backend_llm_server_addresses = server_addresses\n            if self.mode == \"v1\" and not self.llm_proxy.is_running():\n                self._update_proxy_server_v1()\n        self.is_train = is_train\n\n        # 1. Update resources on the server for clients to use\n        if self.mode == \"v0\":\n            llm_resource = LLM(\n                endpoint=f\"http://127.0.0.1:{self.proxy_port}/v1\",\n                model=self.train_information.get(\"model\", \"default-model\"),\n                sampling_parameters={\n                    \"temperature\": self.train_information.get(\"temperature\", 0.7 if is_train else 0.0)\n                },\n            )\n        else:\n            llm_resource = self.llm_proxy.as_resource(\n                sampling_parameters={\n                    \"temperature\": self.train_information.get(\"temperature\", 0.7 if is_train else 0.0)\n                },\n            )\n\n        resources: NamedResources = {\"main_llm\": llm_resource}\n\n        if self.mode == \"v0\":\n            resources_id = await self.server.update_resources(resources)\n        else:\n            # This should be replaced with store.add_resources()\n            resources_id = \"resource-\" + str(uuid.uuid4())\n            await self.store.update_resources(resources_id=resources_id, resources=resources)\n\n        # 2. Queue tasks for agents to process\n        keys = list(data.keys())\n        num_samples = len(data[keys[0]])\n        rollouts_per_sample = self.train_rollout_n if is_train else 1\n\n        for i in range(num_samples):\n            data_id = str(uuid.uuid4())\n            original_sample = {key: data[key][i] for key in keys}\n            original_sample[\"data_id\"] = data_id\n\n            # For training, each sample is rolled out multiple times\n            for _ in range(rollouts_per_sample):\n                task_metadata = {\"data_id\": data_id, \"is_train\": is_train}\n\n                # Data ID is different from Rollout ID, as one data can have multiple rollouts.\n                if self.mode == \"v0\":\n                    rollout_id = await self.server.queue_task(\n                        sample=_to_native(original_sample),\n                        mode=\"train\" if is_train else \"val\",\n                        resources_id=resources_id,\n                        metadata=task_metadata,\n                    )\n                else:\n                    rollout = await self.store.enqueue_rollout(\n                        input=_to_native(original_sample),\n                        mode=\"train\" if is_train else \"val\",\n                        resources_id=resources_id,\n                        metadata=task_metadata,\n                    )\n                    rollout_id = rollout.rollout_id\n\n                # Store original sample data to reconstruct batch information later\n                self._task_id_to_original_sample[rollout_id] = original_sample\n                self._total_tasks_queued += 1\n\n    def set_up_data_and_server(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n        \"\"\"Synchronous wrapper for setting up data and server resources.\"\"\"\n        coro = self._async_set_up(data, server_addresses, is_train)\n\n        if self.mode == \"v0\":\n            if not self.server.loop or not self.server.startup_event.is_set():\n                raise RuntimeError(\"Server is not running or ready.\")\n\n            future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n\n        else:\n            if self._internal_loop is None:\n                raise RuntimeError(\"Internal loop is not running.\")\n            future = asyncio.run_coroutine_threadsafe(coro, self._internal_loop)\n        try:\n            future.result(timeout=60)  # Wait for completion with a timeout\n        except Exception as e:\n            print(f\"Failed to set up data on server: {e}\")\n            raise\n\n    def _validate_data(self, rollout: Rollout):\n        if rollout.final_reward is None:\n            print(\n                f\"Warning: Reward is None for rollout {rollout.rollout_id}, will be auto-set to {self.reward_fillna_value}.\"\n            )\n        if rollout.triplets is None:\n            print(f\"Warning: Triplet is None for rollout {rollout.rollout_id}.\")\n        elif len(rollout.triplets) == 0:\n            print(f\"Warning: Length of triplets is 0 for rollout {rollout.rollout_id}.\")\n        elif any(not r.response.get(\"token_ids\", []) for r in rollout.triplets):\n            print(f\"Warning: Rollout {rollout.rollout_id} contains empty response: {rollout.triplets}\")\n        elif any(not r.prompt.get(\"token_ids\", []) for r in rollout.triplets):\n            print(f\"Warning: Rollout {rollout.rollout_id} contains empty prompt: {rollout.triplets}\")\n\n    async def _validate_data_v1(self, rollout: RolloutV2) -&gt; Rollout:\n        \"\"\"Convert RolloutV2 to Rollout and validate.\n\n        1. Task: construct from RolloutV2\n        2. Triplets: obtained by querying spans and feeding into the adapter\n        3. Final reward: extracted from last triplet's reward, searching backwards if not found\n        \"\"\"\n        # Query spans for this rollout (latest attempt)\n        spans = await self.store.query_spans(rollout.rollout_id, attempt_id=\"latest\")\n\n        # Convert spans to triplets using the adapter\n        if not spans:\n            # No triplets found, will emit a warning later.\n            triplets = []\n        else:\n            triplets = self.adapter.adapt(spans)\n\n        # Extract final reward from triplets\n        final_reward: Optional[float] = None\n        if triplets:\n            # Search backwards through triplets for the first non-None reward\n            for triplet in reversed(triplets):\n                if triplet.reward is not None:\n                    final_reward = triplet.reward\n                    break\n\n        # Construct the Task object from RolloutV2\n        task = Task(\n            rollout_id=rollout.rollout_id,\n            input=rollout.input,\n            mode=rollout.mode,\n            resources_id=rollout.resources_id,\n            metadata=rollout.metadata or {},\n        )\n\n        # Create the Rollout object (without trace and logs as per user's note)\n        result_rollout = Rollout(\n            rollout_id=rollout.rollout_id,\n            task=task,\n            final_reward=final_reward,\n            triplets=triplets,\n            metadata=rollout.metadata or {},\n        )\n\n        # Run the same validation as v0\n        self._validate_data(result_rollout)\n\n        return result_rollout\n\n    async def _async_run_until_finished(self, verbose: bool = True):\n        \"\"\"Async helper to wait for all tasks to complete.\"\"\"\n        while len(self._completed_rollouts_v0) &lt; self._total_tasks_queued:\n            if self.mode == \"v0\":\n                completed_batch = await self.server.retrieve_completed_rollouts()\n            else:\n                completed_batch = await self.store.wait_for_rollouts(\n                    rollout_ids=list(self._task_id_to_original_sample.keys()), timeout=0\n                )\n            for rollout in completed_batch:\n                if rollout.rollout_id in self._completed_rollouts_v0:\n                    # Already processed, skip\n                    continue\n                if isinstance(rollout, RolloutV2):\n                    rollout = await self._validate_data_v1(rollout)\n                else:\n                    self._validate_data(rollout)\n                if rollout.rollout_id not in self._task_id_to_original_sample:\n                    print(f\"Warning: Received unknown rollout ID {rollout.rollout_id}, skipping.\")\n                else:\n                    self._completed_rollouts_v0[rollout.rollout_id] = rollout\n            if verbose:\n                print(f\"Completed {len(self._completed_rollouts_v0)}/{self._total_tasks_queued} tasks...\")\n            await asyncio.sleep(5)\n\n        print(\"All tasks finished.\")\n\n    def run_until_all_finished(self, verbose: bool = True):\n        \"\"\"Synchronously waits for all queued tasks to be completed and reported.\"\"\"\n        if self._total_tasks_queued == 0:\n            print(\"Warning: No tasks were queued.\")\n            return\n\n        if self.mode == \"v0\":\n            if not self.server.loop or not self.server.startup_event.is_set():\n                raise RuntimeError(\"Server is not running or ready.\")\n            loop = self.server.loop\n        else:\n            loop = self._internal_loop\n            assert loop is not None\n\n        coro = self._async_run_until_finished(verbose)\n        future = asyncio.run_coroutine_threadsafe(coro, loop)\n        try:\n            future.result()  # Wait indefinitely for all tasks to complete\n        except Exception as e:\n            print(f\"Error while waiting for tasks to finish: {e}\")\n            raise\n\n    def get_test_metrics(self):\n        \"\"\"Calculates and returns metrics for a validation run.\"\"\"\n        assert not self.is_train, \"This method should only be called during validation.\"\n        assert len(self._completed_rollouts_v0) == self._total_tasks_queued\n\n        sample_stat_list: List[Dict[str, Any]] = []\n        for _, rollout in self._completed_rollouts_v0.items():\n            final_reward = self._fillna_reward(rollout)\n            if not rollout.triplets:\n                print(f\"Warning: No triplets found for test rollout {rollout.rollout_id}.\")\n                sample_stat_list.append({\"reward\": final_reward})\n                continue\n            response_length_list = [len(triplet.response.get(\"token_ids\", [])) for triplet in rollout.triplets]\n            sample_stat_list.append(\n                {\n                    \"sum_response_length\": np.sum(response_length_list),\n                    \"mean_response_length\": np.mean(response_length_list) if response_length_list else 0,\n                    \"turn_count\": len(rollout.triplets),\n                    \"reward\": final_reward,\n                }\n            )\n\n        stats_w_trace = [stat for stat in sample_stat_list if \"sum_response_length\" in stat]\n        return {\n            \"val/n_rollouts\": len(sample_stat_list),\n            \"val/n_rollouts_w_trace\": len(stats_w_trace),\n            \"val/reward\": np.mean(\n                [stat[\"reward\"] for stat in sample_stat_list]\n            ),  # each rollout must have a reward (fillna if missing)\n            \"val/mean_response_length\": np.mean([stat[\"mean_response_length\"] for stat in stats_w_trace]),\n            \"val/sum_response_length\": np.mean([stat[\"sum_response_length\"] for stat in stats_w_trace]),\n            \"val/turn_count\": np.mean([stat[\"turn_count\"] for stat in stats_w_trace]),\n        }\n\n    def get_train_data_batch(self, max_prompt_length: int, max_response_length: int, device: torch.device):\n        \"\"\"\n        Processes completed rollouts to generate a training data batch.\n\n        This function reconstructs the logic from the original AgentModeDaemon,\n        using data retrieved from the new server architecture. It handles padding,\n        truncation, and tensor creation for the PPO training loop.\n        \"\"\"\n        assert self.is_train, \"This method should only be called during training.\"\n        assert len(self._completed_rollouts_v0) == self._total_tasks_queued\n\n        # 1. Reconstruct the `finished_id_to_sample_info` structure from completed rollouts\n        finished_id_to_sample_info: Dict[str, Dict[str, Any]] = {}\n        finished_id_to_final_reward: Dict[str, float] = {}\n        for rollout_id, rollout in self._completed_rollouts_v0.items():\n            original_sample = self._task_id_to_original_sample[rollout_id]\n\n            final_reward = self._fillna_reward(rollout)\n\n            if not rollout.triplets:\n                finished_id_to_final_reward[rollout_id] = final_reward\n                print(f\"Warning: No triplets found for training rollout {rollout.rollout_id}, skipping.\")\n                continue\n\n            # The client should report triplets that contain prompt_ids and response_ids.\n            # Example triplet.prompt: {\"token_ids\": [...]}\n            # Example triplet.response: {\"token_ids\": [...]}\n            trace_list = [\n                {\"prompt_ids\": t.prompt.get(\"token_ids\", []), \"response_ids\": t.response.get(\"token_ids\", [])}\n                for t in rollout.triplets\n            ]\n            info = {\n                \"reward\": final_reward,\n                \"trace_list\": trace_list,\n                \"data_id\": original_sample[\"data_id\"],\n            }\n            finished_id_to_sample_info[rollout_id] = info\n            finished_id_to_final_reward[rollout_id] = final_reward\n        #\n        # --- Data processing and tensor creation logic ---\n        # Get all the reported data.\n        # prompt_ids are left-padded.\n        # response_ids are right-padded.\n        # They are concatenated in the middle.\n        # Discard handling:\n        #   - Those exceeding max_prompt_length will be marked for discard, but not\n        #     discarded here. They are only truncated and marked, to be discarded later.\n        #     This is for the correctness of the advantage calculation.\n        #   - The discard for the PPO mini-batch should also be handled this way.\n        input_ids_list: List[List[int]] = []\n        input_attention_mask_list: List[List[int]] = []\n        response_ids_list: List[List[int]] = []\n        response_attention_mask_list: List[List[int]] = []\n        reward_list: List[float] = []\n        data_id_list: List[str] = []\n        rollout_id_list: List[str] = []\n        turn_index_list: List[int] = []\n        is_drop_list: List[bool] = []\n        n_trunc_sample_because_of_response = 0\n\n        for rollout_id, sample_info in finished_id_to_sample_info.items():\n            for turn_index, trace in enumerate(sample_info[\"trace_list\"]):\n\n                reward_list.append(sample_info[\"reward\"])\n                prompt_ids, response_ids = trace[\"prompt_ids\"], trace[\"response_ids\"]\n\n                # Mark samples with prompts exceeding max_prompt_length to be dropped later\n                if len(prompt_ids) &gt; max_prompt_length:\n                    prompt_ids = prompt_ids[:max_prompt_length]\n                    is_drop_list.append(True)\n                else:\n                    is_drop_list.append(False)\n\n                # Truncate responses that exceed max_response_length\n                if len(response_ids) &gt; max_response_length:\n                    response_ids = response_ids[:max_response_length]\n                    n_trunc_sample_because_of_response += 1\n\n                # Pad prompts to the left and responses to the right\n                one_input_ids, one_input_attention_mask = get_left_padded_ids_and_attention_mask(\n                    prompt_ids, max_prompt_length, self.pad_token_id\n                )\n                one_response_ids, one_response_attention_mask = get_right_padded_ids_and_attention_mask(\n                    response_ids, max_response_length, self.pad_token_id\n                )\n\n                input_ids_list.append(one_input_ids)\n                input_attention_mask_list.append(one_input_attention_mask)\n                response_ids_list.append(one_response_ids)\n                response_attention_mask_list.append(one_response_attention_mask)\n                data_id_list.append(sample_info[\"data_id\"])\n                rollout_id_list.append(rollout_id)\n                turn_index_list.append(turn_index)\n\n        n_transition = len(input_ids_list)\n        batch_input_ids = torch.LongTensor(input_ids_list).to(device)\n        input_attention_mask = torch.LongTensor(input_attention_mask_list).to(device)\n        batch_response_ids = torch.LongTensor(response_ids_list).to(device)\n        response_attention_mask = torch.LongTensor(response_attention_mask_list).to(device)\n\n        # Concatenate prompts and responses to form the full sequence\n        batch_seq = torch.cat([batch_input_ids, batch_response_ids], dim=-1)\n        attention_mask = torch.cat([input_attention_mask, response_attention_mask], dim=-1)\n        position_ids = torch.clamp(torch.cumsum(attention_mask, dim=-1) - 1, min=0)\n        is_drop_mask = torch.BoolTensor(is_drop_list).to(device)\n        scores = torch.tensor(reward_list, dtype=torch.bfloat16).to(device)\n\n        # Create token-level scores by placing the final reward at the last token position\n        token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)\n        # At the eos_mask_idx position of each sample, fill in the corresponding scores.\n        # torch.arange(n_transition) generates [0,1,2,...,bsz-1] as indices for the batch dimension.\n        eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n        token_level_scores[torch.arange(n_transition), eos_mask_idx] = scores\n        # Only take the last response_length part of the sequence to get the token-level scores for the model's response part.\n        token_level_scores = token_level_scores[:, -max_response_length:]\n\n        # Form the final batch using TensorDict\n        batch = TensorDict(\n            {\n                \"prompts\": batch_input_ids,\n                \"responses\": batch_response_ids,\n                \"input_ids\": batch_seq,  # here input_ids become the whole sentences\n                \"attention_mask\": attention_mask,\n                \"position_ids\": position_ids,\n                \"is_drop_mask\": is_drop_mask,\n                \"token_level_scores\": token_level_scores.contiguous(),\n            },\n            batch_size=n_transition,\n        )\n        data_proto = DataProto(batch=batch)\n\n        data_metrics = {\n            \"training/reward\": np.mean(list(finished_id_to_final_reward.values())),\n            \"training/n_rollouts\": len(finished_id_to_final_reward),\n            \"training/n_rollouts_w_trace\": len(finished_id_to_sample_info),\n            \"training/n_truncated_triplets\": n_trunc_sample_because_of_response,\n            \"training/n_triplets\": n_transition,\n        }\n\n        # Add non-tensor data for advantage calculation and logging\n        data_proto.non_tensor_batch[\"data_id_list\"] = np.array(data_id_list)  # type: ignore\n        data_proto.non_tensor_batch[\"rollout_id_list\"] = np.array(rollout_id_list)  # type: ignore\n        data_proto.non_tensor_batch[\"turn_index_list\"] = np.array(turn_index_list)  # type: ignore\n\n        return data_proto, data_metrics\n\n    def clear_data_and_server(self):\n        \"\"\"Resets the internal state of the daemon for the next run.\"\"\"\n        self.backend_llm_server_addresses = []\n        self._completed_rollouts_v0.clear()\n        self._task_id_to_original_sample.clear()\n        self._total_tasks_queued = 0\n        # For a true reset, the server's internal queues would also need clearing.\n        # This implementation assumes that `set_up_data_and_server` is called\n        # for each new run, effectively starting a fresh batch.\n\n    def _fillna_reward(self, rollout: Rollout):\n        if rollout.final_reward is None:\n            if self.reward_fillna_value is not None:  # type: ignore\n                final_reward = self.reward_fillna_value\n            else:\n                raise ValueError(f\"Reward is None for rollout {rollout.rollout_id}, please check the reward function.\")\n        else:\n            final_reward = rollout.final_reward\n        return final_reward\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.clear_data_and_server","title":"<code>clear_data_and_server()</code>","text":"<p>Resets the internal state of the daemon for the next run.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def clear_data_and_server(self):\n    \"\"\"Resets the internal state of the daemon for the next run.\"\"\"\n    self.backend_llm_server_addresses = []\n    self._completed_rollouts_v0.clear()\n    self._task_id_to_original_sample.clear()\n    self._total_tasks_queued = 0\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.get_test_metrics","title":"<code>get_test_metrics()</code>","text":"<p>Calculates and returns metrics for a validation run.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_test_metrics(self):\n    \"\"\"Calculates and returns metrics for a validation run.\"\"\"\n    assert not self.is_train, \"This method should only be called during validation.\"\n    assert len(self._completed_rollouts_v0) == self._total_tasks_queued\n\n    sample_stat_list: List[Dict[str, Any]] = []\n    for _, rollout in self._completed_rollouts_v0.items():\n        final_reward = self._fillna_reward(rollout)\n        if not rollout.triplets:\n            print(f\"Warning: No triplets found for test rollout {rollout.rollout_id}.\")\n            sample_stat_list.append({\"reward\": final_reward})\n            continue\n        response_length_list = [len(triplet.response.get(\"token_ids\", [])) for triplet in rollout.triplets]\n        sample_stat_list.append(\n            {\n                \"sum_response_length\": np.sum(response_length_list),\n                \"mean_response_length\": np.mean(response_length_list) if response_length_list else 0,\n                \"turn_count\": len(rollout.triplets),\n                \"reward\": final_reward,\n            }\n        )\n\n    stats_w_trace = [stat for stat in sample_stat_list if \"sum_response_length\" in stat]\n    return {\n        \"val/n_rollouts\": len(sample_stat_list),\n        \"val/n_rollouts_w_trace\": len(stats_w_trace),\n        \"val/reward\": np.mean(\n            [stat[\"reward\"] for stat in sample_stat_list]\n        ),  # each rollout must have a reward (fillna if missing)\n        \"val/mean_response_length\": np.mean([stat[\"mean_response_length\"] for stat in stats_w_trace]),\n        \"val/sum_response_length\": np.mean([stat[\"sum_response_length\"] for stat in stats_w_trace]),\n        \"val/turn_count\": np.mean([stat[\"turn_count\"] for stat in stats_w_trace]),\n    }\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.get_train_data_batch","title":"<code>get_train_data_batch(max_prompt_length, max_response_length, device)</code>","text":"<p>Processes completed rollouts to generate a training data batch.</p> <p>This function reconstructs the logic from the original AgentModeDaemon, using data retrieved from the new server architecture. It handles padding, truncation, and tensor creation for the PPO training loop.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_train_data_batch(self, max_prompt_length: int, max_response_length: int, device: torch.device):\n    \"\"\"\n    Processes completed rollouts to generate a training data batch.\n\n    This function reconstructs the logic from the original AgentModeDaemon,\n    using data retrieved from the new server architecture. It handles padding,\n    truncation, and tensor creation for the PPO training loop.\n    \"\"\"\n    assert self.is_train, \"This method should only be called during training.\"\n    assert len(self._completed_rollouts_v0) == self._total_tasks_queued\n\n    # 1. Reconstruct the `finished_id_to_sample_info` structure from completed rollouts\n    finished_id_to_sample_info: Dict[str, Dict[str, Any]] = {}\n    finished_id_to_final_reward: Dict[str, float] = {}\n    for rollout_id, rollout in self._completed_rollouts_v0.items():\n        original_sample = self._task_id_to_original_sample[rollout_id]\n\n        final_reward = self._fillna_reward(rollout)\n\n        if not rollout.triplets:\n            finished_id_to_final_reward[rollout_id] = final_reward\n            print(f\"Warning: No triplets found for training rollout {rollout.rollout_id}, skipping.\")\n            continue\n\n        # The client should report triplets that contain prompt_ids and response_ids.\n        # Example triplet.prompt: {\"token_ids\": [...]}\n        # Example triplet.response: {\"token_ids\": [...]}\n        trace_list = [\n            {\"prompt_ids\": t.prompt.get(\"token_ids\", []), \"response_ids\": t.response.get(\"token_ids\", [])}\n            for t in rollout.triplets\n        ]\n        info = {\n            \"reward\": final_reward,\n            \"trace_list\": trace_list,\n            \"data_id\": original_sample[\"data_id\"],\n        }\n        finished_id_to_sample_info[rollout_id] = info\n        finished_id_to_final_reward[rollout_id] = final_reward\n    #\n    # --- Data processing and tensor creation logic ---\n    # Get all the reported data.\n    # prompt_ids are left-padded.\n    # response_ids are right-padded.\n    # They are concatenated in the middle.\n    # Discard handling:\n    #   - Those exceeding max_prompt_length will be marked for discard, but not\n    #     discarded here. They are only truncated and marked, to be discarded later.\n    #     This is for the correctness of the advantage calculation.\n    #   - The discard for the PPO mini-batch should also be handled this way.\n    input_ids_list: List[List[int]] = []\n    input_attention_mask_list: List[List[int]] = []\n    response_ids_list: List[List[int]] = []\n    response_attention_mask_list: List[List[int]] = []\n    reward_list: List[float] = []\n    data_id_list: List[str] = []\n    rollout_id_list: List[str] = []\n    turn_index_list: List[int] = []\n    is_drop_list: List[bool] = []\n    n_trunc_sample_because_of_response = 0\n\n    for rollout_id, sample_info in finished_id_to_sample_info.items():\n        for turn_index, trace in enumerate(sample_info[\"trace_list\"]):\n\n            reward_list.append(sample_info[\"reward\"])\n            prompt_ids, response_ids = trace[\"prompt_ids\"], trace[\"response_ids\"]\n\n            # Mark samples with prompts exceeding max_prompt_length to be dropped later\n            if len(prompt_ids) &gt; max_prompt_length:\n                prompt_ids = prompt_ids[:max_prompt_length]\n                is_drop_list.append(True)\n            else:\n                is_drop_list.append(False)\n\n            # Truncate responses that exceed max_response_length\n            if len(response_ids) &gt; max_response_length:\n                response_ids = response_ids[:max_response_length]\n                n_trunc_sample_because_of_response += 1\n\n            # Pad prompts to the left and responses to the right\n            one_input_ids, one_input_attention_mask = get_left_padded_ids_and_attention_mask(\n                prompt_ids, max_prompt_length, self.pad_token_id\n            )\n            one_response_ids, one_response_attention_mask = get_right_padded_ids_and_attention_mask(\n                response_ids, max_response_length, self.pad_token_id\n            )\n\n            input_ids_list.append(one_input_ids)\n            input_attention_mask_list.append(one_input_attention_mask)\n            response_ids_list.append(one_response_ids)\n            response_attention_mask_list.append(one_response_attention_mask)\n            data_id_list.append(sample_info[\"data_id\"])\n            rollout_id_list.append(rollout_id)\n            turn_index_list.append(turn_index)\n\n    n_transition = len(input_ids_list)\n    batch_input_ids = torch.LongTensor(input_ids_list).to(device)\n    input_attention_mask = torch.LongTensor(input_attention_mask_list).to(device)\n    batch_response_ids = torch.LongTensor(response_ids_list).to(device)\n    response_attention_mask = torch.LongTensor(response_attention_mask_list).to(device)\n\n    # Concatenate prompts and responses to form the full sequence\n    batch_seq = torch.cat([batch_input_ids, batch_response_ids], dim=-1)\n    attention_mask = torch.cat([input_attention_mask, response_attention_mask], dim=-1)\n    position_ids = torch.clamp(torch.cumsum(attention_mask, dim=-1) - 1, min=0)\n    is_drop_mask = torch.BoolTensor(is_drop_list).to(device)\n    scores = torch.tensor(reward_list, dtype=torch.bfloat16).to(device)\n\n    # Create token-level scores by placing the final reward at the last token position\n    token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)\n    # At the eos_mask_idx position of each sample, fill in the corresponding scores.\n    # torch.arange(n_transition) generates [0,1,2,...,bsz-1] as indices for the batch dimension.\n    eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n    token_level_scores[torch.arange(n_transition), eos_mask_idx] = scores\n    # Only take the last response_length part of the sequence to get the token-level scores for the model's response part.\n    token_level_scores = token_level_scores[:, -max_response_length:]\n\n    # Form the final batch using TensorDict\n    batch = TensorDict(\n        {\n            \"prompts\": batch_input_ids,\n            \"responses\": batch_response_ids,\n            \"input_ids\": batch_seq,  # here input_ids become the whole sentences\n            \"attention_mask\": attention_mask,\n            \"position_ids\": position_ids,\n            \"is_drop_mask\": is_drop_mask,\n            \"token_level_scores\": token_level_scores.contiguous(),\n        },\n        batch_size=n_transition,\n    )\n    data_proto = DataProto(batch=batch)\n\n    data_metrics = {\n        \"training/reward\": np.mean(list(finished_id_to_final_reward.values())),\n        \"training/n_rollouts\": len(finished_id_to_final_reward),\n        \"training/n_rollouts_w_trace\": len(finished_id_to_sample_info),\n        \"training/n_truncated_triplets\": n_trunc_sample_because_of_response,\n        \"training/n_triplets\": n_transition,\n    }\n\n    # Add non-tensor data for advantage calculation and logging\n    data_proto.non_tensor_batch[\"data_id_list\"] = np.array(data_id_list)  # type: ignore\n    data_proto.non_tensor_batch[\"rollout_id_list\"] = np.array(rollout_id_list)  # type: ignore\n    data_proto.non_tensor_batch[\"turn_index_list\"] = np.array(turn_index_list)  # type: ignore\n\n    return data_proto, data_metrics\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.run_until_all_finished","title":"<code>run_until_all_finished(verbose=True)</code>","text":"<p>Synchronously waits for all queued tasks to be completed and reported.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def run_until_all_finished(self, verbose: bool = True):\n    \"\"\"Synchronously waits for all queued tasks to be completed and reported.\"\"\"\n    if self._total_tasks_queued == 0:\n        print(\"Warning: No tasks were queued.\")\n        return\n\n    if self.mode == \"v0\":\n        if not self.server.loop or not self.server.startup_event.is_set():\n            raise RuntimeError(\"Server is not running or ready.\")\n        loop = self.server.loop\n    else:\n        loop = self._internal_loop\n        assert loop is not None\n\n    coro = self._async_run_until_finished(verbose)\n    future = asyncio.run_coroutine_threadsafe(coro, loop)\n    try:\n        future.result()  # Wait indefinitely for all tasks to complete\n    except Exception as e:\n        print(f\"Error while waiting for tasks to finish: {e}\")\n        raise\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.set_up_data_and_server","title":"<code>set_up_data_and_server(data, server_addresses, is_train=True)</code>","text":"<p>Synchronous wrapper for setting up data and server resources.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def set_up_data_and_server(self, data: Dict[str, Any], server_addresses: List[str], is_train: bool = True):\n    \"\"\"Synchronous wrapper for setting up data and server resources.\"\"\"\n    coro = self._async_set_up(data, server_addresses, is_train)\n\n    if self.mode == \"v0\":\n        if not self.server.loop or not self.server.startup_event.is_set():\n            raise RuntimeError(\"Server is not running or ready.\")\n\n        future = asyncio.run_coroutine_threadsafe(coro, self.server.loop)\n\n    else:\n        if self._internal_loop is None:\n            raise RuntimeError(\"Internal loop is not running.\")\n        future = asyncio.run_coroutine_threadsafe(coro, self._internal_loop)\n    try:\n        future.result(timeout=60)  # Wait for completion with a timeout\n    except Exception as e:\n        print(f\"Failed to set up data on server: {e}\")\n        raise\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.AgentModeDaemon.start","title":"<code>start()</code>","text":"<p>Starts the main AgentLightningServer and the proxy server.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def start(self):\n    \"\"\"Starts the main AgentLightningServer and the proxy server.\"\"\"\n\n    if self.mode == \"v0\":\n\n        def run_server():\n            \"\"\"Run the AgentLightningServer in a separate thread.\"\"\"\n            asyncio.run(self.server.run_forever())\n\n        self._server_thread = threading.Thread(target=run_server, daemon=True)\n        self._server_thread.start()\n\n        # Wait for the server's internal startup event to be set.\n        print(\"Waiting for AgentLightningServer to start...\")\n        is_ready = self.server.startup_event.wait(timeout=20.0)  # Wait up to 20s\n        if not is_ready:\n            raise RuntimeError(\"AgentLightningServer failed to start within the timeout period.\")\n\n        print(f\"AgentLightningServer control plane running on port {self.server_port}\")\n\n        self._start_proxy_server_v0()\n    else:\n        # Agent lightning server is no longer needed;\n        # Start proxy server in _async_set_up\n        pass\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.BaseTraceTripletAdapter","title":"<code>BaseTraceTripletAdapter</code>","text":"<p>               Bases: <code>TraceAdapter[List[Triplet]]</code></p> <p>Base class for trace triplet adapters.</p> Source code in <code>agentlightning/adapter/triplet.py</code> <pre><code>class BaseTraceTripletAdapter(TraceAdapter[List[Triplet]]):\n    \"\"\"\n    Base class for trace triplet adapters.\n    \"\"\"\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset. You don't have to inherit from this class; you can use a simple list if you want to.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Dataset(Protocol, Generic[T_co]):\n    \"\"\"The general interface for a dataset.\n\n    It's currently implemented as a protocol, having a similar interface to torch.utils.data.Dataset.\n    You don't have to inherit from this class; you can use a simple list if you want to.\n    \"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co: ...\n\n    def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Provide an LLM endpoint and model name as a resource.</p> <p>Attributes:</p> Name Type Description <code>endpoint</code> <code>str</code> <p>The URL of the LLM API endpoint.</p> <code>model</code> <code>str</code> <p>The identifier for the model to be used (e.g., 'gpt-4o').</p> <code>sampling_parameters</code> <code>SamplingParameters</code> <p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class LLM(Resource):\n    \"\"\"\n    Provide an LLM endpoint and model name as a resource.\n\n    Attributes:\n        endpoint (str): The URL of the LLM API endpoint.\n        model (str): The identifier for the model to be used (e.g., 'gpt-4o').\n        sampling_parameters (SamplingParameters): A dictionary of hyperparameters\n            for model inference, such as temperature, top_p, etc.\n    \"\"\"\n\n    resource_type: Literal[\"llm\"] = \"llm\"\n    endpoint: str\n    model: str\n    api_key: Optional[str] = None\n    sampling_parameters: Dict[str, Any] = Field(default_factory=dict)\n\n    def base_url(self, *args: Any, **kwargs: Any) -&gt; str:\n        \"\"\"The base_url to put into openai.OpenAI.\n\n        Users are encouraged to use `base_url` to get the LLM endpoint instead of accessing `endpoint` directly.\n        \"\"\"\n        return self.endpoint\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLM.base_url","title":"<code>base_url(*args, **kwargs)</code>","text":"<p>The base_url to put into openai.OpenAI.</p> <p>Users are encouraged to use <code>base_url</code> to get the LLM endpoint instead of accessing <code>endpoint</code> directly.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>def base_url(self, *args: Any, **kwargs: Any) -&gt; str:\n    \"\"\"The base_url to put into openai.OpenAI.\n\n    Users are encouraged to use `base_url` to get the LLM endpoint instead of accessing `endpoint` directly.\n    \"\"\"\n    return self.endpoint\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy","title":"<code>LLMProxy</code>","text":"<p>Host a LiteLLM OpenAI-compatible proxy bound to a LightningStore.</p> <p>The proxy:</p> <ul> <li>Serves an OpenAI-compatible API via uvicorn.</li> <li>Adds rollout/attempt routing and headers via middleware.</li> <li>Registers OTEL export and token-id callbacks.</li> <li>Writes a LiteLLM worker config file with <code>model_list</code> and settings.</li> </ul> <p>Lifecycle:</p> <ul> <li><code>start()</code> writes config, starts uvicorn server in a thread, and waits until ready.</li> <li><code>stop()</code> tears down the server and removes the temp config file.</li> <li><code>restart()</code> convenience wrapper to stop then start.</li> </ul> <p>Usage Note: As the LLM Proxy sets up an OpenTelemetry tracer, it's recommended to run it in a different process from the main runner (i.e., tracer from agents).</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>TCP port to bind.</p> required <code>model_list</code> <code>List[ModelConfig]</code> <p>LiteLLM <code>model_list</code> entries.</p> required <code>store</code> <code>LightningStore</code> <p>LightningStore used for span sequence and persistence.</p> required <code>host</code> <code>str | None</code> <p>Publicly reachable host used in resource endpoints. Defaults to best-guess IPv4.</p> <code>None</code> <code>litellm_config</code> <code>Dict[str, Any] | None</code> <p>Extra LiteLLM proxy config merged with <code>model_list</code>.</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Default LiteLLM retry count injected into <code>litellm_settings</code>.</p> <code>0</code> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>class LLMProxy:\n    \"\"\"Host a LiteLLM OpenAI-compatible proxy bound to a LightningStore.\n\n    The proxy:\n\n    * Serves an OpenAI-compatible API via uvicorn.\n    * Adds rollout/attempt routing and headers via middleware.\n    * Registers OTEL export and token-id callbacks.\n    * Writes a LiteLLM worker config file with ``model_list`` and settings.\n\n    Lifecycle:\n\n    * ``start()`` writes config, starts uvicorn server in a thread, and waits until ready.\n    * ``stop()`` tears down the server and removes the temp config file.\n    * ``restart()`` convenience wrapper to stop then start.\n\n    Usage Note:\n    As the LLM Proxy sets up an OpenTelemetry tracer, it's recommended to run it in a different\n    process from the main runner (i.e., tracer from agents).\n\n    Args:\n        port: TCP port to bind.\n        model_list: LiteLLM ``model_list`` entries.\n        store: LightningStore used for span sequence and persistence.\n        host: Publicly reachable host used in resource endpoints. Defaults to best-guess IPv4.\n        litellm_config: Extra LiteLLM proxy config merged with ``model_list``.\n        num_retries: Default LiteLLM retry count injected into ``litellm_settings``.\n    \"\"\"\n\n    def __init__(\n        self,\n        port: int,\n        model_list: List[ModelConfig],\n        store: LightningStore,\n        host: str | None = None,\n        litellm_config: Dict[str, Any] | None = None,\n        num_retries: int = 0,\n    ):\n        self.store = store\n        self.host = host or _get_default_ipv4_address()\n        self.port = port\n        self.model_list = model_list\n        self.litellm_config = litellm_config or {}\n\n        # Ensure num_retries is present inside the litellm_settings block.\n        self.litellm_config.setdefault(\"litellm_settings\", {})\n        self.litellm_config[\"litellm_settings\"].setdefault(\"num_retries\", num_retries)\n\n        self._server_thread = None\n        self._config_file = None\n        self._uvicorn_server = None\n        self._ready_event = threading.Event()\n\n    def set_store(self, store: LightningStore) -&gt; None:\n        \"\"\"Set the store for the proxy.\n\n        Args:\n            store: The store to use for the proxy.\n        \"\"\"\n        self.store = store\n\n    def update_model_list(self, model_list: List[ModelConfig]) -&gt; None:\n        \"\"\"Replace the in-memory model list and hot-restart if running.\n\n        Args:\n            model_list: New list of model entries.\n        \"\"\"\n        self.model_list = model_list\n        logger.info(f\"Updating LLMProxy model list to: {model_list}\")\n        if self.is_running():\n            self.restart()\n        # Do nothing if the server is not running.\n\n    def _wait_until_started(self, startup_timeout: float = 20.0):\n        \"\"\"Block until the uvicorn server reports started or timeout.\n\n        Args:\n            startup_timeout: Maximum seconds to wait.\n        \"\"\"\n        start = time.time()\n        while True:\n            if self._uvicorn_server is None:\n                break\n            if self._uvicorn_server.started:\n                self._ready_event.set()\n                break\n            if self._uvicorn_server.should_exit:\n                break\n            if time.time() - start &gt; startup_timeout:\n                break\n            time.sleep(0.01)\n\n    def start(self):\n        \"\"\"Start the proxy server thread and initialize global wiring.\n\n        Side effects:\n\n        * Sets the module-level global store for middleware/exporter access.\n        * Calls ``initialize()`` once to register middleware and callbacks.\n        * Writes a temporary YAML config consumed by LiteLLM worker.\n        * Launches uvicorn in a daemon thread and waits for readiness.\n        \"\"\"\n        if self.is_running():\n            # Trigger restart\n            self.stop()\n\n        global _global_store\n\n        _global_store = self.store\n\n        # Initialize global middleware and callbacks once.\n        initialize()\n\n        # Persist a temp worker config for LiteLLM and point the proxy at it.\n        self._config_file = tempfile.NamedTemporaryFile(suffix=\".yaml\", delete=False).name\n        with open(self._config_file, \"w\") as fp:\n            yaml.safe_dump(\n                {\n                    \"model_list\": self.model_list,\n                    **self.litellm_config,\n                },\n                fp,\n            )\n\n        save_worker_config(config=self._config_file)\n\n        # Bind to all interfaces to allow other hosts to reach it if needed.\n        self._uvicorn_server = uvicorn.Server(uvicorn.Config(app, host=\"0.0.0.0\", port=self.port))\n\n        def run_server():\n            # Serve uvicorn in this background thread with its own event loop.\n            assert self._uvicorn_server is not None\n            asyncio.run(self._uvicorn_server.serve())\n\n        logger.info(\"Starting LLMProxy server thread...\")\n        self._ready_event.clear()\n        self._server_thread = threading.Thread(target=run_server, daemon=True)\n        self._server_thread.start()\n        self._wait_until_started()\n\n    def stop(self):\n        \"\"\"Stop the proxy server and clean up temporary artifacts.\n\n        This is a best-effort graceful shutdown with a bounded join timeout.\n        \"\"\"\n        if not self.is_running():\n            logger.warning(\"LLMProxy is not running. Nothing to stop.\")\n            return\n\n        # Remove worker config to avoid stale references.\n        if self._config_file and os.path.exists(self._config_file):\n            os.unlink(self._config_file)\n\n        logger.info(\"Stopping LLMProxy server thread...\")\n        stop_success = True\n        if self._server_thread is not None and self._uvicorn_server is not None and self._uvicorn_server.started:\n            self._uvicorn_server.should_exit = True\n            self._server_thread.join(timeout=10.0)  # Allow time for graceful shutdown.\n            if self._server_thread.is_alive():\n                logger.error(\n                    \"LLMProxy server thread is still alive after 10 seconds. Cannot kill it because it's a thread.\"\n                )\n                stop_success = False\n            self._server_thread = None\n            self._uvicorn_server = None\n            self._config_file = None\n            self._ready_event.clear()\n            if not _check_port(self.host, self.port):\n                logger.error(f\"Port {self.port} is still in use. Stopping LLMProxy is not successful.\")\n                stop_success = False\n        if stop_success:\n            logger.info(\"LLMProxy server thread stopped.\")\n        else:\n            logger.error(\"LLMProxy server is not stopped successfully.\")\n\n    def restart(self, *, _port: int | None = None) -&gt; None:\n        \"\"\"Restart the proxy if running, else start it.\n\n        Convenience wrapper calling ``stop()`` followed by ``start()``.\n        \"\"\"\n        logger.info(\"Restarting LLMProxy server...\")\n        if self.is_running():\n            self.stop()\n        if _port is not None:\n            self.port = _port\n        self.start()\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Return whether the uvicorn server is active.\n\n        Returns:\n            bool: True if server was started and did not signal exit.\n        \"\"\"\n        return self._uvicorn_server is not None and self._uvicorn_server.started\n\n    def as_resource(\n        self,\n        rollout_id: str | None = None,\n        attempt_id: str | None = None,\n        model: str | None = None,\n        sampling_parameters: Dict[str, Any] | None = None,\n    ) -&gt; LLM:\n        \"\"\"Create an ``LLM`` resource pointing at this proxy with rollout context.\n\n        The returned endpoint is:\n            ``http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}``\n\n        Args:\n            rollout_id: Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.\n            attempt_id: Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.\n            model: Logical model name to use. If omitted and exactly one model\n                is configured, that model is used.\n            sampling_parameters: Optional default sampling parameters.\n\n        Returns:\n            LLM: Configured resource ready for OpenAI-compatible calls.\n\n        Raises:\n            ValueError: If ``model`` is omitted and zero or multiple models are configured.\n        \"\"\"\n        if model is None:\n            if len(self.model_list) == 1:\n                model = self.model_list[0][\"model_name\"]\n            else:\n                raise ValueError(\n                    f\"Multiple or zero models found in model_list: {self.model_list}. Please specify the model.\"\n                )\n\n        if rollout_id is None and attempt_id is None:\n            return ProxyLLM(\n                endpoint=f\"http://{self.host}:{self.port}\",\n                model=model,\n                sampling_parameters=dict(sampling_parameters or {}),\n            )\n        elif rollout_id is not None and attempt_id is not None:\n            return LLM(\n                endpoint=f\"http://{self.host}:{self.port}/rollout/{rollout_id}/attempt/{attempt_id}\",\n                model=model,\n                sampling_parameters=dict(sampling_parameters or {}),\n            )\n        else:\n            raise ValueError(\"Either rollout_id and attempt_id must be provided, or neither.\")\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.as_resource","title":"<code>as_resource(rollout_id=None, attempt_id=None, model=None, sampling_parameters=None)</code>","text":"<p>Create an <code>LLM</code> resource pointing at this proxy with rollout context.</p> The returned endpoint is <p><code>http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>rollout_id</code> <code>str | None</code> <p>Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> <code>None</code> <code>attempt_id</code> <code>str | None</code> <p>Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Logical model name to use. If omitted and exactly one model is configured, that model is used.</p> <code>None</code> <code>sampling_parameters</code> <code>Dict[str, Any] | None</code> <p>Optional default sampling parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LLM</code> <code>LLM</code> <p>Configured resource ready for OpenAI-compatible calls.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is omitted and zero or multiple models are configured.</p> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def as_resource(\n    self,\n    rollout_id: str | None = None,\n    attempt_id: str | None = None,\n    model: str | None = None,\n    sampling_parameters: Dict[str, Any] | None = None,\n) -&gt; LLM:\n    \"\"\"Create an ``LLM`` resource pointing at this proxy with rollout context.\n\n    The returned endpoint is:\n        ``http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}``\n\n    Args:\n        rollout_id: Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.\n        attempt_id: Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.\n        model: Logical model name to use. If omitted and exactly one model\n            is configured, that model is used.\n        sampling_parameters: Optional default sampling parameters.\n\n    Returns:\n        LLM: Configured resource ready for OpenAI-compatible calls.\n\n    Raises:\n        ValueError: If ``model`` is omitted and zero or multiple models are configured.\n    \"\"\"\n    if model is None:\n        if len(self.model_list) == 1:\n            model = self.model_list[0][\"model_name\"]\n        else:\n            raise ValueError(\n                f\"Multiple or zero models found in model_list: {self.model_list}. Please specify the model.\"\n            )\n\n    if rollout_id is None and attempt_id is None:\n        return ProxyLLM(\n            endpoint=f\"http://{self.host}:{self.port}\",\n            model=model,\n            sampling_parameters=dict(sampling_parameters or {}),\n        )\n    elif rollout_id is not None and attempt_id is not None:\n        return LLM(\n            endpoint=f\"http://{self.host}:{self.port}/rollout/{rollout_id}/attempt/{attempt_id}\",\n            model=model,\n            sampling_parameters=dict(sampling_parameters or {}),\n        )\n    else:\n        raise ValueError(\"Either rollout_id and attempt_id must be provided, or neither.\")\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.is_running","title":"<code>is_running()</code>","text":"<p>Return whether the uvicorn server is active.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if server was started and did not signal exit.</p> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Return whether the uvicorn server is active.\n\n    Returns:\n        bool: True if server was started and did not signal exit.\n    \"\"\"\n    return self._uvicorn_server is not None and self._uvicorn_server.started\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.restart","title":"<code>restart(*, _port=None)</code>","text":"<p>Restart the proxy if running, else start it.</p> <p>Convenience wrapper calling <code>stop()</code> followed by <code>start()</code>.</p> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def restart(self, *, _port: int | None = None) -&gt; None:\n    \"\"\"Restart the proxy if running, else start it.\n\n    Convenience wrapper calling ``stop()`` followed by ``start()``.\n    \"\"\"\n    logger.info(\"Restarting LLMProxy server...\")\n    if self.is_running():\n        self.stop()\n    if _port is not None:\n        self.port = _port\n    self.start()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.set_store","title":"<code>set_store(store)</code>","text":"<p>Set the store for the proxy.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>LightningStore</code> <p>The store to use for the proxy.</p> required Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def set_store(self, store: LightningStore) -&gt; None:\n    \"\"\"Set the store for the proxy.\n\n    Args:\n        store: The store to use for the proxy.\n    \"\"\"\n    self.store = store\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.start","title":"<code>start()</code>","text":"<p>Start the proxy server thread and initialize global wiring.</p> <p>Side effects:</p> <ul> <li>Sets the module-level global store for middleware/exporter access.</li> <li>Calls <code>initialize()</code> once to register middleware and callbacks.</li> <li>Writes a temporary YAML config consumed by LiteLLM worker.</li> <li>Launches uvicorn in a daemon thread and waits for readiness.</li> </ul> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def start(self):\n    \"\"\"Start the proxy server thread and initialize global wiring.\n\n    Side effects:\n\n    * Sets the module-level global store for middleware/exporter access.\n    * Calls ``initialize()`` once to register middleware and callbacks.\n    * Writes a temporary YAML config consumed by LiteLLM worker.\n    * Launches uvicorn in a daemon thread and waits for readiness.\n    \"\"\"\n    if self.is_running():\n        # Trigger restart\n        self.stop()\n\n    global _global_store\n\n    _global_store = self.store\n\n    # Initialize global middleware and callbacks once.\n    initialize()\n\n    # Persist a temp worker config for LiteLLM and point the proxy at it.\n    self._config_file = tempfile.NamedTemporaryFile(suffix=\".yaml\", delete=False).name\n    with open(self._config_file, \"w\") as fp:\n        yaml.safe_dump(\n            {\n                \"model_list\": self.model_list,\n                **self.litellm_config,\n            },\n            fp,\n        )\n\n    save_worker_config(config=self._config_file)\n\n    # Bind to all interfaces to allow other hosts to reach it if needed.\n    self._uvicorn_server = uvicorn.Server(uvicorn.Config(app, host=\"0.0.0.0\", port=self.port))\n\n    def run_server():\n        # Serve uvicorn in this background thread with its own event loop.\n        assert self._uvicorn_server is not None\n        asyncio.run(self._uvicorn_server.serve())\n\n    logger.info(\"Starting LLMProxy server thread...\")\n    self._ready_event.clear()\n    self._server_thread = threading.Thread(target=run_server, daemon=True)\n    self._server_thread.start()\n    self._wait_until_started()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.stop","title":"<code>stop()</code>","text":"<p>Stop the proxy server and clean up temporary artifacts.</p> <p>This is a best-effort graceful shutdown with a bounded join timeout.</p> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the proxy server and clean up temporary artifacts.\n\n    This is a best-effort graceful shutdown with a bounded join timeout.\n    \"\"\"\n    if not self.is_running():\n        logger.warning(\"LLMProxy is not running. Nothing to stop.\")\n        return\n\n    # Remove worker config to avoid stale references.\n    if self._config_file and os.path.exists(self._config_file):\n        os.unlink(self._config_file)\n\n    logger.info(\"Stopping LLMProxy server thread...\")\n    stop_success = True\n    if self._server_thread is not None and self._uvicorn_server is not None and self._uvicorn_server.started:\n        self._uvicorn_server.should_exit = True\n        self._server_thread.join(timeout=10.0)  # Allow time for graceful shutdown.\n        if self._server_thread.is_alive():\n            logger.error(\n                \"LLMProxy server thread is still alive after 10 seconds. Cannot kill it because it's a thread.\"\n            )\n            stop_success = False\n        self._server_thread = None\n        self._uvicorn_server = None\n        self._config_file = None\n        self._ready_event.clear()\n        if not _check_port(self.host, self.port):\n            logger.error(f\"Port {self.port} is still in use. Stopping LLMProxy is not successful.\")\n            stop_success = False\n    if stop_success:\n        logger.info(\"LLMProxy server thread stopped.\")\n    else:\n        logger.error(\"LLMProxy server is not stopped successfully.\")\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LLMProxy.update_model_list","title":"<code>update_model_list(model_list)</code>","text":"<p>Replace the in-memory model list and hot-restart if running.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>List[ModelConfig]</code> <p>New list of model entries.</p> required Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>def update_model_list(self, model_list: List[ModelConfig]) -&gt; None:\n    \"\"\"Replace the in-memory model list and hot-restart if running.\n\n    Args:\n        model_list: New list of model entries.\n    \"\"\"\n    self.model_list = model_list\n    logger.info(f\"Updating LLMProxy model list to: {model_list}\")\n    if self.is_running():\n        self.restart()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore","title":"<code>LightningStore</code>","text":"<p>A centralized, thread-safe, async, data store for the lightning's state. This holds the task queue, versioned resources, and completed rollouts.</p> <p>The store has a built-in clock and it should be responsible for tracking the times. All the time-based operations like retry, timeout, etc. should be handled by the store.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>class LightningStore:\n    \"\"\"\n    A centralized, thread-safe, async, data store for the lightning's state.\n    This holds the task queue, versioned resources, and completed rollouts.\n\n    The store has a built-in clock and it should be responsible for tracking the times.\n    All the time-based operations like retry, timeout, etc. should be handled by the store.\n    \"\"\"\n\n    async def start_rollout(\n        self,\n        input: TaskInput,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; AttemptedRollout:\n        \"\"\"\n        Add one incomplete rollout to the store, and get an attempt created for it.\n        This will immediately sets the rollout to a preparing state, and should be\n        used by whoever is going to execute the rollout.\n\n        Return a special rollout with attempt object. Do not update it directly.\n\n        But if the rollout fails or timeouts, it's still possible that the watchdog\n        sends it back to the queue for retry.\n\n        To enqueue a rollout to the task queue, use `enqueue_rollout` instead.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def enqueue_rollout(\n        self,\n        input: TaskInput,\n        mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n        resources_id: str | None = None,\n        metadata: Dict[str, Any] | None = None,\n    ) -&gt; RolloutV2:\n        \"\"\"\n        Adds a new task to the queue with specific metadata and\n        returns the rollout object with its unique ID.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def dequeue_rollout(self) -&gt; Optional[AttemptedRollout]:\n        \"\"\"\n        Retrieves the next task from the queue without blocking.\n        Returns None if the queue is empty.\n\n        Will set the rollout status to preparing.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def start_attempt(self, rollout_id: str) -&gt; AttemptedRollout:\n        \"\"\"\n        Create a new attempt for a given rollout ID and return the attempt details.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def add_span(self, span: Span) -&gt; Span:\n        \"\"\"\n        Add a span to the store.\n\n        This method is responsible for updating the rollout/attempt status to \"running\" if needed.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def add_otel_span(\n        self,\n        rollout_id: str,\n        attempt_id: str,\n        readable_span: ReadableSpan,\n        sequence_id: int | None = None,\n    ) -&gt; Span:\n        \"\"\"\n        Add an opentelemetry span to the store.\n\n        If sequence_id is not provided, it will be fetched from `get_next_span_sequence_id` and assigned automatically.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def query_rollouts(\n        self, *, status: Optional[Sequence[RolloutStatus]] = None, rollout_ids: Optional[Sequence[str]] = None\n    ) -&gt; List[RolloutV2]:\n        \"\"\"\n        Query and retrieve rollouts filtered by their status.\n        If no status is provided, returns all rollouts.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def query_attempts(self, rollout_id: str) -&gt; List[Attempt]:\n        \"\"\"\n        Query and retrieve all attempts associated with a specific rollout ID.\n        Returns an empty list if no attempts are found.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def get_rollout_by_id(self, rollout_id: str) -&gt; Optional[RolloutV2]:\n        \"\"\"\n        Safely retrieves a specific rollout by its ID.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def get_latest_attempt(self, rollout_id: str) -&gt; Optional[Attempt]:\n        \"\"\"\n        Safely retrieves the latest attempt for a given rollout ID.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves a specific version of named resources by its ID.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n        \"\"\"\n        Safely retrieves the latest version of named resources.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def get_next_span_sequence_id(self, rollout_id: str, attempt_id: str) -&gt; int:\n        \"\"\"\n        Get the next span sequence ID for a given rollout and attempt.\n        This should be used to assign a unique sequence ID to each span within an attempt.\n\n        Recommend getting the ID before the operation even begins to avoid racing conditions.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def wait_for_rollouts(self, *, rollout_ids: List[str], timeout: Optional[float] = None) -&gt; List[RolloutV2]:\n        \"\"\"\n        Wait for specified rollouts to complete with a timeout.\n        Returns the completed rollouts, potentially incomplete if timeout is reached.\n\n        TODO: Add support for waiting for 20 new rollouts, or wait until 80% of the pending ids are completed.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def query_spans(self, rollout_id: str, attempt_id: str | Literal[\"latest\"] | None = None) -&gt; List[Span]:\n        \"\"\"\n        Query and retrieve all spans associated with a specific rollout ID.\n        Returns an empty list if no spans are found.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def add_resources(self, resources: NamedResources) -&gt; ResourcesUpdate:\n        \"\"\"\n        Safely stores a new version of named resources and sets it as the latest.\n        Not implemented by many stores yet.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def update_resources(self, resources_id: str, resources: NamedResources) -&gt; ResourcesUpdate:\n        \"\"\"\n        Safely stores a new version or updates an existing version of named resources and sets it as the latest.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def update_rollout(\n        self,\n        rollout_id: str,\n        input: TaskInput | Unset = UNSET,\n        mode: Optional[Literal[\"train\", \"val\", \"test\"]] | Unset = UNSET,\n        resources_id: Optional[str] | Unset = UNSET,\n        status: RolloutStatus | Unset = UNSET,\n        config: RolloutConfig | Unset = UNSET,\n        metadata: Optional[Dict[str, Any]] | Unset = UNSET,\n    ) -&gt; RolloutV2:\n        \"\"\"\n        Update the rollout status and related metadata.\n\n        Not-listed fields here either cannot be updated, or should be auto-updated (e.g., end_time).\n\n        When status is updated to a finished / problematic state, other states like task\n        queues will be updated accordingly.\n\n        Args:\n            rollout_id: Unique identifier for the rollout to update\n            input: New input data for the rollout. If set, will be updated. Can be updated to None\n            mode: New mode for the rollout. If set, will be updated. Can be updated to None\n            resources_id: New resources ID for the rollout. If set, will be updated. Can be updated to None\n            status: New status for the rollout. If set, will be updated\n            config: New config for the rollout. If set, will be updated\n            metadata: Dictionary of additional metadata to update. If set, will replace the existing metadata\n        \"\"\"\n        raise NotImplementedError()\n\n    async def update_attempt(\n        self,\n        rollout_id: str,\n        attempt_id: str | Literal[\"latest\"],\n        status: AttemptStatus | Unset = UNSET,\n        worker_id: str | Unset = UNSET,\n        last_heartbeat_time: float | Unset = UNSET,\n        metadata: Optional[Dict[str, Any]] | Unset = UNSET,\n    ) -&gt; Attempt:\n        \"\"\"\n        Update a specific or latest attempt for a given rollout.\n\n        Update the latest attempt will NOT affect the corresponding rollout status.\n\n\n        Args:\n            rollout_id: Unique identifier for the rollout\n            attempt_id: Unique identifier for the attempt\n            status: Status to set for the attempt, update if provided\n            worker_id: Worker identifier, update if provided\n            last_heartbeat_time: Timestamp of the last heartbeat from the worker\n            metadata: Dictionary of additional metadata to update, will replace the existing metadata\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.add_otel_span","title":"<code>add_otel_span(rollout_id, attempt_id, readable_span, sequence_id=None)</code>  <code>async</code>","text":"<p>Add an opentelemetry span to the store.</p> <p>If sequence_id is not provided, it will be fetched from <code>get_next_span_sequence_id</code> and assigned automatically.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def add_otel_span(\n    self,\n    rollout_id: str,\n    attempt_id: str,\n    readable_span: ReadableSpan,\n    sequence_id: int | None = None,\n) -&gt; Span:\n    \"\"\"\n    Add an opentelemetry span to the store.\n\n    If sequence_id is not provided, it will be fetched from `get_next_span_sequence_id` and assigned automatically.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.add_resources","title":"<code>add_resources(resources)</code>  <code>async</code>","text":"<p>Safely stores a new version of named resources and sets it as the latest. Not implemented by many stores yet.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def add_resources(self, resources: NamedResources) -&gt; ResourcesUpdate:\n    \"\"\"\n    Safely stores a new version of named resources and sets it as the latest.\n    Not implemented by many stores yet.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.add_span","title":"<code>add_span(span)</code>  <code>async</code>","text":"<p>Add a span to the store.</p> <p>This method is responsible for updating the rollout/attempt status to \"running\" if needed.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def add_span(self, span: Span) -&gt; Span:\n    \"\"\"\n    Add a span to the store.\n\n    This method is responsible for updating the rollout/attempt status to \"running\" if needed.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.dequeue_rollout","title":"<code>dequeue_rollout()</code>  <code>async</code>","text":"<p>Retrieves the next task from the queue without blocking. Returns None if the queue is empty.</p> <p>Will set the rollout status to preparing.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def dequeue_rollout(self) -&gt; Optional[AttemptedRollout]:\n    \"\"\"\n    Retrieves the next task from the queue without blocking.\n    Returns None if the queue is empty.\n\n    Will set the rollout status to preparing.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.enqueue_rollout","title":"<code>enqueue_rollout(input, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a new task to the queue with specific metadata and returns the rollout object with its unique ID.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def enqueue_rollout(\n    self,\n    input: TaskInput,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; RolloutV2:\n    \"\"\"\n    Adds a new task to the queue with specific metadata and\n    returns the rollout object with its unique ID.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Safely retrieves the latest attempt for a given rollout ID.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def get_latest_attempt(self, rollout_id: str) -&gt; Optional[Attempt]:\n    \"\"\"\n    Safely retrieves the latest attempt for a given rollout ID.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Safely retrieves the latest version of named resources.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves the latest version of named resources.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.get_next_span_sequence_id","title":"<code>get_next_span_sequence_id(rollout_id, attempt_id)</code>  <code>async</code>","text":"<p>Get the next span sequence ID for a given rollout and attempt. This should be used to assign a unique sequence ID to each span within an attempt.</p> <p>Recommend getting the ID before the operation even begins to avoid racing conditions.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def get_next_span_sequence_id(self, rollout_id: str, attempt_id: str) -&gt; int:\n    \"\"\"\n    Get the next span sequence ID for a given rollout and attempt.\n    This should be used to assign a unique sequence ID to each span within an attempt.\n\n    Recommend getting the ID before the operation even begins to avoid racing conditions.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Safely retrieves a specific version of named resources by its ID.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def get_resources_by_id(self, resources_id: str) -&gt; Optional[ResourcesUpdate]:\n    \"\"\"\n    Safely retrieves a specific version of named resources by its ID.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Safely retrieves a specific rollout by its ID.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def get_rollout_by_id(self, rollout_id: str) -&gt; Optional[RolloutV2]:\n    \"\"\"\n    Safely retrieves a specific rollout by its ID.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.query_attempts","title":"<code>query_attempts(rollout_id)</code>  <code>async</code>","text":"<p>Query and retrieve all attempts associated with a specific rollout ID. Returns an empty list if no attempts are found.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def query_attempts(self, rollout_id: str) -&gt; List[Attempt]:\n    \"\"\"\n    Query and retrieve all attempts associated with a specific rollout ID.\n    Returns an empty list if no attempts are found.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.query_rollouts","title":"<code>query_rollouts(*, status=None, rollout_ids=None)</code>  <code>async</code>","text":"<p>Query and retrieve rollouts filtered by their status. If no status is provided, returns all rollouts.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def query_rollouts(\n    self, *, status: Optional[Sequence[RolloutStatus]] = None, rollout_ids: Optional[Sequence[str]] = None\n) -&gt; List[RolloutV2]:\n    \"\"\"\n    Query and retrieve rollouts filtered by their status.\n    If no status is provided, returns all rollouts.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.query_spans","title":"<code>query_spans(rollout_id, attempt_id=None)</code>  <code>async</code>","text":"<p>Query and retrieve all spans associated with a specific rollout ID. Returns an empty list if no spans are found.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def query_spans(self, rollout_id: str, attempt_id: str | Literal[\"latest\"] | None = None) -&gt; List[Span]:\n    \"\"\"\n    Query and retrieve all spans associated with a specific rollout ID.\n    Returns an empty list if no spans are found.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.start_attempt","title":"<code>start_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Create a new attempt for a given rollout ID and return the attempt details.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def start_attempt(self, rollout_id: str) -&gt; AttemptedRollout:\n    \"\"\"\n    Create a new attempt for a given rollout ID and return the attempt details.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.start_rollout","title":"<code>start_rollout(input, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Add one incomplete rollout to the store, and get an attempt created for it. This will immediately sets the rollout to a preparing state, and should be used by whoever is going to execute the rollout.</p> <p>Return a special rollout with attempt object. Do not update it directly.</p> <p>But if the rollout fails or timeouts, it's still possible that the watchdog sends it back to the queue for retry.</p> <p>To enqueue a rollout to the task queue, use <code>enqueue_rollout</code> instead.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def start_rollout(\n    self,\n    input: TaskInput,\n    mode: Literal[\"train\", \"val\", \"test\"] | None = None,\n    resources_id: str | None = None,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; AttemptedRollout:\n    \"\"\"\n    Add one incomplete rollout to the store, and get an attempt created for it.\n    This will immediately sets the rollout to a preparing state, and should be\n    used by whoever is going to execute the rollout.\n\n    Return a special rollout with attempt object. Do not update it directly.\n\n    But if the rollout fails or timeouts, it's still possible that the watchdog\n    sends it back to the queue for retry.\n\n    To enqueue a rollout to the task queue, use `enqueue_rollout` instead.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.update_attempt","title":"<code>update_attempt(rollout_id, attempt_id, status=UNSET, worker_id=UNSET, last_heartbeat_time=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update a specific or latest attempt for a given rollout.</p> <p>Update the latest attempt will NOT affect the corresponding rollout status.</p> <p>Parameters:</p> Name Type Description Default <code>rollout_id</code> <code>str</code> <p>Unique identifier for the rollout</p> required <code>attempt_id</code> <code>str | Literal['latest']</code> <p>Unique identifier for the attempt</p> required <code>status</code> <code>AttemptStatus | Unset</code> <p>Status to set for the attempt, update if provided</p> <code>UNSET</code> <code>worker_id</code> <code>str | Unset</code> <p>Worker identifier, update if provided</p> <code>UNSET</code> <code>last_heartbeat_time</code> <code>float | Unset</code> <p>Timestamp of the last heartbeat from the worker</p> <code>UNSET</code> <code>metadata</code> <code>Optional[Dict[str, Any]] | Unset</code> <p>Dictionary of additional metadata to update, will replace the existing metadata</p> <code>UNSET</code> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def update_attempt(\n    self,\n    rollout_id: str,\n    attempt_id: str | Literal[\"latest\"],\n    status: AttemptStatus | Unset = UNSET,\n    worker_id: str | Unset = UNSET,\n    last_heartbeat_time: float | Unset = UNSET,\n    metadata: Optional[Dict[str, Any]] | Unset = UNSET,\n) -&gt; Attempt:\n    \"\"\"\n    Update a specific or latest attempt for a given rollout.\n\n    Update the latest attempt will NOT affect the corresponding rollout status.\n\n\n    Args:\n        rollout_id: Unique identifier for the rollout\n        attempt_id: Unique identifier for the attempt\n        status: Status to set for the attempt, update if provided\n        worker_id: Worker identifier, update if provided\n        last_heartbeat_time: Timestamp of the last heartbeat from the worker\n        metadata: Dictionary of additional metadata to update, will replace the existing metadata\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.update_resources","title":"<code>update_resources(resources_id, resources)</code>  <code>async</code>","text":"<p>Safely stores a new version or updates an existing version of named resources and sets it as the latest.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def update_resources(self, resources_id: str, resources: NamedResources) -&gt; ResourcesUpdate:\n    \"\"\"\n    Safely stores a new version or updates an existing version of named resources and sets it as the latest.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.update_rollout","title":"<code>update_rollout(rollout_id, input=UNSET, mode=UNSET, resources_id=UNSET, status=UNSET, config=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update the rollout status and related metadata.</p> <p>Not-listed fields here either cannot be updated, or should be auto-updated (e.g., end_time).</p> <p>When status is updated to a finished / problematic state, other states like task queues will be updated accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>rollout_id</code> <code>str</code> <p>Unique identifier for the rollout to update</p> required <code>input</code> <code>TaskInput | Unset</code> <p>New input data for the rollout. If set, will be updated. Can be updated to None</p> <code>UNSET</code> <code>mode</code> <code>Optional[Literal['train', 'val', 'test']] | Unset</code> <p>New mode for the rollout. If set, will be updated. Can be updated to None</p> <code>UNSET</code> <code>resources_id</code> <code>Optional[str] | Unset</code> <p>New resources ID for the rollout. If set, will be updated. Can be updated to None</p> <code>UNSET</code> <code>status</code> <code>RolloutStatus | Unset</code> <p>New status for the rollout. If set, will be updated</p> <code>UNSET</code> <code>config</code> <code>RolloutConfig | Unset</code> <p>New config for the rollout. If set, will be updated</p> <code>UNSET</code> <code>metadata</code> <code>Optional[Dict[str, Any]] | Unset</code> <p>Dictionary of additional metadata to update. If set, will replace the existing metadata</p> <code>UNSET</code> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def update_rollout(\n    self,\n    rollout_id: str,\n    input: TaskInput | Unset = UNSET,\n    mode: Optional[Literal[\"train\", \"val\", \"test\"]] | Unset = UNSET,\n    resources_id: Optional[str] | Unset = UNSET,\n    status: RolloutStatus | Unset = UNSET,\n    config: RolloutConfig | Unset = UNSET,\n    metadata: Optional[Dict[str, Any]] | Unset = UNSET,\n) -&gt; RolloutV2:\n    \"\"\"\n    Update the rollout status and related metadata.\n\n    Not-listed fields here either cannot be updated, or should be auto-updated (e.g., end_time).\n\n    When status is updated to a finished / problematic state, other states like task\n    queues will be updated accordingly.\n\n    Args:\n        rollout_id: Unique identifier for the rollout to update\n        input: New input data for the rollout. If set, will be updated. Can be updated to None\n        mode: New mode for the rollout. If set, will be updated. Can be updated to None\n        resources_id: New resources ID for the rollout. If set, will be updated. Can be updated to None\n        status: New status for the rollout. If set, will be updated\n        config: New config for the rollout. If set, will be updated\n        metadata: Dictionary of additional metadata to update. If set, will replace the existing metadata\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.LightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for specified rollouts to complete with a timeout. Returns the completed rollouts, potentially incomplete if timeout is reached.</p> <p>TODO: Add support for waiting for 20 new rollouts, or wait until 80% of the pending ids are completed.</p> Source code in <code>agentlightning/store/base.py</code> <pre><code>async def wait_for_rollouts(self, *, rollout_ids: List[str], timeout: Optional[float] = None) -&gt; List[RolloutV2]:\n    \"\"\"\n    Wait for specified rollouts to complete with a timeout.\n    Returns the completed rollouts, potentially incomplete if timeout is reached.\n\n    TODO: Add support for waiting for 20 new rollouts, or wait until 80% of the pending ids are completed.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>LiteLLM model registration entry.</p> <p>This mirrors the items in LiteLLM's <code>model_list</code> section.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Logical model name exposed by the proxy.</p> <code>litellm_params</code> <code>Dict[str, Any]</code> <p>Parameters passed to LiteLLM for this model (e.g., backend model id, api_base, additional options).</p> Source code in <code>agentlightning/llm_proxy.py</code> <pre><code>class ModelConfig(TypedDict):\n    \"\"\"LiteLLM model registration entry.\n\n    This mirrors the items in LiteLLM's ``model_list`` section.\n\n    Attributes:\n        model_name: Logical model name exposed by the proxy.\n        litellm_params: Parameters passed to LiteLLM for this model\n            (e.g., backend model id, api_base, additional options).\n    \"\"\"  # Google style kept concise.\n\n    model_name: str\n    litellm_params: Dict[str, Any]\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.Rollout","title":"<code>Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The standard reporting object from client to server.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Rollout(BaseModel):\n    \"\"\"The standard reporting object from client to server.\"\"\"\n\n    rollout_id: str\n\n    # Echoing the input task\n    task: Optional[Task] = None\n\n    # Primary, high-level feedback\n    final_reward: Optional[float] = None\n\n    # Structured, sequential feedback for RL-style optimization\n    triplets: Optional[List[Triplet]] = None\n\n    # Optional, rich-context data for deep analysis\n    trace: Optional[List[Dict[str, Any]]] = Field(\n        default=None,\n        description=\"A list of spans that conform to the OpenTelemetry JSON format. \"\n        \"Users of the opentelemetry-sdk can generate this by calling \"\n        \"json.loads(readable_span.to_json()).\",\n    )\n    logs: Optional[List[str]] = None\n\n    # A bucket for any other relevant information\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task (rollout request) to be processed by the client agent.</p> Source code in <code>agentlightning/types/core.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"A task (rollout request) to be processed by the client agent.\"\"\"\n\n    rollout_id: str\n    input: TaskInput\n\n    mode: Optional[RolloutMode] = None\n    resources_id: Optional[str] = None\n\n    # Optional fields for tracking task lifecycle\n    create_time: Optional[float] = None\n    last_claim_time: Optional[float] = None\n    num_claims: Optional[int] = None\n\n    # Allow additional metadata fields\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.TraceAdapter","title":"<code>TraceAdapter</code>","text":"<p>               Bases: <code>Adapter[List[Span], T_to]</code>, <code>Generic[T_to]</code></p> <p>Base class for adapters that convert trace spans into other formats.</p> <p>This class specializes <code>Adapter</code> for working with trace spans. It expects a list of Agent-lightning spans as input and produces a custom target format (e.g., reinforcement learning training data, SFT datasets, logs, metrics).</p> Source code in <code>agentlightning/adapter/base.py</code> <pre><code>class TraceAdapter(Adapter[List[Span], T_to], Generic[T_to]):\n    \"\"\"Base class for adapters that convert trace spans into other formats.\n\n    This class specializes `Adapter` for working with trace spans. It expects a list of\n    Agent-lightning spans as input and produces a custom target format\n    (e.g., reinforcement learning training data, SFT datasets, logs, metrics).\n    \"\"\"\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.TraceTripletAdapter","title":"<code>TraceTripletAdapter</code>","text":"<p>               Bases: <code>BaseTraceTripletAdapter</code></p> <p>An adapter to convert OpenTelemetry spans to triplet data.</p> <p>Attributes:</p> Name Type Description <code>repair_hierarchy</code> <p>When <code>repair_hierarchy</code> is set to True, the trace will be repaired with the time information. See <code>TraceTree.repair_hierarchy</code> for more details.</p> <code>llm_call_match</code> <p>Regular expression pattern to match LLM call span names.</p> <code>agent_match</code> <p>Optional regular expression pattern to match agent span names. If None, all agents are matched.</p> <code>exclude_llm_call_in_reward</code> <p>Whether to exclude LLM calls that occur within reward spans.</p> <code>reward_match</code> <p>Policy for matching rewards to LLM calls.</p> Source code in <code>agentlightning/adapter/triplet.py</code> <pre><code>class TraceTripletAdapter(BaseTraceTripletAdapter):\n    \"\"\"\n    An adapter to convert OpenTelemetry spans to triplet data.\n\n    Attributes:\n        repair_hierarchy: When `repair_hierarchy` is set to True, the trace will be repaired with the time information.\n            See `TraceTree.repair_hierarchy` for more details.\n        llm_call_match: Regular expression pattern to match LLM call span names.\n        agent_match: Optional regular expression pattern to match agent span names. If None, all agents are matched.\n        exclude_llm_call_in_reward: Whether to exclude LLM calls that occur within reward spans.\n        reward_match: Policy for matching rewards to LLM calls.\n    \"\"\"\n\n    def __init__(\n        self,\n        repair_hierarchy: bool = True,\n        llm_call_match: str = r\"openai\\.chat\\.completion\",\n        agent_match: Optional[str] = None,\n        exclude_llm_call_in_reward: bool = True,\n        reward_match: RewardMatchPolicy = RewardMatchPolicy.FIRST_OCCURRENCE,\n    ):\n        self.repair_hierarchy = repair_hierarchy\n        self.llm_call_match = llm_call_match\n        self.agent_match = agent_match\n        self.exclude_llm_call_in_reward = exclude_llm_call_in_reward\n        self.reward_match = reward_match\n\n    def visualize(\n        self,\n        source: Union[List[Span], List[ReadableSpan]],\n        /,\n        filename: str = \"trace_tree\",\n        interested_span_match: str | None = None,\n    ) -&gt; TraceTree:\n        \"\"\"\n        Visualize the trace tree.\n\n        Args:\n            source (List[Span]): The list of OpenTelemetry spans to visualize.\n            filename (str): The base filename for the output visualization (default: \"trace_tree\").\n            interested_span_match (str | None): Optional regular expression pattern to highlight or focus on specific spans in the visualization.\n\n        Returns:\n            TraceTree: The constructed trace tree object.\n        \"\"\"\n        source_normalized = [\n            Span.from_opentelemetry(span, \"dummy\", \"dummy\", 0) if isinstance(span, ReadableSpan) else span\n            for span in source\n        ]\n        trace_tree = TraceTree.from_spans(source_normalized)\n        if self.repair_hierarchy:\n            trace_tree.repair_hierarchy()\n        trace_tree.visualize(filename, interested_span_match=interested_span_match)\n        return trace_tree\n\n    def adapt(self, source: Union[List[Span], List[ReadableSpan]], /) -&gt; List[Triplet]:  # type: ignore\n        \"\"\"Convert OpenTelemetry spans to a list of Triplet objects.\"\"\"\n        source_normalized = [\n            Span.from_opentelemetry(span, \"dummy\", \"dummy\", 0) if isinstance(span, ReadableSpan) else span\n            for span in source\n        ]\n        trace_tree = TraceTree.from_spans(source_normalized)\n        if self.repair_hierarchy:\n            trace_tree.repair_hierarchy()\n        trajectory = trace_tree.to_trajectory(\n            llm_call_match=self.llm_call_match,\n            agent_match=self.agent_match,\n            exclude_llm_call_in_reward=self.exclude_llm_call_in_reward,\n            reward_match=self.reward_match,\n        )\n        return trajectory\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.TraceTripletAdapter.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert OpenTelemetry spans to a list of Triplet objects.</p> Source code in <code>agentlightning/adapter/triplet.py</code> <pre><code>def adapt(self, source: Union[List[Span], List[ReadableSpan]], /) -&gt; List[Triplet]:  # type: ignore\n    \"\"\"Convert OpenTelemetry spans to a list of Triplet objects.\"\"\"\n    source_normalized = [\n        Span.from_opentelemetry(span, \"dummy\", \"dummy\", 0) if isinstance(span, ReadableSpan) else span\n        for span in source\n    ]\n    trace_tree = TraceTree.from_spans(source_normalized)\n    if self.repair_hierarchy:\n        trace_tree.repair_hierarchy()\n    trajectory = trace_tree.to_trajectory(\n        llm_call_match=self.llm_call_match,\n        agent_match=self.agent_match,\n        exclude_llm_call_in_reward=self.exclude_llm_call_in_reward,\n        reward_match=self.reward_match,\n    )\n    return trajectory\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.TraceTripletAdapter.visualize","title":"<code>visualize(source, /, filename='trace_tree', interested_span_match=None)</code>","text":"<p>Visualize the trace tree.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>List[Span]</code> <p>The list of OpenTelemetry spans to visualize.</p> required <code>filename</code> <code>str</code> <p>The base filename for the output visualization (default: \"trace_tree\").</p> <code>'trace_tree'</code> <code>interested_span_match</code> <code>str | None</code> <p>Optional regular expression pattern to highlight or focus on specific spans in the visualization.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TraceTree</code> <code>TraceTree</code> <p>The constructed trace tree object.</p> Source code in <code>agentlightning/adapter/triplet.py</code> <pre><code>def visualize(\n    self,\n    source: Union[List[Span], List[ReadableSpan]],\n    /,\n    filename: str = \"trace_tree\",\n    interested_span_match: str | None = None,\n) -&gt; TraceTree:\n    \"\"\"\n    Visualize the trace tree.\n\n    Args:\n        source (List[Span]): The list of OpenTelemetry spans to visualize.\n        filename (str): The base filename for the output visualization (default: \"trace_tree\").\n        interested_span_match (str | None): Optional regular expression pattern to highlight or focus on specific spans in the visualization.\n\n    Returns:\n        TraceTree: The constructed trace tree object.\n    \"\"\"\n    source_normalized = [\n        Span.from_opentelemetry(span, \"dummy\", \"dummy\", 0) if isinstance(span, ReadableSpan) else span\n        for span in source\n    ]\n    trace_tree = TraceTree.from_spans(source_normalized)\n    if self.repair_hierarchy:\n        trace_tree.repair_hierarchy()\n    trace_tree.visualize(filename, interested_span_match=interested_span_match)\n    return trace_tree\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.get_left_padded_ids_and_attention_mask","title":"<code>get_left_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Left-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>the original list of token IDs.</p> required <code>max_length</code> <code>int</code> <p>desired total length after padding/truncation.</p> required <code>pad_token_id</code> <code>int</code> <p>ID to use for padding.</p> required <p>Returns:</p> Name Type Description <code>padded_ids</code> <code>any</code> <p>list of length == max_length.</p> <code>attention_mask</code> <code>any</code> <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_left_padded_ids_and_attention_mask(\n    ids: List[int], max_length: int, pad_token_id: int\n) -&gt; Tuple[List[int], List[int]]:\n    \"\"\"\n    Left-pad (or truncate) a sequence of token IDs to a fixed length,\n    and build the corresponding attention mask.\n\n    Args:\n        ids:             the original list of token IDs.\n        max_length:      desired total length after padding/truncation.\n        pad_token_id:    ID to use for padding.\n\n    Returns:\n        padded_ids (any):      list of length == max_length.\n        attention_mask (any):  list of same length: 1 for non-pad tokens, 0 for pads.\n    \"\"\"\n    seq_len = len(ids)\n\n    if seq_len &gt;= max_length:\n        # too long \u2192 truncate from the left, keep the last max_length tokens\n        trimmed = ids[-max_length:]\n        attention_mask = [1] * max_length\n        return trimmed, attention_mask\n\n    # too short \u2192 pad on the left\n    pad_len = max_length - seq_len\n    padded_ids = [pad_token_id] * pad_len + ids\n    attention_mask = [0] * pad_len + [1] * seq_len\n    return padded_ids, attention_mask\n</code></pre>"},{"location":"reference/rl/#agentlightning.verl.get_right_padded_ids_and_attention_mask","title":"<code>get_right_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Right-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>the original list of token IDs.</p> required <code>max_length</code> <code>int</code> <p>desired total length after padding/truncation.</p> required <code>pad_token_id</code> <code>int</code> <p>ID to use for padding.</p> required <p>Returns:</p> Name Type Description <code>padded_ids</code> <code>any</code> <p>list of length == max_length.</p> <code>attention_mask</code> <code>any</code> <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> Source code in <code>agentlightning/verl/daemon.py</code> <pre><code>def get_right_padded_ids_and_attention_mask(\n    ids: List[int], max_length: int, pad_token_id: int\n) -&gt; Tuple[List[int], List[int]]:\n    \"\"\"\n    Right-pad (or truncate) a sequence of token IDs to a fixed length,\n    and build the corresponding attention mask.\n\n    Args:\n        ids:            the original list of token IDs.\n        max_length:     desired total length after padding/truncation.\n        pad_token_id:   ID to use for padding.\n\n    Returns:\n        padded_ids (any):     list of length == max_length.\n        attention_mask (any): list of same length: 1 for non-pad tokens, 0 for pads.\n    \"\"\"\n    seq_len = len(ids)\n\n    if seq_len &gt;= max_length:\n        # too long \u2192 truncate to the first max_length tokens\n        trimmed = ids[:max_length]\n        attention_mask = [1] * max_length\n        return trimmed, attention_mask\n\n    # too short \u2192 pad on the right\n    pad_len = max_length - seq_len\n    padded_ids = ids + [pad_token_id] * pad_len\n    attention_mask = [1] * seq_len + [0] * pad_len\n    return padded_ids, attention_mask\n</code></pre>"}]}